[["index.html", "基于 R 的数据处理流程指南 前言", " 基于 R 的数据处理流程指南 Teague Tian 2022-09-29 前言 这是一本笔者在日常工作和学习中进行工具整合的R在线工具书。它旨在为数据统计人员提供快速查询和多方法比较的学习建议。 如果读者已经掌握了一些R的编程知识，可以翻看目录挑选感兴趣的内容阅读； 本书尽可能按照数据分析的构筑流程来搭建系统的框架，以便未来此书籍能够持续纳入和迭代更新。 与此同时，数据也尽可能的纳入更多的R社区新工具来帮助减少重复，以及增强代码的可读性。在编写这本书时，还尽可能的将笔者在探索学习R相关工具（Shiny、R pacakge构建、函数式编程）等方面的经验也纳入其中，希望这有助于读者快速入门。 本书的章节概要如下： 第 1 章 如何构建数据分析的目标和构建方法 第 2 章 介绍数据提取的方法和导入导出等相关R操作 第 3 章 介绍数据抽取、转换、探索和函数构建等操作 第 4 章 介绍描述模型、复杂模型、临床统计模型等操作 第 5 章 介绍模型评估方法 第 6 章 介绍模型Shiny应用、rmarkdown和Quarto语法 第 7 章 介绍R的可视化图和表 第 8 章 介绍常用R系统命令、快捷键、正则等 第 9 章 介绍临床医学统计的系统流程 第 10 章 介绍R统计相关实现方法书籍 第 11 章 介绍统计与研究设计 第 12 章 介绍构筑R package的方法 这是我第一次编写比较系统的R语言教程，难免存在错误和不当，恳请读者批评指正。另外，有部分R代码设计库可能存在直接引用的情况，如有侵权，请邮件联系。目前我在业余时间对本书内容进行积极的开发，如果读者有任何建议，欢迎到 GitHub 仓库 Issue 中进行讨论（）。 致谢 感谢互联网各个渠道的知识博主提供的案例参考。站在巨人的肩膀上，simple code creat world！ By Teague Tian 2022/09/28 "],["author.html", "作者简介 学业及工作经历： 研究兴趣： 已完成和正在研究的项目", " 作者简介 学业及工作经历： 安徽农业大学-生物科学学士； 复旦大学-生物工程硕士； 上海派兰数据-医学统计师； 研究兴趣： 1、理解数据的结构，提炼数据的特征，探索相关关系下的因果关系 2、复杂知识体系构建 3、医学领域： 借助R工具来构建加速RWD分析工具和优化现有数据处理流程 借助多批次学习，系统挖掘患者的真实世界治疗反馈信息 4、生态学领域： 物种分布模型与生态位建模 流行病学与生态建模相结合 5、R语言： 使用非标准编程范式来优化现有工具处理体系。 使用shiny，shinydashboard和flexboard来快速生产交互式报告。 使用bookdown和gitbook等来构建知识库体系。 构筑R包。 已完成和正在研究的项目 医学领域： 多种慢性病（糖尿病、静脉血栓和慢性肾病等）统计方案撰写与优化 治理糖尿病药物（GLP-1类）的真实世界研究 生态学领域： 喜旱莲子草全球入侵种群环境生态位的演化（已完成） 通过比较喜旱莲子草在南美洲原生种群的气候生态位与入侵世界各大洲的气候生态位来进一步揭示当前入侵的现状和生态位演化特征，为入侵管理和未来气候变化下的入侵趋势提供见解； 沙棘属植物不同系统发育阶元下生态位的演化特征（已完成） 通过比较沙棘种下、亚种间和种间生态位在不同空间尺度上的变化来为揭示沙棘属物种起源及演化发展的历程提供见解。 沙棘是一种重要的经济作物，通过不同种间或者亚种间的高精度SDM建模，为未来的品种改良和区域种植提供见解。 R工具开发： 医学：医学RWD主题快速统计工具R包pmed（nicheerfeng/pmed: Accelerate the statistical analysis of medical data） 生态学：生态建模综合库解：nichebook（SDMBOOK） 数据分析：基于 R 的数据处理流程指南（） "],["Establish-analysis-objectives.html", "第 1 章 确立分析目标 1.1 基本逻辑 1.2 数据分析的基本流程 1.3 工作处理经验", " 第 1 章 确立分析目标 1.1 基本逻辑 业务逻辑、需求合理性、需求可行性等方面。 针对性制定分析框架和项目计划表。分析框架主要包括：目标变量的定义，大致的分析思路，数据抽样规则，潜在自变量的罗列，项目风险评估，大致的落地应用方案。 1.2 数据分析的基本流程 完整的数据分析主要包括了六大步骤，它们依次为：分析设计、数据收集、数据处理、数据分析、数据展现、报告撰写等，所以也叫数据分析六步曲。 分析设计 首先是明确数据分析目的，只有明确目的，数据分析才不会偏离方向，否则得出的数据分析结果不仅没有指导意义，亦即目的引导。 当分析目的明确后，我们需要对思路进行梳理分析，并搭建分析框架，需要把分析目的分解成若干个不同的分析要点，也就是说要达到这个目的该如何具体开展数据分析？需要从哪几个角度进行分析？采用哪些分析指标？采用哪些逻辑思维？运用哪些理论依据？ 明确数据分析目的以及确定分析思路，是确保数据分析过程有效进行的先决条件，它可以为数据收集、处理以及分析提供清晰的指引方向。 数据收集 数据收集是按照确定的数据分析框架，收集相关数据的过程，它为数据分析提供了素材和依据。这里的数据包括一手数据与二手数据，一手数据主要指可直接获取的数据，如公司内部的数据库、市场调查取得的数据等；二手数据主要指经过加工整理后得到的数据，如统计局在互联网上发布的数据、公开出版物中的数据等。 数据处理 数据处理是指对采集到的数据进行加工整理，形成适合数据分析的样式，保证数据的一致性和有效性。它是数据分析前必不可少的阶段。 数据处理的基本目的是从大量的、可能杂乱无章、难以理解的数据中抽取并推导出对解决问题有价值、有意义的数据。如果数据本身存在错误，那么即使采用最先进的数据分析方法，得到的结果也是错误的，不具备任何参考价值，甚至还会误导决策。 数据处理主要包括数据清洗、数据转化、数据抽取、数据合并、数据计算等处理方法。一般的数据都需要进行一定的处理才能用于后续的数据分析工作，即使再“干净”的原始数据也需要先进行一定的处理才能使用。 数据分析 数据分析是指用适当的分析方法及工具，对收集来的数据进行分析，提取有价值的信息，形成有效结论的过程。 在确定数据分析思路阶段，数据分析师就应当为需要分析的内容确定适合的数据分析方法。到了这个阶段，就能够驾驭数据，从容地进行分析和研究了。 一般的数据分析我们可以通过 Excel 完成，而高级的数据分析就要采用专业的分析软件进行，如数据分析工具 SPSS、SAS、Python、R 语言等。 数据展现 通过数据分析，隐藏在数据内部的关系和规律就会逐渐浮现出来，那么通过什么方式展现出这些关系和规律，才能让别人一目了然呢？一般情况下，数据是通过表格和图形的方式来呈现的，即用图表说话。 常用的数据图表包括饼图、柱形图、条形图、折线图、散点图、雷达图等，当然可以对这些图表进一步整理加工，使之变为我们所需要的图形，例如金字塔图、矩阵图、瀑布图、漏斗图、帕雷托图等。 多数情况下，人们更愿意接受图形这种数据展现方式，因为它能更加有效、直观地传递出分析师所要表达的观点。一般情况下，能用图说明问题的，就不用表格， 能用表格说明问题的，就不用文字。 报告撰写 数据分析报告其实是对整个数据分析过程的一个总结与呈现。通过报告，把数据分析的起因、过程、结果及建议完整地呈现出来，以供决策者参考。所以数据分析报告是通过对数据全方位的科学分析来评估企业运营质量，为决策者提供科学、严谨的决策依据，以降低企业运营风险，提高企业核心竞争力。 一份好的分析报告，首先需要有一个好的分析框架，并且层次明晰，图文并茂， 能够让读者一目了然。结构清晰、主次分明可以使阅读对象正确理解报告内容； 图文并茂，可以令数据更加生动活泼，提高视觉冲击力，有助于读者更形象、直观地看清楚问题和结论，从而产生思考。 其次，需要有明确的结论，没有明确结论的分析称不上分析，同时也失去了报告的意义，因为最初就是为寻找或者求证一个结论才进行分析的，所以千万不要舍本求末。 第三，一定要有建议或解决方案，作为决策者，需要的不仅仅是找出问题，更重要的是建议或解决方案，以便他们在决策时参考。所以，数据分析师不光需要掌握数据分析方法，而且还要了解和熟悉业务，这样才能根据发现的业务问题，提出具有可行性的建议或解决方案。 1.3 工作处理经验 PDCA循环（ plan,do,check,action,cycle）： 是美国统计学家爱.戴明博士创造的一种管理产品质量的方法，其内容包括：做一切工作都有计划、实施、检查和处理四个阶段。第一阶段是计划，包括方针、目标、活动计划书、管理项目；第二阶段是实施，即实地去做；第三阶段是检查，查明执行中的成败并分析原因；第四阶段是处理，肯定成功的经验，使之标准化、规范化，总结失败的教训，引以为鉴。 1.3.1 PPT汇报经验： PPT汇报方法1： 从前，有一只熊猫阿宝，他很喜欢功夫； 每天都重复着和鹅爸爸卖面条的无聊生活； 突然有一天，和平谷举行功夫大赛，阿宝阴差阳错获得了神龙大侠的称号。 正因为这件事，阿宝不得不真的开始学习功夫，在龟仙人的指点下，在师父的食物练功法的训练下，它学会了功夫； 也正因为如此，反派残豹逃出大牢要挑战新的神龙大侠； 最后终于，阿宝用自己苦练的神功，在盖世五侠的帮助下，打败了残豹，和平谷迎来了和平； PPT汇报方法2： 这种方法来自Medium网站上的一位企业咨询专家Andy Raskin，它非常实用于企业向客户推销某种产品，其核心逻辑分为五步，具体如下—— 描述一个当今世界正在发生的趋势； 论证在这个趋势下将会产生赢家和输家； 描述赢家的美好未来； 陈述到达美好未来过程中最关键的步骤； 列举证据，证明你的产品可以如何将蓝图变成现实。 PPT汇报方法3： DISK基本流程： 这个领域的问题 现有的解决方案 你的解决方案 为什么你的解决方案更优？ 你方案优势的证据 你未来可以征服多大的市场？ "],["Data-extraction.html", "第 2 章 数据提取 2.1 数据提取的基本范式 2.2 数据导入与导出", " 第 2 章 数据提取 2.1 数据提取的基本范式 数据提取/采集（探索和检查数据质量问题，发掘数据的内部属性） 数据核验（数据有效性、时效性） 采用PDCA循环法来构造项目处理流程（计划、实施、检查和反馈处理） 关键主键的主导的多源数据融合 2.2 数据导入与导出 2.2.1 剪切板中处理数据 ## 读入粘贴板数据： t1 = read.table(&quot;clipboard&quot;, header = T, sep = &#39;\\t&#39;) # export the linelist data frame to your system&#39;s clipboard clipr::write_clip(linelist) ## 调用文件选择器 read.csv(file.choose()) 2.2.2 读取excle文件 ## 读取excel表数据： library(xlsx) st1 &lt;- xlsx::read.xlsx(&quot;mtcars.xlsx&quot;, 1) # 1为sheet1； ## 另外一种优化的方式： st1 &lt;- openxlsx::read.xlsx(&quot;./fn_out/v11_log.xlsx&quot;) # 1为sheet1； st1 ## 使用openxlsx来读取和写入数据： ### 学习使用：openxlsx library(openxlsx) df &lt;- data.frame(a=1:10,b=1:10,d=1:10) # 第一步创建workbook wb &lt;- createWorkbook(creator = &#39;zhongyf&#39;,title = &#39;test&#39;) # 第二步添加addworksheet addWorksheet(wb,sheetName = &#39;test&#39;) # 第三步写入数据writeDataTable writeData(wb,sheet = &#39;test&#39;,x = df) # 第四步保存saveworkbook。 saveWorkbook(wb, &quot;test.xlsx&quot;, overwrite = TRUE) 2.2.3 加速读取及导出常用数据 2.2.3.1 3.1 RIO包读取数据和导出一般数据 ##### rio包的数据读入与导出练习 #### ### 注意rio包调用的是data.table::fread()进行读取函数； library(rio) install.packages(&quot;rio&quot;) ### 导入csv文件： mtcars &lt;- import(&quot;data/mtcars.csv&quot;) head(mtcars) ### 导入excle的多个子表： multi &lt;- import_list(&quot;data/multi.xlsx&quot;) ### 批量导入多个格式相同的项目文件： paths &lt;- Sys.glob(&quot;data/unemployment/*.csv&quot;) ### 借助here包引用进行数据导入： # 这里是借助linelist_raw的路径来导入data数据； linelist &lt;- import(here(&quot;data&quot;, &quot;linelist_raw.xlsx&quot;)) ### 借助export来批量导出数据： export(linelist, here(&quot;data&quot;,&quot;clean&quot;, &quot;my_linelist.rds&quot;)) ### 保存图片： # 这里是将赋值outputs 输出到epicurves文件夹中的对应文件编号中 ggsave(here(&quot;outputs&quot;, &quot;epicurves&quot;, &quot;epicurve_2021-02-15.png&quot;)) ## 优化现有的代码读入规则： ### RIO包读取特定条件下文档 # 读取指定的excle表： my_data &lt;- import(&quot;my_excel_file.xlsx&quot;, which = &quot;Sheetname&quot;) # 使用here()来辅助数据读取： # Demonstration: importing a specific Excel sheet when using relative pathways with the &#39;here&#39; package linelist_raw &lt;- import(here(&quot;data&quot;, &quot;linelist.xlsx&quot;), which = &quot;Sheet1&quot;)` ## 缺失值读入： ## 将缺失值重新定义为为99； linelist &lt;- import(here(&quot;data&quot;, &quot;my_linelist.xlsx&quot;), na = &quot;99&quot;) ## 将矩阵中的missing和空格等转为 linelist &lt;- import(here(&quot;data&quot;, &quot;my_linelist.csv&quot;), na = c(&quot;Missing&quot;, &quot;&quot;, &quot; &quot;)) ## 跳过行： linelist_raw &lt;- import(&quot;linelist_raw.xlsx&quot;, skip = 1) # does not import header row ### 指定文档的列名读入： # note argument for csv files is &#39;col.names = &#39; linelist_raw &lt;- import(&quot;linelist_raw.csv&quot;, skip = 2, col.names = linelist_raw_names) 2.2.3.2 3.2 data.table包中的fread() ## 用于高性能处理数据： # 浅拷贝：只是拷贝列指针向量(对应数据框中的列)，而实际数据在内存中不做物理拷贝； # 深拷贝：拷贝整个数据列到内存的另一位置，深拷贝这种冗余拷贝极大地影响性能。 @ 简要用法： data.table() :生成数据框，并将数据框、列表和矩阵转为data.table() ## 键和索引： data.table() 支持键和索引。 setkey(dt,v1,v3) ## 设置键； setindex(dt,v1,v3) # 设置索引； ## 特殊符号： data.table()提供了一些辅助操作的特殊符号； .():代替uist( :=:按引用方式增加、修改列 .N:行数 .SD:每个分组的数据子集,除了by或 kelby的列 .SCots:与.sD连用,用来选择包含在SD中的列 .BY:包含所有by分组变量的1ist .I:整数向量seq_len(nrow(x),例如DT[,,I[ whi ch.max( somecol)],by=grp] .GRP:分组索引,1代表第1分组,2代表第2分组, .NGRP:分组数 .EACHI:用于by/ kerby=. EACHI表示根据i表达式的每一行分组 ## 管道符操作：也称为链式操作： DT[...][...][...] ## 数据读写： 函数fread()和fwrite()是data.tabel最强大的函数之二。 # 常用函数读取，不支持直接读取excel文件； fread(&quot;DT.csv&quot;) fread(&quot;DT.csv&quot;,sep=&quot;\\t&quot;) # 选择部分行列读取： HH = fread(&quot;DT.csv&quot;,select =c(&quot;v1&quot;,&quot;v4&quot;)) fread(&quot;DT.csv&quot;,dr op=&quot;v4&quot;,nrows=100) # 读取压缩文件： fread(cmd =&quot;unzip - cq myfile.zip&quot;) fread(&quot;myfile.gz&quot;) # 批量读取： c(&quot;DT.csv&quot;,&quot;FFF.csv&quot;) %&gt;% lapply(fread) %&gt;% rbindlist() ## 多个数据扛，按列表、按行合并； ## 写出数据： fwrite(DT,&quot;DT.csv&quot;) fwrite(DT,&quot;DT.csv&quot;,append =TRUE) fwrite(DT,&quot;DT.csv&quot;,sep=&quot;\\t&quot;) fwrite(setDT(list(0,list(1:5))),&quot;DT2.csv&quot;) ##将列表作为对象写出； fwrite(DT,&quot;myfile.csv.gz&quot;,compress =&quot;gzip&quot;) ## 数据链接：数据链接部分参见 # 左连接： y[x,onm= &quot;v1&quot;] # 注意是以x为左表； y[x] # 若v1是键； merge(x,y,all.x=TRUE,by=&quot;v1&quot;) ## 注意是只要加载了data.table包，将自动动用更快速的data.table::merge()，而不是R base中的merge()，二者语法相同； # 非等链接： 通常数据连接是等值链接，即匹配列的值相等，才认为匹配陈宫，再将匹配成功的其他列连接起来； 很多时候需要非等连接，相当于是按条件链接，即匹配列的值不要求必须相等，只要满足一定条件就认为是匹配成功，再将匹配成功的其他列连接进来。 2.2.3.3 3.3 Vroom包- 超大量级数据集读取 library(here) source(here(&quot;R&quot;, &quot;common.R&quot;), encoding = &quot;utf-8&quot;) ## 数据读入： patient_filepath &lt;- fs::dir_ls(here(&quot;data&quot;, &quot;preprocess&quot;, &quot;patient_clean&quot;), glob = &quot;*/*.csv&quot;) patient &lt;- vroom(patient_filepath) %&gt;% select(-&quot;...1&quot;) %&gt;% distinct() lab &lt;- vroom(lab_filepath) %&gt;% select(-&quot;...1&quot;) %&gt;% distinct() %&gt;% mutate( pbg_diff = end_pbg - start_pbg) %&gt;% replace_with_na_at(.vars = c(&quot;pbg_diff&quot;), condition = ~.x == 0) ## 数据写出： 2.2.3.4 3.3 Arrow包- 数据整洁编码读取 library(here) source(here(&quot;R&quot;, &quot;prepare and analyse data&quot;, &quot;prepare.R&quot;), encoding = &quot;UTF-8&quot;) patient &lt;- open_dataset( here(&quot;data&quot;, &quot;preprocess&quot;, &quot;lan&quot;, &quot;patient_clean&quot;)) %&gt;% collect() write_dataset( data_endpoint3, here(&quot;data&quot;, &quot;analysis&quot;, &quot;lan&quot;, &quot;endpoint3&quot;), format = &quot;parquet&quot;) 2.2.3.4.1 3.3.1 open_dataset的高阶用法： # Task 1 - join start &lt;- Sys.time() left_join( ## 使用open paraquet方法来加速数据的合并与读取 open_dataset( source = &quot;/home/data/CLAIM_HISTORY_MONTH&quot; ) %&gt;% filter(CLAIM_STATUS_TYPE_CD == &quot;PC&quot;) %&gt;% select(CH_SUBM_PROVIDER_ID, BNFT_TYPE_CD, CH_REND_AMT), open_dataset(sources = &quot;/home/data/Provider&quot;) %&gt;% select(provider_id, provider_type, benefit_description), by = c(&quot;CH_SUBM_PROVIDER_ID&quot; = &quot;provider_id&quot;)) %&gt;% collect() end &lt;- Sys.time() end - start 2.2.4 OFFICE格式及图片导出 2.2.4.1 4.1 export() library(export) ## 导出ppt&quot; setwd(&quot;c:/Users/admin/Desktop/&quot;) graph2ppt(file=&quot;effect plot.pptx&quot;, width=7, height=5) ## 导出word： graph2doc ## 导出图片： graph2bitmap graph2eps graph2jpg graph2png graph2svg graph2tif ## 导出pdf版本： graph2pdf ## 导出数据表在excle平台： table2csv table2csv2 table2excel ## 导出数据在word生成的数据表： table2doc ## 导出数据生成对应的网页： table2html ## 导出表到ppt中： table2ppt 2.2.4.2 4.2 eoiffice() ## eoiffice #### install.packages(&quot;eoffice&quot;) library(eoffice) library(ggplot2) ## 写入文件指定类型 ## # topptx 将数字写入 powerpoint 文件 # todocx 将数字写入 word 文件 # tofigure 将图形写入不同的输出格式。 ## 写入ppt和word中： plot(mtcars$mpg, mtcars$disp, col = factor(mtcars$cyl), pch = 20) topptx(p, filename = file.path(tempdir(), &quot;mtcars.pptx&quot;), width = 6, height = 4) ## 写入表格： totable(head(mtcars), filename = file.path(tempdir(), &quot;mtcars2.pptx&quot;)) ## 读取数据： ## 读取ptx和doc文件： tabs &lt;- inpptx(filename = file.path(tempdir(), &quot;mtcars.pptx&quot;), header = TRUE) ## 写入图片：可以到处pdf/png/eps等文件格式： p &lt;- ggplot(mtcars, aes(mpg, disp, color = factor(cyl))) + geom_point() tofigure(p, filename = file.path(tempdir(), &quot;mtcars.pdf&quot;)) 2.2.4.3 4.3 officer() ## officer ## library(officer) library(magrittr) # Package `magrittr` makes officer usage easier. library(ggplot2) my_doc &lt;- read_docx() #初始化一个docx , 里面不填路径使用默认模板 styles_info(my_doc) #显示信息 gg &lt;- ggplot(data = iris, aes(Sepal.Length, Petal.Length)) + geom_point() my_doc %&gt;% #可以使用magrittr方式一步步添加 body_add_par(value = &quot;Table of content&quot;, style = &quot;heading 1&quot;) %&gt;% body_add_toc(level = 2) %&gt;% body_add_break() %&gt;% body_add_par(value = &quot;dataset iris&quot;, style = &quot;heading 2&quot;) %&gt;% body_add_table(value = head(iris), style = &quot;table_template&quot; ) %&gt;% body_add_par(value = &quot;plot examples&quot;, style = &quot;heading 1&quot;) %&gt;% body_add_gg(value = gg, style = &quot;centered&quot; ) %&gt;% print(target = &quot;body_add_demo.docx&quot;) 2.2.4.4 4.4 rrtable() # install.packages(&quot;rrtable&quot;) library(rrtable) ## 自动化报表输出： ## 生成报表图片： df2flextable2( sampleData3 ,vanilla= FALSE ) ## 生成描述性统计图片： mytable2flextable( mytable(Dx~.,data=acs) ,vanilla= FALSE ) ## 生成对应统计分类的html格式： data2HTML(sampleData3) ## 导出OFFICE data2docx(sampleData3) data2pptx(sampleData3) data2pdf(sampleData3) ## 指定形式文件的格式的输出： ## 表导出ppt和word： table2pptx(ft) table2docx ## 导出图片到word或者ppt中： code2docx(plot(iris)) require(ggplot2) gg=ggplot(data=mtcars,aes(x=wt,y=mpg))+geom_point() code2docx(ggobj=gg) code2pptx(ggobj=gg) ## 读取office数据： file2docx() file2HTM() file2pdf() file2pptx 2.2.5 在R中使用sql 2.2.5.1 5.1链接数据库： #安装RODBC包 install.packages(&quot;RODBC&quot;) library(RODBC) mycon&lt;-odbcConnect(&quot;mydsn&quot;,uid=&quot;user&quot;,pwd=&quot;rply&quot;) #通过一个数据源名称（mydsn）和用户名（user）以及密码（rply，如果没有设置，可以直接忽略）打开了一个ODBC数据库连接 data(USArrests) #将R自带的“USArrests”表写进数据库里 sqlSave(mycon,USArrests,rownames=&quot;state&quot;,addPK=TRUE) #将数据流保存，这时打开SQL Server就可以看到新建的USArrests表了 rm(USArrests) #清除USArrests变量 sqlFetch(mycon, &quot;USArrests&quot; ,rownames=&quot;state&quot;) #输出USArrests表中的内容 sqlQuery(mycon,&quot;select * from USArrests&quot;) #对USArrests表执行了SQL语句select，并将结果输出 sqlDrop(channel,&quot;USArrests&quot;) #删除USArrests表 close(mycon) #关闭连接 ———————————————— 版权声明：本文为CSDN博主「悟乙己」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。 原文链接：https://blog.csdn.net/sinat_26917383/article/details/51601539 2.2.5.2 5.2 在R中执行sql命令： ## 参见： https://dept.stat.lsa.umich.edu/~jerrick/courses/stat701/notes/sql.html#sql-queries library(dplyr) library(sqldf) ## SQL版本： SQL3 = sqldf(&quot;SELECT sex, COUNT(primaryid) as Total FROM demo_all GROUP BY sex ORDER BY Total DESC ;&quot;) ## R版本： R3 = demo_all%&gt;%group_by(sex) %&gt;% summarise(Total = n())%&gt;%arrange(desc(Total)) compare(SQL3,R3, allowAll=TRUE) TRUE "],["Data-preprocessing.html", "第 3 章 数据预处理 3.1 数据抽取 3.2 数据清洗 3.3 数据重组与转换", " 第 3 章 数据预处理 该专题涉及到数据抽样、数据清洗、重组、转换及衍生及特征处理 3.1 数据抽取 样本中的缺失值分布（频率）也要与全集一致。 针对稀有事件抽样时，由于人为增加了目标事件的浓度，导致样本中的事件和非事件的比例与全集不一致，所以需要在建模过程中，对新样本使用加权。 评估抽样设计方法与下游工作链的承接关系 基于抽样数据做规模分析评估（可信度），保证后续分析的有效性 ## 方法1：within dragons &lt;- within(dragons, sample &lt;- factor(mountainRange:site)) ## 方法2： # 简单随机抽样： sample（x,n,replace=T） dplyr::sample_n((tbl, size, replace = FALSE) 参数说明：tbl数据，size选取的数据行数，replace=true/false是否替换样本（主要参数） # 分层抽样： strata（data，stratanames=NULL,size,method=c(&quot;srswor&quot;,&quot;srswr&quot;,&quot;poisson&quot;,&quot;systematic&quot;),pik,description=FALSE） # 整群抽样： cluster（data，clusteraname,size,method=c(&quot;srswor&quot;,&quot;srswr&quot;,&quot;poisson&quot;,&quot;systematic&quot;),pik,description=FALSE） 3.2 数据清洗 3.2.1 选择数据子集 根据项目方案中不同策略人群和研究终点来设计不同的取样数据子集 数据子集的确定应使用统一的函数构造体系，并且保存成对应的rds文件方便后续读取 在此步骤提取的数据子集为粗子集 3.2.2 数据去重 ## 数据去重： distinct() unique() dplyr::n_distinct() ## 删除重复数据： new_data1 &lt;- unique (data ) ---删除的为所有字段均重复的记录 ## duplicated # 使用这个函数返回的是重复元素的下表或者布尔逻辑； rattlerdups=duplicated(rattler[, c(&quot;lon&quot;, &quot;lat&quot;)]) rattler &lt;-rattler[!rattlerdups, ] ## 使用dplyr包内置的distinct()进行去重： # added to a chain of pipes (e.g. data cleaning) obs %&gt;% distinct(across(-recordID), # reduces data frame to only unique rows (keeps first one of any duplicates) .keep_all = TRUE) ## 使用 janitor来使用get_dupes()来实现复杂去重，实际上是观测重叠值； obs %&gt;% janitor::get_dupes() ## 全部列重复； obs %&gt;% janitor::get_dupes(-recordID) ##除recordID 以外的列均重复 obs %&gt;% janitor::get_dupes(name, purpose) ## 仅name和purpose 列重复； 3.2.3 缺失值处理 从业务逻辑和商业意义上，分析缺失原因，是否需要补全缺失值，应该如何做。 直接删除带有缺失值的观察对象（或称数据元组），适用于带有缺失值的数据元组比例很少的情况。 直接删除有大量缺失的变量，适用于缺失值占比超过50%，且不具备商业意义。 对缺失值进行替换，利用全集中的代表属性（众数、均值、MAX、MIN等），或者人为定义替换。 对缺失值进行赋值，用回归、决策树、贝叶斯定理等模型去预测缺失值。 #### 删除缺失： ###################################### 缺失值删除： ###################### ## 去除包含缺失值的行： na.omit 函数语法： na.omit (x) drop_na(case_id, date_onset, age) ## 删除指定缺失值的行： ## 删除包含所有含0的值或者行： row_sub = apply(nall1, 1, function(row) all(row !=0 )) nall1$.[row_sub] # 删除某些列都是na的行，借助if_all()也可以； df_dup %&gt;% filter(!if_all(where(is.numeric),is.na)) ## 使用filter()定向删除na值： linelist &lt;- filter(linelist, !is.na(case_id)) #################################### 缺失值替换 ######################## ## 重新对缺失值赋值： elmat[is.na(elmat)]= 0 ## 缺失值替换： ## replace_na()仅能针对单行进行匹配： linelist %&gt;% mutate(hospital = replace_na(hospital, &quot;Missing&quot;)) linelist %&gt;% mutate(hospital = na_if(hospital, &quot;Missing&quot;)) ## 使用mutate_at()来指定列进行条件替换： data %&gt;% mutate_at(vars(&quot;高血压&quot;, &quot;心脏病&quot;, &quot;糖尿病&quot;), ~ replace_na(., 0)) ## 全表的缺失值替换： linelist %&gt;% replace(is.na(.),0) ## 逻辑判断条件下的na值定义： linelist &lt;- linelist %&gt;% mutate(temp = replace(temp, temp &gt; 40, NA)) ## 使用forcats进行缺失值填充： mutate(hospital = forcats::fct_explicit_na(hospital)) mutate(delay_cat = forcats::fct_explicit_na(delay_cat, na_level = &quot;Missing delay&quot;)) 3.2.3.1 缺失值处理R包：ZOO RMSE &lt;- function(sim, obs){ sqrt(sum((sim-obs)^2)/length(sim)) } library(zoo) library(forecast) load(url(&quot;https://userpage.fu-berlin.de/soga/300/30100_data_sets/NA_datasets.Rdata&quot;)) plot.ts(temp.sample, main = &#39;Weather station Berlin-Dahlem&#39;, ylab = &quot;°C&quot;) plot.ts(temp.NA, ylab = expression(&quot;°C&quot;), cex.main = 0.85, type = &#39;o&#39;, cex = 0.3, pch = 16) length(is.na(temp.NA)) na.perc &lt;- round(sum(is.na(temp.NA))/length(temp.sample),3)*100 na.perc ## IMPUTING ## temp.NA.imp &lt;- na.aggregate(temp.NA, FUN = mean, as.yearmon) ## 除了na.aggregate外，还有另外的几种形式：￥￥￥￥￥￥￥￥￥ ## na.locf()： 用最近的非NA取代每个NA的通用函数。 对于每个个体，缺失的值将由该变量的最后一个观察值替换。 ## na.StructTS() 用季节卡尔曼滤波器填充NA值的泛型函数。 # na.interp() 对非季节序列采用线性插值，对季节序列进行周期性stl分解，替换缺失值。 ## ERROR ## rmse.NA &lt;- RMSE(temp.NA.imp, temp.sample) rmse.NA ## PLOTTING ## plot.ts(temp.NA.imp, ylab = expression(&quot;°C&quot;), main = &quot;na.aggregate()&quot;, cex.main = 0.85, col = &#39;red&#39;) points(temp.NA, cex = 0.3, pch = 16) text(2013.5, -10, paste(&#39;RMSE: &#39;, round(rmse.NA,4)), cex = 0.85) legend(&#39;bottomright&#39;, legend = &#39;Imputed values&#39;, lty = 1, col = &#39;red&#39;, cex = 0.65) ## IMPUTING ## temp.NA.imp &lt;- na.locf(temp.NA, fromLast = F) ## ERROR ## rmse.NA &lt;- RMSE(temp.NA.imp, temp.sample) rmse.NA ## PLOTTING ## plot.ts(temp.NA.imp, ylab = expression(&quot;°C&quot;), main = &quot;na.locf()&quot;, cex.main = 0.85, col = &#39;red&#39;) points(temp.NA, type = &#39;o&#39;, cex = 0.3, pch = 16) text(2013.5, -10, paste(&#39;RMSE: &#39;, round(rmse.NA,4)), cex = 0.85) legend(&#39;bottomright&#39;, legend = &#39;Imputed values&#39;, lty = 1, col = &#39;red&#39;, cex = 0.65) ## IMPUTING ## temp.NA.imp &lt;- na.interp(temp.NA) ## ERROR ## rmse.NA &lt;- RMSE(temp.NA.imp, temp.sample) rmse.NA ## [1] 1.982686 ## PLOTTING ## plot.ts(temp.NA.imp, ylab = expression(&quot;°C&quot;), main = &quot;na.interp()&quot;, cex.main = 0.85, col = &#39;red&#39;) points(temp.NA, cex = 0.3, pch = 16) text(2013.5, -10, paste(&#39;RMSE: &#39;, round(rmse.NA,4)), cex = 0.85) legend(&#39;bottomright&#39;, legend = &#39;Imputed values&#39;, lty = 1, col = &#39;red&#39;, cex = 0.65) 3.2.3.2 缺失值评估 参见:https://zhuanlan.zhihu.com/p/455056143 ## 常用函数: summary() str() skimr::skim dplyr::glimpse() ## 展示套叠数据格式； dplur::ff_glimpse() ## 展示数据全貌，包含缺失值和summary的汇总； ## 使用visdat: library(visdat) vis_dat(airquality) ## 使用可视化方法探索缺失值的存在： library(naniar) ggplot(airquality, aes(x = Solar.R, y = Ozone)) + geom_miss_point() - 分面查看缺失值： ggplot(airquality, aes(x = Solar.R, y = Ozone)) + geom_miss_point() + facet_wrap(~Month) + theme_dark() - 统计缺失值： pct_miss(linelist) pct_complete_case(linelist) 3.2.3.2.1 3.2.5 一致化处理 数据集中会存在某一个数据列的数据至标准不一致或命名规则不一致的情况，可以使用分列功能将不一致的数据列中的数据值进行拆分。例如将薪水7k-9k划分为最高薪资（9k）和最低薪资（7k）。 标准化处理： 对于很多模型，如线性回归、逻辑回归、Kmeans聚类等，需要计算不同特征的系数，或者计算样本距离。 这种情况下，如果不同特征的数值量级差的特别大，会严重影响系数和距离的计算，甚至这种计算都会失去意义；所以在建模前必须要做的就是要去量纲，做标准化处理。 当然有些模型是不需要做数据标准化处理的，如决策树、随机森林、朴素贝叶斯等。 字符型数据转化成数值性数据 将分类变量转为虚拟亚变量，实现one-hot编码 3.2.3.2.2 3.2.6 异常值处理 看数据是否处于异常，可以用 3σ原则，PCA，箱线图等等，至于是否要处理也要看建模的目标对于异常值的考虑。 3.2.4 一致化处理 - 数据集中会存在某一个数据列的数据至标准不一致或命名规则不一致的情况，可以使用分列功能将不一致的数据列中的数据值进行拆分。例如将薪水7k-9k划分为最高薪资（9k）和最低薪资（7k）。 ## 标准化处理： 对于很多模型，如线性回归、逻辑回归、Kmeans聚类等，需要计算不同特征的系数，或者计算样本距离。 这种情况下，如果不同特征的数值量级差的特别大，会严重影响系数和距离的计算，甚至这种计算都会失去意义；所以在建模前必须要做的就是要去量纲，做标准化处理。 当然有些模型是不需要做数据标准化处理的，如决策树、随机森林、朴素贝叶斯等。 - 字符型数据转化成数值性数据 - 将分类变量转为虚拟亚变量，实现one-hot编码 3.2.5 异常值处理 看数据是否处于异常，可以用 3σ原则，PCA，箱线图等等 ，至于是否要处理也要看建模的目标对于异常值的考虑。 3.2.5.1 1.6.1 处理零值数据 ##1.1 去除那些每个值都是零的行 ##转换思维，每个数据都为0的行，则该行的和也为0 data1=data[which(rowSums(data) &gt; 0),] head(data1) ##1.2 去除那些存在值为零的行 data2=data[which(rowSums(data==0)==0),] head(data2) ##换一种思路，将为零的值替换为NA #2.1 把０替换成NA,然后按照处理缺失值的办法， data0[data0==0]&lt;-NA data3=na.omit(data0)##去除所有含NA的行 head(data3) ##3.1 使用complete()函数自主操作 data4=data0[complete.cases(data0),] ##3.2 去除前六列含有NA的z值，可自由设置 data5=data0[complete.cases(data0[,1:6]),] ###4.1 x[is.na(x)] &lt;- 0使用该函数对NA值进行赋值 data0=read.csv(&quot;eSet.csv&quot;,row.names = 1) data0[data0==0]&lt;-NA data0[is.na(data0)]&lt;-0 ##4.2 可以把NA赋值给任何值,相互换 data0[data0==111]&lt;-NA data0[is.na(data0)]&lt;-&quot;h&quot; 3.2.6 插值填补 data %&gt;% group_by(patient_id) %&gt;% arrange(result_datetime) %&gt;% tidyr::fill(lab_fp_6,.direction =&quot;updown&quot;) 3.3 数据重组与转换 3.3.1 数据修改-数据增加与删除 3.3.1.1 数据增加： ## 添加行：dplyr::add_row() # 使用.before和.after.指定要添加的行的位置。.before = 3将新行放在当前第三行之前。默认行为是将行添加到末尾。未指定的列将留空 ( NA)。 linelist &lt;- linelist %&gt;% add_row(row_num = 666, case_id = &quot;abc&quot;, generation = 4, `infection date` = as.Date(&quot;2020-10-10&quot;), .before = 2) ## 添加新列： transform(BOD,ZIMU=(paste0(&quot;A&quot;,1:6))) ## 使用tribble()生成新的数据： ## 只需要添加一个~x1即可； df &lt;- tribble( ~x1, &quot;a,b,c&quot;, &quot;d,e,f,g&quot;) 3.3.1.2 数据合并： ## 左右列合并： # 常用包括左右链接、全链接、内链接、半链接和反链接； left_join(x,y,by) ##仅保留与左侧相对应的行 right(x,y,by) full_join(x,y,by) ## 保留全部对应的行，对不上为空值； inner_join(x,y,by) ## 保留相同对应的行，对不上删除； semi_join(x,y,by) ## 与内链接类似； anti_join(x,y,by) ## ## 多个表相互链接： 前面讲过，reduce可以实现逐步迭代的功能，因此可以使用reduce来逐步实现累计合并的功能： files = list.files(&quot;./da/&quot;,pattern = &quot;xlsx&quot;, full.names =TRUE) map(files,read_xlsx) %&gt;% reduce(full_join,by = &quot;name&quot;) ## 单个表多个sheet链接： path = &quot;./yue.xlsx&quot; map(excel_sheet(path)),~ read-xlsx(path,sheet = .x)) %&gt;% reduce(full_join, by =&quot;name&quot;) 3.3.1.3 添加分类列： #### # 生成快速随机代码 x &lt;- rep(1:14, 14) ## 1:14 按次序生成14遍 y &lt;- as.integer(gl(14, 14)) ## 这里是生成分组序列，然后再换位字符串；1到14各自14遍 age_seq = seq(from = 0, to = 90, by = 5) # 0-90,5做间隔 (f &lt;- gl(2,5, labels=c(&quot;CK&quot;, &quot;T&quot;))) # [1] CK CK CK CK CK T T T T T 3.3.1.4 数据删除： ## 方法1：base::transform() ## 删除某列： transform(BOD,demand= NULL ) ## 方法2： data$size &lt;- NULL data[[&quot;size&quot;]] &lt;- NULL data[[3]] &lt;- NULL data &lt;- subset(data, select=-size) ## tidy: rows_delete(tibble(var=&quot;AAA&quot;)) 3.3.1.5 数据分列与合并 3.3.1.5.1 数据分列与合并： ## 数据分列与合并： ## 1 数据分列： ## 1.1 一列转多列： test %&gt;% separate(.,&quot;ad_date&quot;,c(&quot;y&quot;,&quot;m&quot;,&#39;d&#39;),sep= &quot;-&quot;, remove = FALSE) %&gt;% ## 1.2 一列转一列，列内数据按分组及字符分割展开： data %&gt;% separate_rows(.,diag,sep=&quot;,&quot;) ## 1.3 列数据横向展开，独热编码： data %&gt;% separate_rows(.,diag,sep=&quot;,&quot;) %&gt;% mutate(x =1) %&gt;% spread(diag,x) %&gt;% ## 2 数据合并： ## 2.1 对应1.1 一列转多列：合并多列到一列中： df = data.frame(n, s, b) %&gt;% unite(x, c(n, s), sep = &quot; &quot;, remove = FALSE) df = data.frame(n, s, b) %&gt;% mutate(x = paste0(s,b,collapse= &quot;&quot;)) ## 2.2 对应1.2 一列数据按分组展开： df &lt;- tibble( x = 1:3,y = c(&quot;a&quot;, &quot;d_e_f&quot;, &quot;g_h&quot;), z = c(&quot;1&quot;, &quot;2&quot;, &quot;5&quot;)) separate_rows(df,y, z,convert=TRUE,sep=&quot;_&quot;) %&gt;% group_by(x,z) %&gt;% ## 注意这里不能使用mutate来构建，mutate并不会汇总数据； summarize(y= str_c(y,collapse=&quot;_&quot;)) %&gt;% ungroup() 3.3.1.5.2 字符串分列与去重（条件匹配）： ## 多分类数据的合并汇总实现方法： data &lt;- data.frame( patient_id = c(rep(1:4,each=3)), visit_id = paste0(&quot;v&quot;,c(1:12)), lab_name = rep(&quot;血肌酐&quot;,12), result_num = c(1000:1006,10:14), result_unit = c(rep(&quot;umol/l&quot;,7),rep(&quot;mg/dl&quot;,5)), sex = c(rep(c(&quot;male&quot;,&quot;men&quot;),time = 6)), age = c(sample(18:40,size = 12,replace = T)), diag = c(rep(paste(&quot;糖尿病&quot;,&quot;高血压&quot;,&quot;心脏病&quot;,sep =&quot;,&quot;),time = 4), rep(paste(&quot;糖尿病&quot;,&quot;心脏病&quot;,sep =&quot;,&quot;),time = 4), rep(paste(&quot;高血压&quot;,sep =&quot;,&quot;),time = 4)) ) data &lt;- data %&gt;% mutate(caldiag_1 = case_when( grepl(&quot;糖尿病|心脏病&quot;, diag) ~ &quot;大病&quot;, TRUE ~ &quot;0&quot; )) %&gt;% mutate(caldiag_2 = case_when( grepl(&quot;高血压&quot;, diag) ~ &quot;小病&quot;, TRUE ~ &quot;0&quot; )) ## 改进方法2： data %&gt;% separate_rows(.,diag,sep=&quot;,&quot;) %&gt;% mutate(caldiag = case_when( grepl(&quot;糖尿病|心脏病&quot;, diag) ~ &quot;大病&quot;, diag == &quot;高血压&quot; ~ &quot;小病&quot;, TRUE ~ &quot;缺失&quot; )) %&gt;% select(!diag) %&gt;% group_by(visit_id) %&gt;% summarize(y= str_c(unique(caldiag),collapse=&quot;,&quot;)) %&gt;% ungroup() %&gt;% right_join(data,by=&quot;visit_id&quot;) %&gt;% View() ## 改进方法3： ## 字符串去重：只适合全部变量均可匹配： ## 这种方法不好的原因在于全局匹配，置换后的数据行还会包含未被分类的数据集： # 类似这样：&quot;大病,终末期肾脏疾病&quot; data %&gt;% mutate( cla = str_replace_all(.$diag,c(&quot;糖尿病|心脏病&quot; = &quot;大病&quot;, &quot;高血压&quot; = &quot;小病&quot;)) %&gt;% map(., function(x) paste(unique(unlist(str_split(x,&quot;,&quot;))), collapse = &quot;,&quot;)) %&gt;% unlist() ) 3.3.1.5.3 批量重命名（列名） ## 单个变量重命名： iris %&gt;% rename(&quot;1&quot; = &quot;Sepal.Length&quot;,&quot;2&quot; =&quot;Sepal.Width&quot; ) %&gt;% names() ## 批量构造虚拟数据集进行填充后重新批量命名： out1_df = matrix(&quot;NA&quot;,ncol = length(out1),nrow = dim(lab_fp)[1]) %&gt;% data.frame() %&gt;% rename_at(vars(names(.)) ,~ out1) ## 数据列重命名： rename(flights,YEAR=year) ## 构造列名： setNames( 1:3, c(&quot;foo&quot;, &quot;bar&quot;, &quot;baz&quot;) ) # this is just a short form of tmp &lt;- 1:3 names(tmp) &lt;- c(&quot;foo&quot;, &quot;bar&quot;, &quot;baz&quot;) 3.3.2 数据值重编码（非列名） 3.3.2.1 二分类重编码 ## 方法1，构建函数： f3 &lt;- function(x) ifelse(x &gt;= 0, 1, 0) ## 方法2：使用plyr包中的revalue() 或 mapvalues() library(plyr) data$scode &lt;- revalue(data$sex, c(M = &quot;1&quot;, F = &quot;2&quot;)) data$scode &lt;- mapvalues(data$sex, from = c(&quot;M&quot;, &quot;F&quot;), to = c(&quot;1&quot;, &quot;2&quot;)) str &lt;- c(&quot;alpha&quot;, &quot;beta&quot;, &quot;gamma&quot;) mapvalues(str, from = c(&quot;beta&quot;, &quot;gamma&quot;), to = c(&quot;two&quot;, &quot;three&quot;)) ## 方法3：base-R直接赋值： data$scode[data$sex == &quot;M&quot;] &lt;- &quot;1&quot; data$category[data$control &lt; 7] &lt;- &quot;low&quot; str[str == &quot;beta&quot;] &lt;- &quot;two&quot; ## 方法4： 使用正则来进行选择匹配： str &lt;- c(&quot;alpha&quot;, &quot;beta&quot;, &quot;gamma&quot;) sub(&quot;^alpha$&quot;, &quot;one&quot;, str) # 把所有列的 &#39;a&#39; 替代为 &#39;X&#39; gsub(&quot;a&quot;, &quot;X&quot;, str) #&gt; [1] &quot;XlphX&quot; &quot;betX&quot; &quot;gXmmX&quot; ## 方法5使用if-else() # 注意这种mutate指定的方法比赋值参数更好用； df &lt;- df %&gt;% mutate(status= if_else(.$points &gt; 20, &#39;Good&#39;, &#39;Bad&#39;)) 3.3.2.2 多分类重编码 ## 方法1：使用age_categories()进行数据的多分类(连续变量) linelist &lt;- linelist %&gt;% mutate(age_cat = cut( age_years, breaks = c(0, 5, 10, 15, 20, 30, 50, 70, 100), right = FALSE, include.lowest = TRUE, labels = c(&quot;0-4&quot;, &quot;5-9&quot;, &quot;10-14&quot;, &quot;15-19&quot;, &quot;20-29&quot;, &quot;30-49&quot;, &quot;50-69&quot;, &quot;70-100&quot;)), age_cat = fct_explicit_na( age_cat, na_level = &quot;Missing age&quot;)) ## 方法2：使用cut函数进行数据多分组分类： age = c(23, 15,36,47,65,53) cut(age,breaks =c(0,18,45,100),labels = c(&quot;y&quot;,&quot;m&quot;,&quot;o&quot;)) ### 方法3使用dplyr::ntile()来实现数据的均等分类： # make groups with ntile() ntile_data &lt;- linelist %&gt;% mutate(even_groups = ntile(age_years, 10)) ### 方法4：使用sjmisc::rec()函数来匹配数据后，生成对应编码新值； library(sjmisc) df %&gt;% rec(math,rec =&quot;min:59= 不及格&quot;；60:74 =中；75:85= 良；85：max= &quot;优&quot;， append = FALSE) ## 方法5：case_when()：多条件编码； df %&gt;% mutate(sex = if_else(sex =&quot;男&quot;,&quot;M&quot;,&quot;F&quot;)) # 等于男则为M，否则为F； df %&gt;% mutate(math =case_when(math &gt;= 75~&quot;high&quot;, math &gt;= 60~&quot;middle&quot;, TRUE ~ &quot;LOW&quot;)) ## 最后一个TRUE表示所有剩余分支； 3.3.2.3 批量分类case_when/fcase： ## 批量分类方法1： linelist &lt;- linelist %&gt;% mutate(across( ## 下面这个all_of()的用法很棒！ .cols = all_of(c(explanatory_vars, &quot;outcome&quot;)), .fns = ~case_when( . %in% c(&quot;m&quot;, &quot;yes&quot;, &quot;Death&quot;) ~ 1, ## 将其分类转为1和0； . %in% c(&quot;f&quot;, &quot;no&quot;, &quot;Recover&quot;) ~ 0, TRUE ~ NA_real_) ) ) ## case_when是一种极其占有内存和浪费时间的方法： ## 优先使用data.table()中的fcase来实现条件替换,效率提高10倍： ## 与case_when()体系不同的是： 同义替换间使用逗号，而不是~； 最后的默认参数，使用default，而不是TRUE; linelist &lt;- linelist %&gt;% mutate(across( ## 下面这个all_of()的用法很棒！ .cols = all_of(c(explanatory_vars, &quot;outcome&quot;)), .fns = ~fcase( . %in% c(&quot;m&quot;, &quot;yes&quot;, &quot;Death&quot;) ,1, ## 将其分类转为1和0； . %in% c(&quot;f&quot;, &quot;no&quot;, &quot;Recover&quot;) , 0, ## 注意：default必须是一个固定的参数值，不能像case_when那样实现非过滤等价； default = NA_real_) )) 3.3.2.4 数据类型重编码 # transform函数 mtcars %&gt;% transform(cyl = factor(cyl), hp = factor(hp)) %&gt;% as_tibble() ## 指定数据形式的转换： transform(BOD,as.numeric(Time)) # mutate函数 dta %&gt;% mutate(cyl = factor(cyl), hp = factor(hp)) %&gt;% as_tibble() ## 较多变量：使用.cross() dta %&gt;% mutate(across(where(is.integer), factor)) %&gt;% is.interger() %&gt;% as_tibble() # 判断数字为整形； ## 但在实际项目中R的行列参数类型往往为double()类型，因此单纯使用上面两种类型，往往找不到对应的类型变量； # 因此可以使用整除的方法进行筛选变量： dta %&gt;% mutate(across(function(col) all(col %% 1==0), factor)) %&gt;% as_tibble() ## 即整数除以1后的余数为0 ## 补充一种时间序列的处理方法： mutate(across(.cols = where(is.POSIXct), .fns = as.Date)) ## 自定义高效函数重新编码： tr2 = function(data,aim_col,funss){ ## 将变量解析为字符串： aim_cha = deparse(substitute(funss)) ## 使用bangbang来运行解析式： if(!!aim_cha == &quot;num&quot;){ ## 使用{{}}来纳入变量参数： return( data %&gt;% mutate(across(.cols = {{aim_col}}, .fns = as.numeric))) }else if(!!aim_cha == &quot;dat&quot;){ return( data %&gt;% mutate(across(.cols = {{aim_col}}, .fns = as_date))) }else if(!!aim_cha == &quot;cha&quot;){ return( data %&gt;% mutate(across(.cols = {{aim_col}}, .fns = as.character))) }else if(!!aim_cha == &quot;fac&quot;){ return( data %&gt;% mutate(across(.cols = {{aim_col}}, .fns = as.factor))) }else{&quot;input function fasle&quot;}} iris %&gt;% tr2(.,Species,fac) ## 对于多参数模型，还不能很好的纳入across中.col这种方法； iris %&gt;% tr(.,c(&quot;Sepal.Length&quot;,&quot;Sepal.Width&quot;),num) 3.3.2.5 因子编码调整 3.3.2.5.1 基于forcats包的分类 # base R mutate(pbg_cat = factor( pbg_cat, levels = c(&quot;[0, 7.8)&quot;, &quot;[7.8, 10.0)&quot;, &quot;[10.0, 11.1)&quot;, &quot;[11.1, 12.0)&quot;, &quot;[12.0, +)&quot;))) # 调整分组排序： mutate(delay_cat = fct_relevel(delay_cat, &quot;&lt;2 days&quot;, &quot;2-5 days&quot;, &quot;&gt;5 days&quot;)) # 添加额外分组标化： mutate(delay_cat = fct_expand(delay_cat, &quot;Not admitted to hospital&quot;, &quot;Transfer to other jurisdiction&quot;)) # 删除分组： mutate(delay_cat = fct_drop(delay_cat)) ## # 分组重编码： mutate(delay_cat = fct_recode( delay_cat, &quot;Less than 2 days&quot; = &quot;&lt;2 days&quot;, &quot;2 to 5 days&quot; = &quot;2-5 days&quot;, &quot;More than 5 days&quot; = &quot;&gt;5 days&quot;)) ## fct_recode重编码的另外一种形式： meldata &lt;- meldata %&gt;% mutate(sex.factor = # Make new variable sex %&gt;% # from existing variable factor() %&gt;% # convert to factor fct_recode( # forcats function &quot;Female&quot; = &quot;0&quot;, # new on left, old on right &quot;Male&quot; = &quot;1&quot;) %&gt;% ff_label(&quot;Sex&quot;), # Optional label for finalfit # same thing but more condensed code: ulcer.factor = factor(ulcer) %&gt;% fct_recode(&quot;Present&quot; = &quot;1&quot;, &quot;Absent&quot; = &quot;0&quot;) %&gt;% ff_label(&quot;Ulcerated tumour&quot;), status.factor = factor(status) %&gt;% fct_recode(&quot;Died melanoma&quot; = &quot;1&quot;, &quot;Alive&quot; = &quot;2&quot;, &quot;Died - other causes&quot; = &quot;3&quot;) %&gt;% ff_label(&quot;Status&quot;)) ## 合并分组后重编码： # 新生成变量命名为new，列内将原来Species中的setosa和virginica修改命名为newss；暂时不知道其底层逻辑是多少，但可能是一种便捷的处理方法： iris %&gt;% mutate(new = fct_collapse(Species, newss = c(&quot;setosa&quot;,&quot;virginica&quot;)) ) %&gt;% View() 3.3.2.5.2 批量标签分类调整： ## 批量分类方法2： expss::apply_labels(fbg_diff = &quot;空腹血糖变化差值&quot;, fbg_reg = &quot;末次空腹血糖&lt;6.1m/mol&quot;, age = &quot;年龄&quot;, sex = &quot;性别&quot; ) ## 批量分类方法3： visit_clean &lt;- visit %&gt;% mutate( insurance_type = recode(insurance_type, is.na = &quot;医保类型缺失&quot;)) ## 批量修改重分类方法4： tmp$group &lt;- plyr::mapvalues(tmp$group, from = c(&quot;GFP- early&quot;,&quot;GFP+ early&quot;,&quot;GFP- late&quot;,&quot;GFP+ late&quot;), to = c(&quot;HhOFF early&quot;, &quot;HhON early&quot;, &quot;HhOFF late&quot;, &quot;HhON late&quot;)) tmp$group &lt;- factor(tmp$group, levels = c(&quot;HhOFF early&quot;, &quot;HhON early&quot;, &quot;HhOFF late&quot;, &quot;HhON late&quot;)) 3.3.2.5.3 高效标签编码datapasta ## datapasta 提供了一种范式，可以将变量名按照固定格式输出到代码块中，方便粘贴使用： install.packages(&quot;datapasta&quot;) library(datapasta ) iris %&gt;% distinct(Species) %&gt;% arrange(Species) %&gt;% pull() %&gt;% datapasta::vector_paste_vertical() ## 纵向输出，同时将代码结果掩盖： c(&quot;setosa&quot;, &quot;versicolor&quot;, &quot;virginica&quot;) ## 横向输出，同时将代码结果掩盖： iris %&gt;% distinct(Species) %&gt;% arrange(Species) %&gt;% pull() %&gt;% datapasta::vector_paste() c(&quot;setosa&quot;, &quot;versicolor&quot;, &quot;virginica&quot;) ## 借助上面的函数实现高效的case_when之类的数据替换： ## 数据替换后：可以借助Rstudi0的快捷键使用批量多项选择： Mac 上的 Opt 键 + 鼠标左键拖动 Windows 上的 Alt 键 + 鼠标左键拖动 # 这个过程简单来说，就是按住alt+左键之后，批量选择纵向输出列，然后对这些列进行批量代码替换和处理。 3.3.2.6 构造哑变量 library(caret) library(ISLR) dummies &lt;- dummyVars(~League+Division+NewLeague, data = Hitters) dummies &lt;- predict(dummies, newdata = Hitters) head(dummies) 3.3.3 数据塑形 3.3.3.1 数据排序 &gt; # 第一列升序，然后是第三列降序 &gt; r2 = iris[order(iris[,1],-iris[3]),] &gt; # 第一列升序，然后是第三列升序序 &gt; r2 = iris[order(iris[,1],iris[3]),] ## 或者使用dplyr::arrange() library(dplyr) data(&quot;iris&quot;) head(iris) # 第一列升序，然后是第三列升序 arrange(iris,iris[,1],iris[,3]) # 第一列升序，然后是第三列降序 arrange(iris,iris[,1],-iris[,3]) # # 如果列名不止一个，会将后面的列在前面列的基础上进行排序 arrange(flights,year,month,day) # min_rank()，从小到大排名；需逆向排序的话，加一个desc() df %&gt;% mutate(rank =min_rank(desc(math))) %&gt;% arrange(ranks) ### desc 逆排序： df_dup() %&gt;% arrange(math,sex,decreasing=TRUE) ## 反向排序 ## 数据列位置的位移 # select()与everything()连用可以将某几列移到数据库的开头 select(flights,day,dep_time,everything()) 3.3.3.2 行列互转 ## 宽表格变长表格：gather(),melt(),pivot_longer() ## 长表格变宽表格：spread(),cast(),pivot_wider() ####################### 宽表变长表： #################### ## 宽矩阵转为长矩阵： reshape2::melt() ## 宽表变长表： df_q %&gt;% pivot_wider(names_from =q,values_from =math_qs,names_prefix =&quot;q_&quot;) ###################### 长表变宽表 ########################## ### 长表变宽表； pivot_longer(cols = starts_with(&quot;malaria_&quot;), names_to = &quot;age_group&quot;,values_to = &quot;counts&quot;) ## cols:用选择列语法选择要变形的列； # names_to:为存放变形列的列名中的值，指定新列名； # values_to:为存放变形列中的值，指定新列名； ## 比较gather和spread的用法：均是tidyr包的函数； library(tidyr) data2 &lt;- gather(data, category, value, X, Y, Z) # 上面代码是指将data中x、y、z列值转为以分类数据行为基础的数据长矩阵，其中x、y、z原有的值转为列值； spread的用法与gather相反，是将长矩阵转为宽矩阵： gather(data, category, value, -time) ## 这里的-time是指被忽略的那一列； ## 参见案例如下： https://www.cnblogs.com/jialinliu/p/15228201.html 3.3.3.3 列表转数据框 3.3.3.3.0.1 列表常用处理 # 代码细节： as.list() # 将数据进行分为若干个列表； is.list() # 将数据收纳为一个列表； ## 删除列表的子表： y &lt;- list(a = 1, b = 2) y[&quot;b&quot;] &lt;- list(NULL) str(y) ## 列表的赋值定义方法： x &lt;- c(&quot;m&quot;, &quot;f&quot;, &quot;u&quot;, &quot;f&quot;, &quot;f&quot;, &quot;m&quot;, &quot;m&quot;) lookup &lt;- c(m = &quot;Male&quot;, f = &quot;Female&quot;, u = NA) lookup[x] #3 删除对应的属性： unname(lookup[x]) ## 列表调整内部子集的顺序： ## 前提是设计 Freedom &lt;- c(1, 2, 3, 2, 1, 2) Equality &lt;- c(2, 3, 1, 1, 2, 1) TypeCountry &lt;- c(&quot;South&quot;, &quot;East&quot;, &quot;East&quot;, &quot;North&quot;, &quot;South&quot;, &quot;West&quot;) Example &lt;- list(Freedom, Equality, TypeCountry) names(Example) &lt;- c(&quot;Freedom&quot;, &quot;Equality&quot;, &quot;TypeCountry&quot;) Ex &lt;- Example[c(&quot;TypeCountry&quot;,&quot;Freedom&quot;, &quot;Equality&quot;)] 3.3.3.3.0.2 列表与purrr # list_modify()有很多用途，其中之一可以是删除列表元素 # keep()保留指定给 的元素.p =，或者提供给 的函数的.p =计算结果为 TRUE # discard()删除指定给 的元素.p，或者提供给的函数.p =计算结果为 TRUE的元素 # compact()删除所有空元素 ## 举例： combined %&gt;% list_modify(&quot;Central Hospital&quot; = NULL) # remove list element by name # keep only list elements with more than 500 rows combined %&gt;% keep(.p = ~nrow(.x) &gt; 500) # Discard list elements that are not data frames ## 丢弃列表中非数据框的类型数据； combined %&gt;% discard(.p = ~class(.x) != &quot;data.frame&quot;) # keep only list elements where ct_blood column mean is over 25 # 深入列表进行计算和筛选： combined %&gt;% discard(.p = ~mean(.x$ct_blood) &gt; 25) 3.3.3.4 日期数据重塑 查看本地环境下的时区设计： ## 查看时区： Sys.timezone(location = TRUE) 3.3.3.4.1 3.4.1 lubridate install.packages(&quot;lubridate&quot;) library(lubridate) today() ## 查看年月日； now() ## 最详细的日期； ## 时间修正形式： ymd(&quot;20110604&quot;) mdy(&quot;06-04-2011&quot;) dmy(&quot;04/06/2011&quot;) lubridate::as_datetime(&quot;2012.04/1612:00:05&quot;) lubridate::as_datetime(44677.4375*24*60*60, origin = as.POSIXct(&quot;1899-12-30 00:00:00&quot;,tz = &quot;UTC&quot;)) #[1] &quot;2022-04-26 10:30:00 UTC&quot; ## 提取信息： ## 使用下面这些函数直接提取对应数据中参数值： # minute、hour、day、wday、yday、week、month、year arrive &lt;- ymd_hms(&quot;2011-06-04 12:00:00&quot;, tz = &quot;Pacific/Auckland&quot;) second(arrive) &lt;- 25 ## 修改时间： ymd(20110101) + dyears(1) #&gt; [1] &quot;2012-01-01 06:00:00 UTC&quot; ymd(20110101) + years(1) #&gt; [1] &quot;2012-01-01&quot; 3.3.3.4.2 3.4.2 base-r-date ## 规整日期； as.Date(&quot;2012/12/14&quot;) [1] &quot;2012-12-14&quot; as.Date(index_date, format=&#39;%Y-%m-%d&#39;), as.Date(admission_dt, format=&#39;%Y-%m-%d&#39;),units=&#39;days&#39;))))) # 组建时间： make_data(2020,8,27 ) # 输出&quot;2020-08-27&quot; make_datatime(2020,8,27,21,27,15) 2020-08-27 21:27:15 UTc # 还可以使用format来构建函数： d = make_data(2020,3,5) format(d ,&quot;%Y/%m/%d&quot;) ## 时间序列分析： ts(data, start =1, end, frequency =1, ...)函数： 其中data为数值向量或矩阵；start设置起始时刻；end设置结束时刻； frequency设置时间频率，默认为1；当填入4时表示季度；填入12表示月份； 例如： ts(data =1:10,start =2010,frequency =4) 3.3.3.4.3 3.4.3 计算时间差： # 提取时间差： &gt; a &lt;- &#39;2016-06-02 23:29:00&#39; &gt; b &lt;- &#39;2016-06-05 03:24:00&#39; &gt; as.double(difftime(b,a)) [1] 2.163194 &gt; as.double(difftime(b,a,units=&quot;weeks&quot;)) [1] 0.3090278 #还可以指定units &gt; as.double(difftime(b,a,units=&quot;weeks&quot;), units = &#39;days&#39;) [1] 2.163194 ## 计算时间差的其他方法： inpatient_time = as.integer(discharge_datetime - admission_datetime) ## 计算时间差： as.period(interval(birthDate, refDate), unit = &#39;year&#39;)$year ## 计算时间差： as.numeric(time_length(interval(index_date, as.Date(&quot;2021-12-31&quot;)), &#39;month&#39;)) 3.3.4 数据提取 3.3.4.1 字符串提取 ## 查看字符的正则类型： ## 查看字符的正则类型： ## 正则表达式： x &lt;- c(&quot;New theme&quot;, &quot;Old times&quot;, &quot;In the present theme&quot;) str_view(x, &quot;the&quot;) 3.3.4.1.1 字符串统计与转换： ## R语言字符串处理： ################# 统计字符串长度： ######################## library(stringr) x &lt;- c(&#39;abcd&#39;, 1379, &#39;偷闲阁&#39;, NA) nchar(x) nchar(x, type = &#39;bytes&#39;) nchar(x, keepNA = F) nzchar(x) str_length(c(&quot;a&quot;, &quot;bc&quot;, &quot;def&quot;, &quot;北京&quot;)) 3.3.4.1.2 字符串连接、构建、删除： ###################### 字符串 连接 ######################## ## base -R - 连接； paste() 函数； rep() ## 字符连接： library(stringr) str_c(c(&quot;x&quot;, &quot;y&quot;), c(&quot;a&quot;, &quot;b&quot;), sep=&quot;*&quot;) str_c(&quot;data&quot;, 1:3, &quot;.txt&quot;) ## 类似与paste0 str_c(&quot;x&quot;,1:3,sep = &quot;&quot;,collaspse = &quot;_&quot;) # 输出结果为：&quot;x1_x2_x3&quot; ## 字符连接： paste0(&quot;hh&quot;,&quot;123&quot;) output_file = stringr::str_glue(&quot;outputs/Report_{Sys.Date()}.docx&quot;) ########################## 字符构建： ######################## str_dup(c(&quot;a&quot;,&quot;b&quot;),c(3,2)) # 输出为 [1] &quot;aaa&quot; &quot;bb&quot; ## 使用正则匹配来实现字符串构建：interesting！ sprintf(&quot;tour%03d.jpg&quot;, c(1, 5, 10, 15, 100)) ## 字符串中插入变量值： name &lt;- &quot;李明&quot; tele &lt;- &quot;13512345678&quot; str_glue(&quot;姓名: {name}\\n电话号码: {tele}\\n&quot;) str_glue_data(list(name = &quot;王五&quot;, tele = &quot;13500000000&quot;), &quot;姓名: {name}&quot;, &quot;电话号码: {tele}&quot;, .sep=&quot;; &quot;) ######################## 字符串删除 ########################## ## 去掉空白字符串： tt[nchar(x)&gt;0] ## 移除字符集中的空格： str_trim(c(&quot;a &quot;,&quot;b &quot;, &quot;a b&quot;)) # [1] &quot;a&quot; &quot;b&quot; &quot;a b&quot; ## 选项which=&quot;left&quot;可以仅删去开头的空格， 选项which=&quot;right&quot;可以仅删去结尾的空格。 trimws(c(&quot; 李明&quot;, &quot;李明 &quot;, &quot; 李明 &quot;, &quot;李 明&quot;), which=&quot;left&quot;) 3.3.4.1.3 字符位置排序与检索 ##################### 字符排序 ################ str_sort(x ,decreasing,locale , ....) # locale可指定语言，默认为en，可变为&quot;ch&quot;。并且这种排序方法会改变字符串的序列，按照汉语拼音的方法； x= c(&quot;banana&quot;,&quot;apple&quot;,&quot;pear&quot;) str_sort(x) # [1] &quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot; ##################### 字符位置查询 ################# ### 主要有grepl()和grep()函数： x &lt;- c(&#39;describe&#39;, &#39;the&#39;, &#39;city&#39;, &#39;you&#39;, &#39;live&#39;, &#39;in&#39;) ## 字符检索grep--返回下标数字： grep(&#39;i&#39;, x) ## 检索下标 [1] 1 3 5 6 grep(&#39;i&#39;, x, invert = T) # 返回不匹配的选项； grep(&#39;i&#39;, x, value = T) # 返回检索值； ## 字符检索grepl-- 返回逻辑值： grepl(&#39;i&#39;, x) -- true/false startsWith(c(&quot;xyz123&quot;, &quot;tu004&quot;), &quot;tu&quot;) # 条件判断并范湖逻辑值； ## 字符检索regexpr- regexec： ## 字符返回位置：结果包含了匹配的具体位置和字符串长度信息 text &lt;- c(&quot;Hellow,Adam!&quot;, &quot;Hi, Adam!&quot;) regexpr(&quot;Adam&quot;, text) ## 返回具体位置 gregexpr(&quot;Adam&quot;, text) # 下面两个返回的结果是相似； regexec(&quot;Adam&quot;, text) ## 基于string包中的str_（）相关匹配函数： str_detect(string, pattern, negaate =FALSE) 检测是否存在匹配； str_which(string,pattern,negate = FALSE) _查找匹配的索引； str_count(string, pattern) ：计算匹配的次数 str_locate(string, pattern) ：定位匹配的位置，返回end和start； str starts( string, pattern) : 检测是否以pattern开头 str_ends(string, pattern) : 检测是否以pattern结尾 string:为要检测的字符串 pattern:为匹配的模式,可以是正则表达式 negate:默认为 FALSE表示正常匹配,若为TRUE则反匹配(找不匹配)。 3.3.4.1.4 字符串转换/替换： ######################## 字符串 转换 ############################### ## 大小写转换： tolower() toupper() # 批量将字符串转为数值型数据： xh2 &lt;- as.data.frame(lapply(xh[,1:2],as.numeric)) %&gt;% data.frame(.,xh[,3]) ## 批量构建循环体中的字符串： explanatory_vars %&gt;% str_c(&quot;outcome ~ &quot;, .) ## 字符串格式化输出：将字符串转为变量名： str_splot_fixed(string,pattern ,n) # 返回矩阵，n控制行数； ######################## 字符串 替换 ############################## ### 字符筛选后替换：chartr DNA &lt;- &quot;AtGCtttACC&quot; chartr(&quot;Tt&quot;,&quot;Bb&quot;,DNA) ## 转为小写 chartr(&quot;Tt&quot;,&quot;BB&quot;,DNA) ## 转为大写 chartr(&quot;!;&quot;, &quot;.,&quot;, c(&quot;Hi; boy!&quot;, &quot;How do you do!&quot;)) ## 多条件替换； ## 字符查询位置后替换：base-sub、gsub ## sub 只做一次检索替换： text&lt;-c(&quot;Hello, Adam&quot;,&quot;Hi,Adam!Adam!&quot;,&quot;How are you,Ava&quot;) sub(pattern=&quot;Adam&quot;,replacement=&quot;word&quot;,text) ##在该输出结果中&quot;Hi,word!Adam!&quot; ## gsub 遇到即发生替换 gsub(pattern=&quot;Adam|Ava&quot;,replacement=&quot;word&quot;,text) [1] &quot;Hello, word&quot; &quot;Hi,word!&quot; &quot;How are you,word&quot; ## 字符查询位置后替换：string-str_sub: s &lt;- &quot;term2017&quot; str_sub(s, 5, 8) &lt;- &quot;18&quot; ## 返回&quot;term18&quot; ## substr替换 ## substr(x,from,to) hh &lt;- c(&quot;ahbdhfdf&quot;) substr(hh,2,4) &lt;- &#39; jjj&#39; ## 字符串提取后替换： str_replace(string,pattern,replacement) str_replace(x,&quot;-&quot;,&quot;/&quot;) # 将-转为/ 3.3.4.1.5 字符串拆分/提取： ######################## 拆分 ################### ### base-R: strsplit(x, split, fixed = FALSE, perl = FALSE, useBytes = FALSE) ## 注意： 1、split为特殊字符时，使用“[$]”来进行分割； 2、strsplit分割后，split作为分割字符串不再保留； ### library(stringr) str_split(string, pattern, n = Inf, simplify = FALSE) ## 返回列表，与strsplit类似； str_splot_fixed(string,pattern ,n) # 返回矩阵，n控制行数； ## 拆分数字并提取： drug_spec_unit = as.numeric(readr::parse_number(drug_spec_unit)) ######################## 提取 ####################### ## 提取string - substr/str_sub substr/(x, start, stop) ## 给定范围提取： substring(text, first, last = 1000000L) &gt; x &lt;- &quot;123456789&quot; str_sub(x,5,8) ## 5678 &gt; substr(x, c(2,4), c(4,5,8)) [1] &quot;234&quot; &gt; substring(x, c(2,4), c(4,5,8)) [1] &quot;234&quot; &quot;45&quot; &quot;2345678&quot; ## 基于str_split() str_split(string,pattern) ## 返回列表； str_splot_fixed(string,pattern ,n) # 返回矩阵，n控制行数； ## 基于正则提取： # 提取所有中文字符： words_vec &lt;- str_extract_all(poem_autumnwindow, &quot;[[:alpha:]]&quot;)[[1]] head(words_vec) # 提取数字： s &lt;- c(&quot;10-0.16-1700.0-42.csv&quot;, &quot;12-0.22-1799.1.csv&quot;) pat &lt;- &quot;[0-9]+[.][0-9]+|[0-9]+&quot; s1 &lt;- str_match_all(s, pat); s1 ## 正则匹配提取：str_extract()和str_match() str_extract(string,pattern) :只提取匹配的内容； str_match(string,pattern) ：提取匹配的内容以及各个分组捕获，返回结果矩阵； c(&quot;1978-2000&quot;,&quot;2011-2020-2099&quot;) pat&lt;-&quot;\\b(19|20)([e-9]{2})\\b&quot; #正则表达式 str_extract(x, pat) ##[1]&quot;1978&quot;&quot;2011&quot; str_match(x, pat) [,1] [,2] [,3] ##[1,]&quot;1978&quot;&quot;19&quot;&quot;78&quot; ##[2,]&quot;2011&quot;&quot;26&quot;&quot;11 ## 利用正则仅提取数据： str_match_all( &quot;[0-9]+[.][0-9]+|[0-9]+&quot;) 3.3.4.2 列批量提取 ## 快速数据框处理： library(dplyr) ## 使用select函数筛选： ############################### select选择特定的列； select(flights,year:day) # select()函数可以搭配使用一些辅助函数： starts_with(“abc”)：匹配以abc开头的名称 end_with(abc)：匹配以abc结尾的名称 contains(“xyz”)：匹配含xyz的名称 matches(“”) num_range(“x”，1：3)：匹配x1，x2，x3 # .col用于指定列，.fns用于对指定列进行函数运算； across(.col =everything(),.fns =NULL,...,.names) df %&gt;% ## 选定所有列，进行归一化； mutate(across(where(is.numeric),rescale)) # select()与everything()连用可以将某几列移到数据库的开头 select(flights,day,dep_time,everything()) ## select()函数的其他用法： select(name, sex&gt;0.6 &amp; math&lt; 0.9) 3.3.4.3 条件筛选提取 3.3.4.3.1 使用subset、filter、mutate过滤： ## base R : tt = tt[,-1] tt2 = subset(data_set, select = -c(val1)) ### filter() 根据值或条件筛选行： # filter() ::按数据需求，行或者列进行筛选； # 如果想要过滤得到11月与12月出发的所有航班 filter(flights,month == 11 | month == 12) ## 注意这里的 “|”，表示并且的意思； df_dup %&gt;% filter(sex == &quot;男&quot;，math &gt;80) # 多条件筛选； df_dup %&gt;% filter(sex == &#39;女&#39;，is.na(english) | math&gt;80)) # 闭区间筛选： df_dup %&gt;% filter(between(math,70,80)) # 指定列范围内根据条件筛选： df[,4:6] %&gt;% filter(across(everything()),~ .x&gt;75) df[,4:6] %&gt;% filter(if_any(where(is.numeric)),~ .x&gt;75) # 上述两公式等价； df[,4:6] %&gt;% filter(across(everything()),~ !is.na(.x)) df[,4:6] %&gt;% filter(if_any(where(is。character)),is.na) (# 根据包含指定值筛选行) df[,4:6] %&gt;% filter(across(everything()),~str_detect(.x,&quot;bl&quot;))) ## 按行号过滤数据： linelist %&gt;% filter(row_number() == 5) ## 除了使用filter进行过滤外，还可以使用mutate()函数进行过滤： ## 例如： iris %&gt;% mutate(test = fcase(iris$Sepal.Length&gt;5,1, default = 2), iris$Species == &#39;virginica&#39; ) 3.3.4.3.2 取交并补 ## 集合运算(或者说向量比较运算) dplyr::intersect(x, y) ## 返回x和y共同包含的观测 dplyr::union(x,y) ## 返回x和y中所有的唯一观测； dplyr::setdiff(x,y) ## 返回在x中但不在y中的观测 3.3.4.3.3 嵌套构建与提取 3.3.4.3.3.0.1 嵌套构建 library(tidyr) df &lt;- tibble( ## 构建一种嵌套数据集：用metadata来嵌套数据内部： character = c(&quot;Toothless&quot;, &quot;Dory&quot;), metadata = list( list( species = &quot;dragon&quot;, color = &quot;black&quot;, films = c( &quot;How to Train Your Dragon&quot;, &quot;How to Train Your Dragon 2&quot;, &quot;How to Train Your Dragon: The Hidden World&quot; ) ), list( species = &quot;blue tang&quot;, color = &quot;blue&quot;, films = c(&quot;Finding Nemo&quot;, &quot;Finding Dory&quot;) ) ) ) ## 使用hoist来提取数据中参数，并指定或创建新的列名； # 其中有趣的点是1L或3L这种形式，表示的是从list表中提取对应参数位置； df %&gt;% hoist(metadata, &quot;species&quot;, first_film = list(&quot;films&quot;, 1L), third_film = list(&quot;films&quot;, 3L) ) %&gt;% View() character species first_film third_film metadata &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;list&gt; 1 Toothless dragon How to Train Y~ How to Tr~ &lt;named list&gt; 2 Dory blue tang Finding Nemo NA &lt;named list&gt; 3.3.4.3.3.1 嵌套提取 ## 第一种方法：使用map_depth实现指定提取深度； testlist2 %&gt;% map_depth(-2, &#39;veggie&#39;) ## 使用`[`和`[[`做深层提取：可以通过?`[`查询； &gt; test &lt;- list(a=1:10,b=letters[1:10]) &gt; test $a [1] 1 2 3 4 5 6 7 8 9 10 $b [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &gt; &quot;[[&quot;(test,1) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 &gt; &quot;[[&quot;(test,2) #&gt; [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &gt; &quot;[&quot;(test,1) #&gt; $a [1] 1 2 3 4 5 6 7 8 9 10 &gt; &quot;[&quot;(test,2) #&gt; $b [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; ## `[`和`[[`与lapply结合来提取深层列表元素： lapply(x, &#39;[[&#39;, VarNames[[type]]) 3.3.4.3.4 行筛选 ## 行筛选： data.frame() %&gt;% filter(!row_number() == 1) ## 上一行和下一行的值进行条件筛选： ## lag是访问当前行之前的行：lead是访问当前行的下一行： data %&gt;% mutate(trend = ifelse(newdosage &gt; lead(newdosage),&#39;lower&#39;,&#39;stable&#39;)) %&gt;% replace_na(list(trend = &quot;stable&quot;)) %&gt;% ungroup() %&gt;% distinct() ## 批量行值筛选： # 在这里是获取每个患者每个变量值的最大值； data %&gt;% group_by( patient_id) %&gt;% mutate(across(.col=c(&quot;xjgs&quot;,&quot;xjgs_one&quot;,&quot;xjgs_more&quot;,&quot;cz&quot;,&quot;cz_one&quot;,&quot;cz_more&quot;,&quot;gzdm&quot;,&quot;bwdxjt&quot;,&quot;xlsj&quot;,&quot;qxxxzb&quot;), max,.names = &quot;{.col}&quot;)) "],["Model-building.html", "第 4 章 构建模型 4.1 描述统计模型 4.2 复杂统计模型 4.3 临床统计模型 4.4 构造模型的辅助函数或参数 4.5 深度学习 4.6 机器学习", " 第 4 章 构建模型 4.1 描述统计模型 4.1.1 快速构建描述性统计 4.1.1.1 summarise/across ### 关键的两个函数： ## summarise() 支持分组后统计，内部参数较为麻烦，可以直接在help中检索summarise（） ## 指定函数构造统计： summary_table &lt;- linelist %&gt;% # group_by(hospital) %&gt;% # summarise( # cases = n(), # delay_max = max(days_onset_hosp, na.rm = T), # delay_mean = round(mean(days_onset_hosp, na.rm=T), digits = 1), # delay_sd = round(sd(days_onset_hosp, na.rm = T), digits = 1), # delay_3 = sum(days_onset_hosp &gt;= 3, na.rm = T), # pct_delay_3 = scales::percent(delay_3 / cases) # ) summary_table # print ## 条件统计： ## 形式1： 在内部筛选： linelist %&gt;% group_by(hospital) %&gt;% summarise( max_temp_fvr = max(temp[fever == &quot;yes&quot;], na.rm = T), max_temp_no = max(temp[fever == &quot;no&quot;], na.rm = T) ) ## 形式2：在外部筛选： mtcars %&gt;% mutate(model=rownames(mtcars), vs=ifelse(vs==0, &quot;vshaped&quot;, &quot;straight&quot;), am=ifelse(am==0, &quot;auto&quot;, &quot;manual&quot;), across(c(&quot;cyl&quot;, &quot;gear&quot;), factor), .before=1) %&gt;% as_tibble() ## 形式3：直接条件匹配： mtcars %&gt;% group_by(cyl) %&gt;% summarise_mean(where(is.numeric)) # 形式4： 指定字符后匹配，与形式1相似： df %&gt;% group_by(class,sex ) %&gt;% summarise(across(contains(&quot;h&quot;),mean,na.rm= TRUE)) ## 结合across()构造批处理- 支持指定列和批函数： 支持批量处理列数据，支持.col和.fun() 具体参考： https://www.tidyverse.org/blog/2020/04/dplyr-1-0-0-colwise/ df %&gt;% group_by(g1, g2) %&gt;% summarise(across(a:d, mean)) df %&gt;% group_by(g1, g2) %&gt;% ## 注意这里的n= n()是内置的count()函数； summarise(across(where(is.numeric), mean),n= n()) # summarise和across()的完整形式： linelist %&gt;% group_by(outcome) %&gt;% summarise(across(.cols = c(age_years, temp, wt_kg, ht_cm), .fns = mean, na.rm=T)) ## 对同一数据指定多组函数： df_grp = df %&gt;% group_by(class) %&gt;% group_by(class) %&gt;% summarise(across(where(is.numeric), list(sum=sum,mean =mean,min= min),na.rm=TRUE)) df_grp 4.1.1.2 mutate/across ## mutate和 across 常用案例说明：添加新列 ## 指定条件后添加函数： df %&gt;% mutate_if(is.numeric, mean, na.rm = TRUE) df %&gt;% mutate(across(where(is.numeric), mean, na.rm = TRUE)) df %&gt;% mutate_at(vars(x, starts_with(&quot;y&quot;)), mean, na.rm = TRUE) df %&gt;% mutate(across(c(x, starts_with(&quot;y&quot;)), mean, na.rm = TRUE)) ## 利用.col和.fus 来构造函数批处理体： linelist &lt;- linelist %&gt;% mutate(across(.cols = c(temp, ht_cm, wt_kg), .fns = as.character)) ## 指定所有条案后批函数处理： df %&gt;% mutate_all(mean, na.rm = TRUE) df %&gt;% mutate(across(everything(), mean, na.rm = TRUE) 4.1.2 高级分组统计 4.1.2.1 数据分组统计： 4.1.2.1.1 基础分组统计 ## 基于base R的数据分组统计： data_split = split(iris[,1:3],iris$Species) ## 基于 base with by的分组统计： with(UScrime,by(Prob,So,median)) ## 基于tapply() (f &lt;- gl(2,5, labels=c(&quot;CK&quot;, &quot;T&quot;))) &gt; tapply(x, f, sum) CK T 15 40 ## 将数据框分组按列表转化： ir &lt;- iris %&gt;% group_by(Species) ## 将数据按分类数据分组 group_split(ir) ## 将分组数据转为各类列表 ## 基于函数来使用tidy的快速分组处理数据： hh %&gt;% subset(.,aa==&#39;种子&#39; ) %&gt;% group_by(cc,dd) %&gt;% summarise(mfv = mean(hh)) %&gt;% data.frame() -&gt;zz ## MAP映射运算 mtcars %&gt;% split(.$cyl) %&gt;% ## 这一步是构建分组函数来讲所有数据分组后构建数据框； map(~ lm(mpg ~ wt,data= .x)) # 构建函数体； ### 分组数据使用tally()计数; df %&gt;% group_by(math_level =cut(math,breaks =c(0,60,75,80,100), right =FALSE)) %&gt;% tally() 4.1.2.1.2 stby/分组进阶统计 library(summarytools) ##参见：https://cran.r-project.org/web/packages/summarytools/vignettes/introduction.html#header ## 分组统计： (iris_stats_by_species &lt;- stby(data = iris, INDICES = iris$Species, FUN = descr, stats = &quot;common&quot;, transpose = TRUE)) ## 添加with的分组统计： with(tobacco, stby(data = BMI, INDICES = age.gr, FUN = descr, stats = c(&quot;mean&quot;, &quot;sd&quot;, &quot;min&quot;, &quot;med&quot;, &quot;max&quot;)) ) ## 使用group_by()的分组统计： library(dplyr) tobacco$gender %&lt;&gt;% forcats::fct_explicit_na() tobacco %&gt;% group_by(gender) %&gt;% descr(stats = &quot;fivenum&quot;) 4.1.2.2 构造函数处理流 ################# 构建函数处理流： #################### count_data &lt;- linelist %&gt;% group_by(hospital, date_hospitalisation) %&gt;% summarize(n_cases = dplyr::n()) %&gt;% filter(date_hospitalisation &gt; as.Date(&quot;2013-06-01&quot;)) %&gt;% ungroup() ## summarise()的复杂用法： # &quot;Roll-up&quot; values into one row per group (per &quot;personID&quot;) cases_rolled &lt;- obs %&gt;% group_by(personID) %&gt;% arrange(date, .by_group = TRUE) %&gt;% ## 这里是排序的意思 summarise( across(everything(), ## 记住across()来指定对应的函数类型； ~paste0(na.omit(.x), collapse = &quot;; &quot;))) # function is defined which combines non-NA values 4.1.3 常用统计函数 4.1.3.1 常用描述性统计函数： ## 计算标准差函数： std &lt;- function(x) sd(x)/sqrt(length(x)) ## 数学运算： cumsum 累加和 cumprod 累加积 cummin 累加最小值 cummax 累加最大值 cummean 累加均值 # use the appropriate rounding function for your work 危险： round()使用“银行家四舍五入法”，仅当上限为偶数时才从 0.5 向上舍入。使用round_half_up()from janitor始终将一半四舍五入到最接近的整数。看到这个解释 round(c(2.5, 3.5)) ## [1] 2 4 janitor::round_half_up(c(2.5, 3.5)) ## [1] 3 4 4.1.3.2 中级统计函数 4.1.3.2.1 base R中的统计函数： ## t统计： t.test(age_years ~ gender, data = linelist) ## 夏皮罗-威尔克测试-检验数据是否来自正态分布总体： shapiro.test(linelist$age_years) ## 秩和检验-wilcoxon: # 确定两个数值样本是否来自同一分布，当它们的总体不是正态分布或方差不等时。 %&gt;% wilcox.test(age_years ~ outcome, data = linelist) ## Kruskal-Wallis 检验 # 是 Wilcoxon 秩和检验的扩展，可用于检验两个以上样本分布的差异 kruskal.test(age_years ~ outcome, linelist) ##卡方检验-用于检验分类变量的统计显著性： ## 注意卡方检验在tidy系列中分析时，必须首先构建交叉表： ## linelist %&gt;% tabyl(gender,outcome) %&gt;% ## tabyl()用于生成交叉表； select(-1) %&gt;% chisq_test() chisq.test(linelist$gender, linelist$outcome) 4.1.3.2.2 tidy流中的统计函数流 4.1.3.2.2.1 描述性回归/tbl_summary ## 使用gtsummary()来优化统计结果输出： ## 卡方： linelist %&gt;% select(gender, outcome) %&gt;% # keep variables of interest tbl_summary(by = outcome) %&gt;% # produce summary table and specify grouping variable add_p() # 默认为卡方检验； ## t检验： linelist %&gt;% select(age_years, outcome) %&gt;% tbl_summary( statistic = age_years ~ &quot;{mean} ({sd})&quot;, by = outcome) %&gt;% add_p(age_years ~ &quot;t.test&quot;) ## wilcoxon秩和检验； linelist %&gt;% select(age_years, outcome) %&gt;% tbl_summary( statistic = age_years ~ &quot;{median} ({p25}, {p75})&quot;, by = outcome) %&gt;% add_p(age_years ~ &quot;wilcox.test&quot;) ## 复杂形式： linelist %&gt;% select(age_years, gender, outcome, fever, temp, hospital) %&gt;% # keep only columns of interest tbl_summary( by = outcome, ## 按组进行划分； statistic = list(all_continuous() ~ &quot;{mean} ({sd})&quot;, &quot; all_categorical() ~ &quot;{n} / {N} ({p}%)&quot;), digits = all_continuous() ~ 1, ## 四舍五入 # rounding for continuous columns type = all_categorical() ~ &quot;categorical&quot;, ## 两种指定类型； label = list( # display labels for column names outcome ~ &quot;Outcome&quot;, age_years ~ &quot;Age (years)&quot;, gender ~ &quot;Gender&quot;, temp ~ &quot;Temperature&quot;, hospital ~ &quot;Hospital&quot;), missing_text = &quot;Missing&quot; ) ## 补充： 1、type还有更详细的写法： type = all_continuous() ~ &#39;continuous2&#39;, statistic = all_continuous()~c( &quot;{mean} ({sd})&quot;, &quot;{median} ({p25}, {p75})&quot;), 4.1.3.2.2.2 参数回归/tbl_uvregression() ## 使用gtsummary中的tbl_uvregression()来进行参数回归： univ_tab &lt;- linelist %&gt;% dplyr::select(explanatory_vars, outcome) %&gt;% tbl_uvregression( method = glm, y = outcome, method.args = list(family = binomial), exponentiate = TRUE ) ## view univariate results table univ_tab ## run a regression with all variables of interest mv_reg &lt;- explanatory_vars %&gt;% ## begin with vector of explanatory column names str_c(collapse = &quot;+&quot;) %&gt;% ## combine all names of the variables of interest separated by a plus str_c(&quot;outcome ~ &quot;, .) %&gt;% ## combine the names of variables of interest with outcome in formula style glm(family = &quot;binomial&quot;, ## define type of glm as logistic, data = linelist) ## define your dataset final_mv_reg &lt;- mv_reg %&gt;% step(direction = &quot;forward&quot;, trace = FALSE) options(scipen=999) ## 关闭科学计数法 ## show results table of final regression mv_tab &lt;- tbl_regression(final_mv_reg, exponentiate = TRUE) ## 合并建模结果： tbl_merge( tbls = list(univ_tab, mv_tab), # combine tab_spanner = c(&quot;**Univariate**&quot;, &quot;**Multivariable**&quot;)) # set header names ## 模型比较： model1 &lt;- glm(outcome ~ age_cat, family = &quot;binomial&quot;, data = linelist) model2 &lt;- glm(outcome ~ age_cat + gender, family = &quot;binomial&quot;, data = linelist) lmtest::lrtest(model1, model2) 4.1.4 批处理函数 4.1.4.1 apply函数族 ## APPLY: 这个函数的使用格式为：apply(X, MARGIN, FUN, ...)。它应用的数据类型是数组或矩阵，返回值类型由FUN函数结果的长度确定。 # 举例： apply(a, MARGIN=1, FUN=quantile, probs=seq(0,1, 0.25)) ## lapply、sapply和vapply函数： 它们应用的数据类型都是列表，对每一个列表元素应用FUN函数 # lappy是最基本的原型函数，sapply和vapply都是lapply的改进版。 sapply返回的结果比较“友好”，如果结果很整齐，就会得到向量或矩阵或数组 ## mapply:相当于sapply的多变量版本； mapply应用的数据类型为向量或列表，FUN函数对每个数据元素应用FUN函数；如果参数长度为1，得到的结果和sapply是一样的；但如果参数长度不是1，FUN函数将按向量顺序和循环规则（短向量重复）逐个取参数应用到对应数据元素： &gt; sapply(X=1:4, FUN=rep, times=4) [,1] [,2] [,3] [,4] [1,] 1 2 3 4 [2,] 1 2 3 4 [3,] 1 2 3 4 [4,] 1 2 3 4 &gt; mapply(rep, x = 1:4, times=4) [,1] [,2] [,3] [,4] [1,] 1 2 3 4 [2,] 1 2 3 4 [3,] 1 2 3 4 [4,] 1 2 3 4 &gt; mapply(rep, x = 1:4, times=1:4) [[1]] [1] 1 [[2]] [1] 2 2 [[3]] [1] 3 3 3 [[4]] [1] 4 4 4 4 ## tapply和by(with)函数： tapply是table()函数的进阶版本； &gt; (x &lt;- 1:10) [1] 1 2 3 4 5 6 7 8 9 10 &gt; (f &lt;- gl(2,5, labels=c(&quot;CK&quot;, &quot;T&quot;))) [1] CK CK CK CK CK T T T T T &gt; table(f) f CK T 5 5 &gt; tapply(x, f, sum) CK T 15 40 ## by和with函数： # 工作原理类似于加载数据，分组后经分类按函数运算； with(iris,by(sp1,sp2,sum)) #### aggregate: #### 首先将数据进行分组（按行），然后对每一组数据进行函数统计，最后把结果组合成一个比较nice的表格返回。 # 第一种实现思路是：加载，构建分组，函数运算； &gt; aggregate(mtcars, by=list(cyl, gear), FUN=mean) # 第二种思路是构建数据框内的函数体系： &gt; aggregate(cbind(mpg,hp) ~ cyl+gear, FUN=mean) 表示使用 cyl 和 gear 的因子组合对 cbind(mpg,hp) 数据进行操作。 4.1.4.2 map/tidy函数映射式批建模 4.1.4.2.1 批量构造模型： ## 直接读取形式： lm_results &lt;- lm(ht_cm ~ age, data = linelist) summary(lm_results) tidy(lm_results) ## 使用tidy整理统计检验结果-流形式 model &lt;- glm(outcome ~ age_cat, family = &quot;binomial&quot;, data = linelist) %&gt;% tidy(exponentiate = TRUE, conf.int = TRUE) %&gt;% mutate(across(where(is.numeric), round, digits = 2)) ## 使用map和bind_rows()来批量生成对应数据： models &lt;- explanatory_vars %&gt;% # begin with variables of interest str_c(&quot;outcome ~ &quot;, .) %&gt;% # combine each variable into formula (&quot;outcome ~ variable of interest&quot;) # iterate through each univariate formula map( .f = ~glm( formula = as.formula(.x), family = &quot;binomial&quot;, # specify type of glm (logistic) data = linelist)) %&gt;% # dataset # tidy up each of the glm regression outputs from above map( .f = ~tidy( .x, exponentiate = TRUE, conf.int = TRUE)) %&gt;% # collapse the list of regression outputs in to one data frame bind_rows() %&gt;% # round all numeric columns mutate(across(where(is.numeric), round, digits = 2)) 4.1.4.2.2 批量构造简单统计 ### 借助map函数变体来构造统计： x &lt;- list(1:5, c(1:10, NA)) map_dbl(x, ~ mean(.x, na.rm = TRUE)) #&gt; [1] 3.0 5.5 4.1.4.2.3 tidyr::nest/broom:tidy mtcars &lt;- as_tibble(mtcars) # to play nicely with list-cols nest_mtcars &lt;- mtcars %&gt;% nest(data = c(-am)) ## 这里是将非am列以外的数据套叠； nest_mtcars ## 输出形式如下： am data &lt;dbl&gt; &lt;list&gt; 1 1 &lt;tibble [13 x 10]&gt; 2 0 &lt;tibble [19 x 10]&gt; ## 根据套叠来输出可行的结果： nest_mtcars %&gt;% mutate( ## 这里会默认返回线性折叠结果，并依据am分组统计： fit = map(data, ~ lm(wt ~ mpg + qsec + gear, data = .x)), # S3 list-col ## 来自broom包的tidy函数，自动整理fit拟合的结果； tidied = map(fit, tidy) ) %&gt;% unnest(tidied) %&gt;% select(-data, -fit) 4.1.5 BROOM优雅输出 4.1.5.1 base r中常用的统计解析 函数 功能用途 summary() 展示拟合模型的详细结果 coefficients() 展示拟合模型的模型参数 fitted() 列出拟合模型的预测值 residuals() 列出拟合模型的残差值 anova() 生成拟合模型的方差分析表 AIC() 输出赤池系数（评价模型好坏） confint() 提供模型参数的知心区间(默认0.95） plot() 生成评价模型的诊断图 predict() 通过模型对新的数据集变量预测 head(women) ## height weight ## 1 58 115 ## 2 59 117 ## 3 60 120 ## 4 61 123 ## 5 62 126 ## 6 63 129 fit&lt;-lm(weight~height,data = women) fitted(fit) #列出拟合模型的预测值 ## 1 2 3 4 5 6 7 8 ## 112.5833 116.0333 119.4833 122.9333 126.3833 129.8333 133.2833 136.7333 ## 9 10 11 12 13 14 15 ## 140.1833 143.6333 147.0833 150.5333 153.9833 157.4333 160.8833 residuals(fit) #列出拟合模型的残差值 ## 1 2 3 4 5 6 ## 2.41666667 0.96666667 0.51666667 0.06666667 -0.38333333 -0.83333333 ## 7 8 9 10 11 12 ## -1.28333333 -1.73333333 -1.18333333 -1.63333333 -1.08333333 -0.53333333 ## 13 14 15 ## 0.01666667 1.56666667 3.11666667 predict(fit) #通过模型对新的数据集变量预测 ## 1 2 3 4 5 6 7 8 ## 112.5833 116.0333 119.4833 122.9333 126.3833 129.8333 133.2833 136.7333 ## 9 10 11 12 13 14 15 ## 140.1833 143.6333 147.0833 150.5333 153.9833 157.4333 160.8833 4.1.5.2 Broom包中的优雅解析 library(broom) ## 关键是broom体系适配于tidy体系： #返回模型系数估计及其统计量 fit&lt;-lm(weight~height,data = women) fit %&gt;% tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -87.5 5.94 -14.7 1.71e- 9 ## 2 height 3.45 0.0911 37.9 1.09e-14 #返回模型诊断信息 fit&lt;-lm(weight~height,data = women) fit %&gt;% glance(fit) ## # A tibble: 1 x 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.991 0.990 1.53 1433. 1.09e-14 1 -26.5 59.1 61.2 ## # ... with 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; #augment函数返回预测值,残差等模型结果的原始值 fit&lt;-lm(weight~height,data = women) fit%&gt;% augment() 4.2 复杂统计模型 4.2.1 变量筛选 4.2.1.1 变量筛选的基本流程 模型构建中第一个问题就是变量筛选：最常规的办法是先单后多（先进行单因素分析，单因素有意义的再一起纳入多因素模型中）； 但如果变量数目过多，变量间存在共线性或存在较多缺失值而又不愿舍弃含缺失值的样本时，先单后多就存在很多局限性。 此时可以使用具有变量筛选功能的方法： -共线性问题：岭回归(Ridge Regression)，LASSO回归，弹性网络(Elastic Net Regression) -缺失值情况：随机森林模型 ## 常见的筛选变量方法 正则技术（岭回归、LASSO回归、弹性网络） 支持向量机 逐步回归（向前法、向后法、向前向后法） 最优子集（Best Subset Select） 树模型（使用的较少） 随机森林模型 主成分分析（提取多个自变量的主成分，将主成分得分作为最终的自变量） ## 常见的模型评价指标 拟合优度检验（卡方值&amp;P值） ROC（AUC、sen、spe、accuracy等） calibration（C-index） MSE rmse 模型验证是为了防止过拟合情况（所构建的模型对于本次数据有很好的效果，但是对全新的数据效果不理想） ## 常见的模型验证方法 cross validation（简单交叉，K-fold cross validation，N-fold cross validation） bootstrap cross validation + bootstrap（目前最常用） # 交叉验证（cv）和bootstrap的区别： .Bootstrap重抽样和交叉验证的区别。其相同之处，都是在数据集较小的时候常用的方法，提高结果的稳定性。不同之处，其一，两者的目的不同。CV主要用于模型选择上，例如KNN中选多大的K，使得估计的误差比较小。而Bootstrap主要用来看选定的模型的不确定性，例如参数的标准差多大。其二，两者的resample方法不同。在k fold CV中，把原始数据集分成k等分（各等分之间没交集），每一次验证中，把其中一份作为验证集，剩余的作为训练集。而在Bootstrap中，并不区分验证集和训练集，并且在resample中，是可放回抽样的，即同一个样本可以重复出现。 ## 变量筛选原则~~先单后多 如果某个变量单因素分析时P &lt; 0.05（这个标准可以根据实际案例设置成0.1或者更严格的0.01），就纳入多因素模型。 单因素分析可以用t检验，卡方检验，秩和检验，Logistic回归等。 因变量：Group，1表示疾病，0表示对照 自变量：连续变量和分类变量都有（将分类变量处理成factor--2分类是否处理成因子不影响结果，多分类必须处理成因子） 4.2.1.2 变量筛选的实施方法： 2.1 参与评估的变量过多，通过数据的时间尺度来去除非因果耦联因子； # 方法： ############ 子集选择法（Subset Selection）： 子集选择法分为最优子集选择、逐步筛选法等，这部分方法依赖于下述模型评判指标： 此外，还有考虑数据的共线性问题：包括pearson相关性分析和方差膨胀分析； ############ 系数压缩法（Shrinkage）: 适用于岭回归和LASSO；两种方法可以用于自变量相关性非常强时，在进行线性回归进行参数估计时，会导致解不可逆，并且十分不稳定。岭回归由于使用L2范数作为限制，所以只能将估计的参数进行压缩，使得模型更加稳定，但不能对变量进行筛选。 ########## 降维法（Dimension Reduction）。 主成分回归以及偏最小二乘回归。主成分回归法主要是先对自变量进行主成分分析，然后挑选变化后的，较为重要的一些新变量进行回归。这部分新变量的构造不依赖于因变量，相当于无监督学习。而偏最小二乘回归相当于在主成分回归的基础上再进行适当的添加，新变量的构造不仅依赖于原本的自变量，还受因变量的影响。 ## 前后筛选和后向筛选的区别; 以向后（backward）为例，其原理是，假设有N个自变量或预测指标，先将这N个合并在一起进行一个总的回归，每个自变量会有一个p值，将p&gt;0.05的变量中p值最大的去掉，剩下N-1个自变量，再重复上述过程，直到剩下的自变量都是显著性的。 向前（forward）是另一种算法：先进行N次单因素回归（每个自变量分别进行单因素回归），选出其中SSq（sum of square）最大的那个（设为A），留在回归方程；然后将A和剩下的N-1个自变量分别组合，形成N-1个两因素回归，再选取其中SSq最大的，留下（此时为两个自变量了），和剩下的N-2个去组合~~~以此类推，直到自变量总的SSq不比残差的SSq大，就截止。 4.2.1.3 变量筛选的重要性评估方法： ########## 基于机器学习的方法： 随机森林分类模型以及对重要变量的选择 gx.rf&lt;-randomForest(gdp~.,data=gxdata_without_x,importance=TRUE, ntree=1000) importance(gx.rf) varImpPlot(gx.rf) ######### 标准化回归系数 文章的部分分析中，作者采用多元回归模型，定量分析了海拔、土壤养分、物种多样性、功能多样性和性状组成等在不同气候区对地上生物量影响的相对重要性。简单来说，首先每个非生物或生物变量（作为自变量）都标准化到均值0标准差1，随后拟合了它们与地上生物量（作为因变量）的多元线性回归。并在模型优化后（包括变量选择，去除共线性等），最终通过比较每个自变量的标准化回归系数，确定各类非生物或生物因素对影响地上生物量的相对重要性。-- 这里是对称森林图； 这里既可以使用线性模型，也可以使用非线性模型来构建对应的函数； ############ 相对权重方法： 原理：它是对所有可能子模型添加一个自变量引起的R2平均增加量的一个近似值。 fit &lt;- lm(fish~acre+do2+depth+no3+so4+temp, data = dat) summary(fit) #展示拟合回归的简单统计 rel.weights &lt;- relweights(fit) rel.weights ################### 基于方差分解的多种方法： ### 使用R包：relaimpo 它针对多元线性回归模型提供了评估变量相对重要性的多种指标。R语言中，线性回归通常通过lm()函数实现，lm()的拟合结果可直接作为relaimpo包的输入，relaimpo包中的函数将在此基础上，计算已构建好的回归模型中各个自变量对R2（代表自变量对响应变量总方差的解释程度）的贡献，实现定量分析相对重要性的目的。您也可以将它近似理解为方差分解的原理。 library(relaimpo) #使用函数 calc.relimp() 评估模型中各个自变量的相对重要性，详情 ?calc.relimp #可以将上述 lm() 拟合的多元线性回归模型直接输入到 calc.relimp() crf &lt;- calc.relimp(fit_lm, rela = TRUE, type = c(&#39;lmg&#39;, &#39;last&#39;, &#39;first&#39;, &#39;betasq&#39;, &#39;pratt&#39;, &#39;genizi&#39;, &#39;car&#39;)) ## 这里标红色为多种评估变量重要性的指标； crf plot(crf) 4.2.1.4 多元回归中变量筛选的方法： 4.2.1.4.1 前向、后向、混合、暴力破解(AIC/BIC) # load processed data set from previous section load(url(&quot;https://userpage.fu-berlin.de/soga/300/30100_data_sets/dwd_30200.RData&quot;)) # load helper functions from previous section load(url(&quot;https://userpage.fu-berlin.de/soga/300/30100_data_sets/helper_functions_30200.RData&quot;)) # load list object from previous section load(url(&quot;https://userpage.fu-berlin.de/soga/300/30100_data_sets/model_outcome_I_30200.RData&quot;)) ## 均方根误差 rmse = function(model){ sqrt(sum((model$residual)^2)/nrow(model$model))} # build the model m.multi.interact &lt;- lm(formula = MEAN.ANNUAL.RAINFALL ~ ALTITUDE + log(ALTITUDE) + I(ALTITUDE^2) + MAX.RAINFALL + log(MAX.RAINFALL) + I(MAX.RAINFALL^2) + I(MAX.RAINFALL*ALTITUDE), data = train.set) # calculate rmse on training set print(paste(&#39;RMSE on training set:&#39;, rmse(m.multi.interact))) # prediction m.multi.interact.pred &lt;- predict(m.multi.interact, newdata = test.set) # calculate RMSE for the test data set print(paste(&#39;RMSE on test set:&#39;, rmse2(y.test, m.multi.interact.pred))) # store model object and results of rmse in the `list` object named `model.outcome` model.outcome[[&#39;multi.interact&#39;]] = list(&#39;model&#39; = m.multi.interact, &#39;rmse&#39; = data.frame(&#39;name&#39;= &#39;multiple interactions&#39;, &#39;train.RMSE&#39; = rmse(m.multi.interact), &#39;test.RMSE&#39; = rmse2(y.test, m.multi.interact.pred))) summary(m.multi.interact) ##################### 引入前向和后向选择 ########################## # the Akaike information criterion (AIC) and # the Bayesian information criterion (BIC). ################### 前向选择 ############################## # define (empty) model test.model &lt;- lm(formula = MEAN.ANNUAL.RAINFALL ~ 1,data = train.set) # define scope, which features to include test.scope &lt;- as.formula(lm(MEAN.ANNUAL.RAINFALL ~ ALTITUDE + MAX.RAINFALL + MEAN.CLOUD.COVER + MEAN.ANNUAL.AIR.TEMP, data = train.set)) # call the step function\\ ## 注意前向选择模型的构建方式：，先设置空白模型，然后比对复杂模型； step(test.model, scope = test.scope, direction = &#39;forward&#39;) # build the model m.forward.test &lt;- lm(formula = MEAN.ANNUAL.RAINFALL ~ MAX.RAINFALL + MEAN.ANNUAL.AIR.TEMP, data = train.set) # calculate rmse on training set print(paste(&#39;RMSE on training set:&#39;, rmse(m.forward.test ))) # prediction m.forward.test.pred &lt;- predict(m.forward.test , newdata = test.set) # calculate RMSE for the test data set print(paste(&#39;RMSE on test set:&#39;, rmse2(y.test, m.forward.test.pred))) ## 空模型和前向选择模型比较： # define (empty) model m.forward.full.baseline &lt;- lm(formula = MEAN.ANNUAL.RAINFALL ~ 1, data = train.set) # define scope, which features to include m.forward.full.scope &lt;- as.formula(lm(MEAN.ANNUAL.RAINFALL ~., data = train.set)) # call the step function m.forward.full &lt;- step(m.forward.full.baseline, scope = m.forward.full.scope, direction = &#39;forward&#39;, trace = 0) m.forward.full # calculate rmse on training set print(paste(&#39;RMSE on training set:&#39;, rmse(m.forward.full))) # prediction m.forward.full.test.pred &lt;- predict(m.forward.full, newdata = test.set) # calculate RMSE for the test data set print(paste(&#39;RMSE on test set:&#39;, rmse2(y.test, m.forward.full.test.pred))) # store model object and results of rmse in the `list` object named `model.outcome` model.outcome[[&#39;forward.full&#39;]] = list(&#39;model&#39; = m.forward.full, &#39;rmse&#39; = data.frame(&#39;name&#39;= &#39;forward model&#39;, &#39;train.RMSE&#39; = rmse(m.forward.full), &#39;test.RMSE&#39; = rmse2(y.test, m.forward.full.test.pred))) ###################### 后向选择 ############################## # define (sophisticated) model test.model &lt;- lm(formula = MEAN.ANNUAL.RAINFALL ~ ALTITUDE + MAX.RAINFALL + MEAN.CLOUD.COVER + MEAN.ANNUAL.AIR.TEMP, data = train.set) # call the step function step(test.model, direction = &#39;backward&#39;) # define (empty) model m.backward.full.baseline &lt;- lm(formula = MEAN.ANNUAL.RAINFALL ~. , data = train.set) # call the step function m.backward.full &lt;- step(m.backward.full.baseline, direction = &#39;backward&#39;, trace = 0) ################## 混合选择 ########################### # define model m.both.full.baseline &lt;- lm(formula = MEAN.ANNUAL.RAINFALL ~., data = train.set) # call the step function m.both.full &lt;- step(m.both.full.baseline, direction = &#39;both&#39;,trace = 0) m.both.full # calculate rmse on training set print(paste(&#39;RMSE on training set:&#39;, rmse(m.both.full))) ######################### ### Brute force - BIC ### ###########难怪称之为 蛮力破解的BIC ######################### # encode log-transformation features &lt;- colnames(train.set)[!(colnames(train.set) %in% c(&#39;MEAN.ANNUAL.RAINFALL&#39;, &#39;MIN.AIR.TEMP&#39;))] log_transform &lt;- paste(paste0(&#39;log(&#39;,features,&#39;)&#39;), collapse=&quot; + &quot;) ## 这个公式牛逼：加上所有的log选择，加上所有的二次项，加上所有的交互项； ### 这种模型的构建方式值得选择 formula = as.formula(paste0(&#39;MEAN.ANNUAL.RAINFALL ~ . +&#39;, log_transform ,&#39;+ .^2 + .*.&#39;)) # define model m.both.force.BIC.baseline &lt;- lm(formula = formula, data = train.set) # call the step function m.both.force.BIC &lt;- step(m.both.force.BIC.baseline,direction = &#39;both&#39;, trace = 0, k= log(nrow(train.set))) # clculate rmse on training set print(paste(&#39;RMSE on training set:&#39;, rmse(m.both.force.BIC))) 4.2.1.4.2 StepReg复杂筛选 ## select = c(&quot;AIC&quot;, &quot;AICc&quot;, &quot;BIC&quot;, &quot;CP&quot;, &quot;HQ&quot;, &quot;HQc&quot;, &quot;Rsq&quot;, &quot;adjRsq&quot;, &quot;SL&quot;, &quot;SBC&quot;): ## 基于前向、后向和双向,实现多条件、多参数检验筛选函数条件： ## 支持一般线性模型的参数变量选择： stepwise( formula, data, include = NULL, ## 支持选定变量强制参与最终统计建模： selection = c(&quot;forward&quot;, &quot;backward&quot;, &quot;bidirection&quot;, &quot;score&quot;), select = c(&quot;AIC&quot;, &quot;AICc&quot;, &quot;BIC&quot;, &quot;CP&quot;, &quot;HQ&quot;, &quot;HQc&quot;, &quot;Rsq&quot;, &quot;adjRsq&quot;, &quot;SL&quot;, &quot;SBC&quot;), sle = 0.15, sls = 0.15, multivarStat = c(&quot;Pillai&quot;, &quot;Wilks&quot;, &quot;Hotelling-Lawley&quot;, &quot;Roy&quot;), weights = NULL, best = NULL) ## 支持cox回归的参数变量选择： stepwiseCox( formula, data, include = NULL, selection = c(&quot;forward&quot;, &quot;backward&quot;, &quot;bidirection&quot;, &quot;score&quot;), select = c(&quot;SL&quot;, &quot;AIC&quot;, &quot;AICc&quot;, &quot;SBC&quot;, &quot;HQ&quot;, &quot;HQc&quot;, &quot;IC(3/2)&quot;, &quot;IC(1)&quot;), sle = 0.15, sls = 0.15, method = c(&quot;efron&quot;, &quot;breslow&quot;, &quot;exact&quot;), weights = NULL, best = NULL) ## 支持逻辑回归的变量筛选： stepwiseLogit( formula, data, include = NULL, selection = c(&quot;forward&quot;, &quot;backward&quot;, &quot;bidirection&quot;, &quot;score&quot;), select = c(&quot;SL&quot;, &quot;AIC&quot;, &quot;AICc&quot;, &quot;SBC&quot;, &quot;HQ&quot;, &quot;HQc&quot;, &quot;IC(3/2)&quot;, &quot;IC(1)&quot;), sle = 0.15, stepwiseLogit 9 sls = 0.15, sigMethod = c(&quot;Rao&quot;, &quot;LRT&quot;), weights = NULL, best = NULL) 4.2.1.4.3 逻辑回归筛选变量 library(tidyverse) # 加载数据处理包 # 读取数据 data &lt;- read_csv(&#39;Clinical/Clinical.RocData.Modified.csv&#39;,show_col_types = F) str(data) # 查看数据类型 names(data) # 查看变量名称 # 将分类变量处理成因子 data$Group &lt;- factor(data$Group, levels = c(&#39;0&#39;,&#39;1&#39;), labels = c(&#39;Con&#39;, &#39;AD&#39;)) data$EducationLevel &lt;- factor(data$EducationLevel, levels = c(&#39;1&#39;,&#39;2&#39;,&#39;3&#39;,&#39;4&#39;,&#39;5&#39;), labels = c(&#39;小学&#39;, &#39;初中&#39;, &#39;高中&#39;, &#39;大学&#39;, &#39;研究生&#39;)) data$Gender &lt;- factor(data$Gender, levels = c(&#39;0&#39;,&#39;1&#39;), labels = c(&#39;Female&#39;, &#39;Male&#39;)) summary(data) # 查看各变量的基本统计信息 # 连续型自变量 x1 &lt;- colnames(data)[4:60] # 分类自变量 x2 &lt;- colnames(data)[2:3] # t检验--数据符合正态分布 library(tableone) table1 &lt;- CreateTableOne(vars = c(x1, x2), # 指定对哪些变量进行分析 data = data, factorVars = x2, # 指定分类变量 strata = &#39;Group&#39;, # 指定分组 # 是否对总样本进行分析 addOverall = T) result1 &lt;- print(table1, # 是否对分类变量全部展示 showAllLevels = T) write.csv(result1, &#39;Clinical/单因素分析-t检验.csv&#39;) # 秩和检验 library(tableone) table2 &lt;- CreateTableOne(vars = c(x1, x2), # 指定对哪些变量进行分析 data = data, factorVars = x2, # 指定分类变量 strata = &#39;Group&#39;, # 指定分组 # 是否对总样本进行分析 addOverall = F) result2 &lt;- print(table2, # 是否对分类变量全部展示 showAllLevels = F, # 指定非参数检验变量，exact选项可以指定确切概率检验的变量 nonnormal = x1) write.csv(result2, &#39;Clinical/单因素分析-秩和检验.csv&#39;) # 单因素Logistic model &lt;- glm(Group~TP, data = data, family = binomial()) # 查看模型结果 summary(model)$coefficients # 计算OR值及可信区间 exp(cbind(&#39;OR&#39; = coef(model), confint(model))) # 多因素模型 model &lt;- glm(Group~TP+`LYMPH#`+SBP+DBP+NEUT+FN+`APOA1/APOB`+APOA1+ALB+GLB, data = data, family = binomial()) # 查看模型结果 summary(model)$coefficients # 计算OR值及可信区间 exp(cbind(&#39;OR&#39; = coef(model), confint(model))) # 非常规先单后多-筛选协变量 # 有用的思想是显著保留，不显著去除； # 指定自变量X是FN，剩下的都是协变量Z covar_method &lt;- function(var){ ## 这里是保留了基本变量： model &lt;- glm(Group~FN, data = data, family = binomial()) coef &lt;- coef(model)[2] form &lt;- as.formula(paste0(&#39;Group~FN+&#39;,var)) ## 采用逐步纳入的方法进行遍历，遍历后保留OR值增加的变量； model2 &lt;- glm(form, data = data, family = binomial()) coef2 &lt;- coef(model2)[2] ratio &lt;- abs(coef2-coef)/coef if (ratio &gt; 0.1) { return(var) } } var &lt;- c(x1,x2) var &lt;- var[-which(var %in% c(&#39;FN&#39;, &#39;MoCA-B&#39;,&#39;P-LCR&#39;,&#39;HDL-c&#39;,&#39;LDL-c&#39;,&#39;RDE-SD&#39;,&#39;APOA1/APOB&#39;,&#39;A/G&#39;,&#39;NEUT#&#39;, &#39;LYMPH#&#39;,&#39;MONO#&#39;,&#39;EO#&#39;,&#39;BASO#&#39;))] #去除一些变量名命名不规范的变量，以免引起错误 lapply(var, covar_method) # 将阳性的协变量与自变量一起进行多因素回归 4.2.1.4.4 岭回归（ridge）和脊回归（lasso）筛选变量 对于高维数据，普通的变量筛选方法并不见效或者需要消耗大量计算成本和时间成本；且难以避免模型的过拟合及多重共线性问题。此时需要在模型拟合的RSS最小化过程中加入一个正则化项，称之为收缩惩罚；这个惩罚项包含了一个希腊字母λ和对系数β的权重规范化。（RSS+收缩惩罚最小化） 正则化可以对高维数据的系数进行限制，甚至将其缩减到0，避免多重共线性，也可以有效避免过拟合。（岭回归，LASSO，弹性回归） 岭回归中，正则化项是所有变量系数的平方和(L2-norm)，当λ增加时，系数βj缩小，趋向于0，但是永不为0. LASSO回归中，正则化项是变量系数的绝对值和(L1-norm)，这个收缩惩罚项可以使βj收缩到0，因此LASSO具有变量筛选的功能。但是当自变量存在高度共线性或高度两两相关时，LASSO可能会将某个自变量强制删除，这会损失模型预测能力！ 弹性网络中，当α等于0时，弹性网络等价于岭回归；当α等于1时，等价于LASSO；弹性网络技能做到岭回归不能做的变量筛选，又能实现LASSO不能做的变量分组。 ## 简单来说： 岭回归不能实现变量筛选，但可以实现一定程度的变量惩罚压缩； 脊回归能实现变量筛选； 弹性网络：既可以实现变量筛选，也可以做到变量压缩。 library(corrplot) # 相关系数分析用 rm(list = ls()) gc() data &lt;- read_csv(&#39;Clinical/Clinical.RocData.Modified.csv&#39;,show_col_types = F) str(data) names(data) # 查看变量名称 names(data)[9] &lt;- &#39;MoCA_B&#39; names(data)[22:26] &lt;- c(&#39;NEUT数&#39;,&#39;LYMPH数&#39;,&#39;MONO数&#39;,&#39;EO数&#39;,&#39;BASO数&#39;) names(data)[32:33] &lt;- c(&#39;RDE_SD&#39;,&#39;P_LCR&#39;) names(data)[37] &lt;- &#39;A与G的比值&#39; names(data)[54:55] &lt;- c(&#39;HDL_c&#39;,&#39;LDL_c&#39;) names(data)[58] &lt;- &#39;APOA1与APOB的比值&#39; data &lt;- na.omit(data) # 进行NA的行删除，因为LASSO无法处理含有NA值的数据 corr &lt;- cor(as.matrix(data)) write.csv(corr, &#39;Clinical/Correlation.csv&#39;) corrplot.mixed(corr) # 简单进行可视化,查看是存在多重共线性，存在才进行LASSO # LASSO(正则化技术)不要将分类变量处理成factor，若涉及多分类变量，手动设置哑变量（可用ifelse函数设置） # 3分类举例：若data中存在一个变量X有A/B/C三个水平，此时需要将X处理成两个哑变量 data$X_B &lt;- ifelse(data$X == &#39;B&#39;, 1, 0) data$X_C &lt;- ifelse(data$X == &#39;C&#39;, 1, 0) library(glmnet) # 通过glmnet函数进行岭回归，LASSO回归，弹性网络 library(caret) # 帮助鉴定合适的参数 # 正则化技术需要将数据储存在矩阵里面，而不能是数据框 # 将因变量和自变量处理成矩阵，自变量类型不能是double，否则报错 X &lt;- as.matrix(data[2:60]) Y &lt;- as.matrix(data[1]) # glmnet()语法中alpha=0表示岭回归，1表示LASSO回归 myRidge &lt;- glmnet(X, Y, alpha = 0, family = &#39;binomial&#39;, nlambda = 1000) myRidge$lambda[1000] # 选最优模型的lambda值 plot(myRidge) # L1范数与系数值之间的关系 plot(myRidge, xvar = &#39;lambda&#39;) # lambda值减小，压缩参数也减小，而系数绝对值增大 plot(myRidge, xvar = &#39;dev&#39;) # 随着偏差百分比增加，系数绝对值增加 # 查看系数 myCoef &lt;- coef(myRidge, s = 4.525281) write.csv(as.matrix(myCoef),&#39;Clinical/岭回归系数.csv&#39;) # glmnet()语法中alpha=0表示岭回归，1表示LASSO回归 myLasso &lt;- glmnet(X, Y, alpha = 1, family = &#39;binomial&#39;, nlambda = 500) # glmnet默认运行100次 NROW(myLasso$lambda) myLasso$lambda[500] # 选最优模型的lambda值 myLasso$df[500] # 最优模型留下的变量数 # lambda = 0.004525281 收敛于最优解，有7个变量留下来 # 绘制图形 plot(myLasso, xvar = &#39;lambda&#39;, label = T) lasso.coef &lt;- coef(myLasso, s = 0.004525281) # 最优解下的回归系数 # 只有筛选出的自变量才有回归系数 write.csv(as.matrix(lasso.coef),&#39;Clinical/LASSO回归系数.csv&#39;) # 通过交叉验证进行LASSO回归 lambdas &lt;- seq(0,0.5,length.out = 200) set.seed(20220629) # nfolds = 3，表示3折交叉验证 cv.lasso &lt;- cv.glmnet(X, Y, alpha = 1, lambda = lambdas, nfolds = 5, family = &#39;binomial&#39;) plot(cv.lasso) # 纵坐标是MSE # 两条虚线：均方误差最小时对应的lambda对数值；距离最小均方误差1个标准误时对应的lambda对数值 # 一般第二条虚线对应的是我们的最优解 plot(cv.lasso$glmnet.fit, xvar = &#39;lambda&#39;, label = T) plot(cv.lasso$glmnet.fit, xvar = &#39;dev&#39;, label = T) # 如何找到最优lambda：通常距离最小均方误差（MSE）1个标准误时对应的lambda lasso_lse &lt;- cv.lasso$lambda.1se #提取最优lambda lasso.coef &lt;- coef(cv.lasso$glmnet.fit, s = lasso_lse, exact = F) # 没有回归系数的变量即为已剔除变量 # 弹性网络：寻找α和λ的最优组合 grid &lt;- expand.grid(.alpha = seq(0, 1, by = 0.1), .lambda = seq(0, 0.2, by = 0.01)) table(grid) as.matrix(head(grid)) # trainControl函数设定重抽样的方法LOOCV(留一法)，cv(简单交叉验证)，bootstrp con &lt;- trainControl(method = &#39;LOOCV&#39;) # 留一法消耗的时间是最多的 # 弹性网络中必须将因变量处理成因子 data$Group &lt;- factor(data$Group) set.seed(20220629) enet.train &lt;- train(Group ~ ., data = data, method = &#39;glmnet&#39;, trControl = con, tuneGrid = grid) enet.train # 选择原则是Accuracy最大，最优参数是α = 0.7；λ = 0.2 # 用最优组合来拟合模型 enet &lt;- glmnet(X, Y, family = &#39;binomial&#39;, alpha = 0.7, lambda = 0.2) enet.coef &lt;- coef(enet, s = 0.2, exact = T) enet.coef 4.2.1.4.5 支持向量机 data &lt;- read_csv(&#39;Clinical/Clinical.RocData.Modified.csv&#39;,show_col_types = F) %&gt;% as.data.frame() data &lt;- na.omit(data) # 进行NA的行删除 str(data) names(data) # 查看变量名称 names(data)[9] &lt;- &#39;MoCA_B&#39; names(data)[22:26] &lt;- c(&#39;NEUT数&#39;,&#39;LYMPH数&#39;,&#39;MONO数&#39;,&#39;EO数&#39;,&#39;BASO数&#39;) names(data)[32:33] &lt;- c(&#39;RDE_SD&#39;,&#39;P_LCR&#39;) names(data)[37] &lt;- &#39;A与G的比值&#39; names(data)[54:55] &lt;- c(&#39;HDL_c&#39;,&#39;LDL_c&#39;) names(data)[58] &lt;- &#39;APOA1与APOB的比值&#39; library(e1071) # 将因变量处理成因子型 data$Group &lt;- factor(data$Group) # 线性核函数 # tune.svm函数进行交叉验证选择最优cost成本函数 set.seed(20220629) linner.tune &lt;- tune.svm(Group ~ ., data = data, kernal = &#39;linner&#39;, cost = c(0.001,0.01,0.1,1,5,10)) summary(linner.tune) # best parameters: cost = 0.01 # best performance: 0.1583333 best.linner &lt;- linner.tune$best.model best.linner # 对拟合的最佳模型进行检验 linner.pred &lt;- predict(best.linner, newdata = data) table(linner.pred, data$Group) # 多项式核函数 set.seed(20220629) poly.tune &lt;- tune.svm(Group ~ ., data = data, kernal = &#39;polynomial&#39;, degree = c(3,4,5), coef0 = c(0.1,0.5,1,2,3,4)) summary(poly.tune) # best parameters: degree = 3, coef0 = 0.1 # best performance: 0.08333333 best.poly &lt;- poly.tune$best.model best.poly # 对拟合的最佳模型进行检验 poly.pred &lt;- predict(best.poly, newdata = data) table(poly.pred, data$Group) # 径向基核函数 set.seed(20220629) rbf.tune &lt;- tune.svm(Group ~ ., data = data, kernal = &#39;radial&#39;, gamma = c(0.5,1,2,3,4,5,6)) summary(rbf.tune) # best parameters: gamma = 3 # best performance: 0.425 best.rbf &lt;- rbf.tune$best.model best.rbf # 对拟合的最佳模型进行检验 rbf.pred &lt;- predict(best.rbf, newdata = data) table(rbf.pred, data$Group) # sigmoid核函数 set.seed(20220629) sigmoid.tune &lt;- tune.svm(Group ~ ., data = data, kernal = &#39;sigmoid&#39;, gamma = c(0.1,0.5,1,2,3,4,5), coef0 = c(0.1,0.5,1,2,3,4,5)) summary(sigmoid.tune) # best parameters: gamma = 3, coef0 = 0.1 # best performance: 0.425 best.sigmoid &lt;- sigmoid.tune$best.model best.sigmoid # 对拟合的最佳模型进行检验 sigmoid.pred &lt;- predict(best.sigmoid, newdata = data) table(sigmoid.pred, data$Group) # 四种核函数最佳模型 library(caret) confusionMatrix(poly.pred, data$Group, positive = &#39;1&#39;) # 支持向量机的变量筛选 set.seed(20220629) # 设置筛选方法 selecMeth &lt;- rfeControl(functions = lrFuncs, method = &#39;cv&#39;, # 指定是交叉验证 number = 10) # 指定nfold数 # rfe函数中有3种变量筛选方法：svmLinner; svmPoly; svmRadial # size指定自变量个数 # 指定自变量和因变量 X &lt;- data[,2:60] Y &lt;- data[,1] svm.feature &lt;- rfe(X,Y,sizes = 30:1, rfeControl = selecMeth, method = &#39;svmPoly&#39;) svm.feature # TP, MMSE, MoCA_B, DBP # 利用筛选出的自变量进行分析 svm.4 &lt;- svm(Group ~ TP+MMSE+MoCA_B+DBP, data = data, kernel = &#39;polynomial&#39;, degree = 3, coef0 = 0.1) names(data) # 对模型进行预测 svm.4.pred &lt;- predict(svm.4, newdata = data[,c(34,8,9,6)]) table(svm.4.pred, data$Group) # 图形绘制 plot(svm.4, data = data, TP ~ MoCA_B, # 只展现2维的，维度过高不易观察 svSymbol = 2, # 指定支持向量的形状为三角形 dataSymbol = 1) # 支持向量之外的数据为圆圈 4.2.1.4.6 随机森林变量筛选 为提升模型的预测能力，我们可以生成多个树模型，然后将树模型的结果组合起来。随机森林的两个方法：装袋&amp;变量随机。 使用数据集的约2/3的数据建立树模型，剩下的1/3成为袋外数据（验证前期建立的模型的准确率），这个过程重复N次，最后取平均结果。每个树都任其生长，不进行任何基于测量误差的剪枝，这意味着每一个树模型的方差都很大。但是对多个树模型进行平均化，可以降低方差，同时又不增加偏差。 除了对样本进行随机选择，我们对自变量也进行随机选择。对于分类问题，每次抽取的自变量数是自变量总数的平方根；对于回归问题，每次抽取的自变量数是自变量总数的1/3。 随机森林可以对变量的重要性进行评分和排序，并不能实现变量的筛选。 data &lt;- read_csv(&#39;Clinical/Clinical.RocData.Modified.csv&#39;,show_col_types = F) str(data) names(data) # 查看变量名称 data &lt;- na.omit(data) # 进行NA的行删除，防止bug # 因变量：Group，1表示疾病，0表示对照 # 自变量：连续变量和分类变量都有（将分类变量处理成factor--2分类是否处理成因子不影响结果， # 多分类必须处理成因子） # 将分类变量处理成因子 names(data)[9] &lt;- &#39;MoCA_B&#39; names(data)[22:26] &lt;- c(&#39;NEUT数&#39;,&#39;LYMPH数&#39;,&#39;MONO数&#39;,&#39;EO数&#39;,&#39;BASO数&#39;) names(data)[32:33] &lt;- c(&#39;RDE_SD&#39;,&#39;P_LCR&#39;) names(data)[37] &lt;- &#39;A与G的比值&#39; names(data)[54:55] &lt;- c(&#39;HDL_c&#39;,&#39;LDL_c&#39;) names(data)[58] &lt;- &#39;APOA1与APOB的比值&#39; # 将因变量处理成factor data$Group &lt;- factor(data$Group, levels = c(&#39;0&#39;,&#39;1&#39;), labels = c(&#39;Con&#39;, &#39;AD&#39;)) data$EducationLevel &lt;- factor(data$EducationLevel, levels = c(&#39;1&#39;,&#39;2&#39;,&#39;3&#39;,&#39;4&#39;,&#39;5&#39;), labels = c(&#39;小学&#39;, &#39;初中&#39;, &#39;高中&#39;, &#39;大学&#39;, &#39;研究生&#39;)) data$Gender &lt;- factor(data$Gender, levels = c(&#39;0&#39;,&#39;1&#39;), labels = c(&#39;Female&#39;, &#39;Male&#39;)) summary(data) # 查看各变量的基本统计信息 # 随机森林的拟合 library(randomForest) library(randomForest) set.seed(20220627) # 因为方法有随机选择，所以需要设置随机数，保证代码的复现性 model1 &lt;- randomForest(Group~., data = data) model1 # Call: # randomForest(formula = Group ~ ., data = data) # Type of random forest: classification # 使用的是分类随机森林模型 # Number of trees: 500 # 生成了500棵不同的树 # No. of variables tried at each split: 7 #每次树的分枝随机抽取7个变量 # # OOB estimate of error rate: 2.63% # 袋外数据估计的错误率是2.63% # Confusion matrix: # Con AD class.error # Con 20 0 0.00000000 #预测错误率是0 # AD 1 17 0.05555556 #预测错误率是0.056 # 画误差和树数量的关系图 plot(model1) # 绿色的线表示AD的误差 # 红色的线表示Con的误差 # 黑色的线表示总样本的误差 # 找出总样本最小误差对应的树数量 which.min(model1$err.rate[,1]) # 重新拟合模型，将ntree = 47，mtry设置每次迭代次数 set.seed(20220627) # 因为方法有随机选择，所以需要设置随机数，保证代码的复现性 model2 &lt;- randomForest(Group~., data = data, ntree = 47, mtry = 7) model2 # 变量的重要性评分 importance(model2) # 绘制图形 varImpPlot(model2) 4.2.1.4.7 Lasso回归做COX变量选择 ## 这里唯一的区别就是将生存数据纳入其中，并需要指定分布族为“cox”形式： ## 筛选变量前，首先将自变量数据（因子变量）转变成矩阵（matrix） x.factors &lt;- model.matrix(~ dt$oper.name+dt$relapse+dt$group.tumor.dia+dt$sex+dt$age.group+dt$region+dt$smoking+dt$group.hepth.medical.his+dt$group.ther.be.op+dt$BCLC+dt$group.melt.time+dt$tumor.single.double+dt$group.tumor.num+dt$group.tumor.size+dt$group.tumor.location+dt$AFP+dt$CEA+dt$CA199,dt)[,-1] #将矩阵的因子变量与其它定量边量合并成数据框，定义了自变量。 x=as.matrix(data.frame(x.factors,dt[,c(21:44)])) #设置应变量，生存时间和生存状态（生存数据） y &lt;- data.matrix(Surv(dt$live.time,dt$outcome)) #调用glmnet包中的glmnet函数，注意family那里一定要制定是“cox”，如果是做logistic需要换成&quot;binomial&quot;。 fit &lt;-glmnet(x,y,family = &quot;cox&quot;,alpha = 1) plot(fit,label=T) plot(fit,xvar=&quot;lambda&quot;,label=T) ##见图一 #主要在做交叉验证,lasso fitcv &lt;- cv.glmnet(x,y,family=&quot;cox&quot;, alpha=1,nfolds=10) plot(fitcv) ## 见图2 print(fitcv) ## 1个标准差对应变量少，选此收缩系数 ## Lambda Measure SE Nonzero ## min 0.02632 11.96 0.2057 21 ## 1se 0.11661 12.15 0.1779 5 coef(fitcv, s=&quot;lambda.1se&quot;) 4.2.1.4.8 最优子集模型筛选变量 最优子集回归是多元线性回归方程的自变量选择的一类方法。从全部自变量所有可能的自变量组合的子集回归方程中挑选最优者。如m个自变量会拟合2m-1个子集回归方程,然后用回归方程的统计量作准则(如交叉验证误差、Cp、BIC、调整R2等指标)从中挑选。 library(leaps) sub.fit &lt;- regsubsets(BSAAM ~ ., data = data)# 执行最优子集回归 best.summary &lt;- summary(sub.fit) # 按照模型评价标准找到评价指标 # 执行最优子集回归后返回的是自变量组合的子集回归方程，以及每个回归方程对应的评价指标,采用which函数选取最优的回归方程。其中调整R2越大越好，马洛斯Cp越小越好。 which.min(best.summary$cp)#马洛斯Cp值 which.max(best.summary$adjr2) #调整R2 which.min(best.summary$bic) #贝叶斯信息准则 ## 通过作图观测最佳变量集合的数量集： 将返回结果的调整R2作图，可以看到在模型变量个数为3的时候，调整R2最大。plot(best.summary$adjr2, type = &quot;l&quot;,xlab = &quot;numbers of Features&quot;, ylab = &quot;adjr2&quot;,main = &quot;adjr2 by Feature Inclusion&quot;) plot(sub.fit, scale = &quot;adjr2&quot;,main = &quot;Best Subset Features&quot;) coef(sub.fit, 3) #(Intercept) APSLAKE OPRC OPSLAKE # 15424.597 1712.481 1797.465 2389.838 ## 以往的学习思路中常忘记处理这一步： ## 将筛选的变量建模并进行共线性检查，方差膨胀系数大于5说明有严重的共线性。对这两个强相关的变量，我们分别做模型，挑选调整R2大的模型。最终我们保留f3模型。 f2 &lt;- lm(BSAAM ~ APSLAKE + OPRC + OPSLAKE, data = data) vif(f2) APSLAKE OPRC OPSLAKE 1.011499 6.452569 6.444748 ####这两个强相关的变量分别做模型，挑选R2 大的模型 f3 &lt;- lm(BSAAM ~ APSLAKE + OPSLAKE, data = data)#调整R2:0.9002 f4 &lt;- lm(BSAAM ~ APSLAKE + OPRC, data = data)#调整R2:0.862 4.2.2 线性统计回归 4.2.2.1 一般线性模型： # 构建数据： students &lt;- read.csv(&quot;https://userpage.fu-berlin.de/soga/200/2010_data_sets/students.csv&quot;) n &lt;- 12 sample.idx &lt;- sample(1:nrow(students), n) data &lt;- students[sample.idx , c(&#39;height&#39;,&#39;weight&#39;)] x &lt;- data$height y &lt;- data$weight x.bar &lt;- mean(x) y.bar &lt;- mean(y) b1 &lt;- sum((x-x.bar)*(y-y.bar)) / sum((x-x.bar)^2) b1 ## 求解线性斜率： cov(x,y) / var(x) ## 求解截距： b0 &lt;- y.bar - b1*x.bar b0 ## 使用一般线性方程求解： # 一般线性方程也被称之为广义线性模型： model &lt;- lm(weight ~ height, data = data) # 查看建模结果： model ## 查看截距和斜率： coef(model) ## 在给定置信区间中查看斜率和截距的范围： confint.default(model, level = 0.90) ## 查看模型的残差： residuals(model) sum(residuals(model)) ## 一个好的模型需要残差满足两个基本条件： # 第1是残差的和基本等于0，并且再0轴附近上下波动； # 第2是残差服从正态分布，这包括测试集和训练集都要服从；】 ## 可视化建模结果： plot(data$height, data$weight) abline(model, col = &#39;red&#39;) legend(&quot;topleft&quot;, legend = c(&#39;data points&#39;, &#39;regression line&#39;), cex = 0.7, col = c(&#39;black&#39;, &#39;red&#39;), lwd = c(NA, 1), pch = c(1, NA)) ## 查看模型的效应对应拟合值： fitted(model) ## 将拟合值纳入到可视化比较中： plot(data$height, data$weight) abline(model, col =&#39;red&#39;) points(data$height, fitted(model), col = &#39;green&#39;, pch = 15, cex = 0.7) legend(&quot;topleft&quot;, legend = c(&#39;data points&#39;, &#39;regression line&#39;, &#39;fitted values&#39;), cex = 0.7, col = c(&#39;black&#39;, &#39;red&#39;, &#39;green&#39;), lwd = c(NA, 1, NA), pch = c(1, NA, 15)) ## 模型预测： new.data &lt;- list(height=c(165, 172, 183)) predict(model, new.data) ## 在给定预测区间中设置范围： predict(model, interval = &quot;confidence&quot;, level = 0.99) predict(model, interval = &quot;prediction&quot;, level = 0.99) ## 可视化置信区间范围： new.values &lt;- seq(min(data$height)*0.5, max(data$height)*1.5, by = 0.1) conf &lt;- predict(model, newdata = list(&#39;height&#39; = new.values), interval=&quot;confidence&quot;, level = 0.99) pred &lt;- predict(model, newdata = list(&#39;height&#39; = new.values), interval=&quot;prediction&quot;, level = 0.99) plot(data$height, data$weight) abline(model, col=&#39;red&#39;) lines(new.values, conf[,&#39;lwr&#39;]) lines(new.values, conf[,&#39;upr&#39;]) lines(new.values, pred[,&#39;lwr&#39;], lty=2) lines(new.values, pred[,&#39;upr&#39;], lty=2) legend(&quot;topleft&quot;, legend = c(&#39;data points&#39;, &#39;regression line&#39;, &#39;confidence bands&#39;, &#39;prediction bands&#39;), cex = 0.7, col = c(&#39;black&#39;, &#39;red&#39;, &#39;black&#39;,&#39;black&#39;), lwd = c(NA, 1, 1, 1), lty = c(NA, 1, 1, 2), pch = c(1, NA, NA, NA)) ## 多项式线性模型拟合： m1 &lt;- lm(y ~ poly(x, 2), data = poly.data) m2 &lt;- lm(y ~ x + I(x^2), data = poly.data) 4.2.2.2 广义线性模型 广义线性模型（generalize linear model，GLM）扩展了线性模型的框架，它允许非正态响应变量的分析。 广义线性模型是一类服务于一组来自指数分布族的响应变量的模型框架，正态分布、指数分布、伽马分布、卡方分布、贝塔分布、伯努利分布、二项分布、负二项分布、多项分布、泊松分布、集合分布等都属于指数分布族。它们覆盖了生物学数据类型的更大范围，因此广义线性模型也广泛用于生物学领域的数据分析中。 ## 建模方法的选择： ## 连续变量： 例如血糖变化值：使用协方差分析模型（一般线性模型）； ## 时间次数： 例如呕吐次数：使用Posiison回归模型； 如果一定时间内次数间隔断层较大，则使用负二项回归； ## 改善率： logisitic回归模型； ## OS,PFS: COX比例风险模型； 4.2.2.2.1 2.3.1 二分类logistic 回归： ## 二分类logistic 回归： ## 变量筛选： m2 &lt;- glm(as.formula(formula_all),data = target_data) library(MASS) step.model &lt;- stepAIC(m2,direction = &#39;backward&#39;,trace = FALSE) summary(step.model) ## 结合stepAIC给出的变量参考建议： drop1(step.model) ## 如果客户确认后的总变量超过了10个，还是用adjusted R square好一点： ## 构建glm回归： ## 基于上一步变量筛选来构筑新的模型： ## 构建新模型： new_model = glm(x ~ age + 下肢疾病并发症 + 口服降糖药 + 心血管药物 + 糖化血红蛋白 + 总胆固醇 + 低密度脂蛋白,data = target_data,family = binomial) ## 计算协变量的OR值： # 计算OR值及可信区间 exp(cbind(&#39;OR&#39; = coef(model), confint(new_model))) ## 回归可视化： new_model %&gt;% tbl_regression( exponentiate =TRUE, pvalue_fun = ~style_pvalue(.x,digits =2)) %&gt;% add_global_p() %&gt;% bold_p(t=0.1) %&gt;% bold_labels() %&gt;% italicize_levels() 4.2.2.2.2 2.3.2 多分类logistic 回归： ## 有序多分类和无序多分类的区别说明： · 如果Y有多个选项，并且各个选项之间可以对比大小，例如，1代表“不愿意”，2代表“无所谓”，3代表“愿意”，这3个选项具有对比意义，数值越高，代表样本的愿意程度越高，那么应该使用多元有序Logistic回归分析(SPSSAU【进阶方法-&gt;有序logit】)； · 如果Y有多个选项，并且各个选项之间不具有对比意义，例如，1代表“淘宝”，2代表“天猫”，3代表“京东”，4代表“亚马逊中国”，数值仅代表不同类别，数值大小不具有对比意义，那么应该使用多元无序Logistic回归分析(SPSSAU【进阶方法-&gt;多分类logit】)。 ## 有序多分类建模： ## 有序多分类建模需要满足的条件： 假设3：自变量之间无多重共线性。 假设4：模型满足“比例优势”假设。意思是无论因变量的分割点在什么位置，模型中各个自变量对因变量的影响不变，也就是自变量对因变量的回归系数与分割点无关。这个检验，在spss中也称之为平行线检验； 平行线检验χ2 = 8.620, P = 0.375，说明平行性假设成立，即各回归方程相互平行，可以使用有序Logistic过程进行分析。 如果平行线假设不能满足，可以考虑一下两种方法进行处理：①进行无序多分类Logistic回归，而非有序Logistic回归，并能接受因变量失去有序的属性；② 用不同的分割点将因变量变为二分类变量，分别进行二项Logistic回归。 但是，当样本量过大时，平行线检验会过于敏感。即当比例优势存在时，也会显示P&lt;0.05。此时，可以尝试将因变量设置为哑变量，并拟合多个二分类Logistic回归模型，通过观察自变量对各哑变量的OR值是否近似来判断。 4.2.2.2.2.1 2.3.2.1 有序多分类逻辑回归 library(MASS) ###生成模拟数据 n &lt;-1000 # define sample size set.seed(2022) # so can reproduce the results age &lt;- rnorm(n, 60, 10) blood.pressure &lt;- rnorm(n, 125, 15) sex &lt;- factor(sample(c(&#39;female&#39;,&#39;male&#39;), n,TRUE)) outcome&lt;- factor(sample(c(1,2,3),n,TRUE),levels = c(&quot;1&quot;,&quot;2&quot;,&quot;3&quot;), labels = c(&quot;poor&quot;,&quot;fair&quot;,&quot;good&quot;)) data&lt;-data.frame(age,blood.pressure,sex,outcome) head(data) #####单因素分析------------ fit0&lt;-polr(ordered(outcome)~ 1, data=data)#####空模型 fit1&lt;-polr(ordered(outcome)~ +sex, data=data) summary(fit1) #####平行线检验--- ### 用于多分类逻辑回归中的平行假设检验： library(brant) brant(fit1) ###p&gt;0.1,满足平行线检验 ####检验模型整体是否有意义 anova(fit0,fit1) ####p=0.0687，接近有意义 #####生成OR、95%CI和P值 #####生成OR、95%CI和P值 OR&lt;- round(exp(fit1$coefficients),2) CI &lt;- round(exp(confint(fit1)), 2) CI&lt;- data.frame(CI[1],CI[2]) colnames(CI) &lt;- c(&quot;Lower&quot;, &quot;Higher&quot;) P &lt;- (pnorm(abs( coef(summary(fit1))[,&quot;t value&quot;]),lower.tail = FALSE)*2)[1] out&lt;- as.data.frame(cbind(OR, CI, P)) out #####输出的即为我们需要的OR、95%CI和P value ######多因素分析，方法类似----------------------------- fit2&lt;-polr(ordered(outcome)~ +sex+age+blood.pressure, data=data) summary(fit2) #####平行线检验--- brant(fit2) ###p&gt;0.1,三个变量都满足平行线检验 ####检验模型整体是否有意义 anova(fit0,fit2) #####生成OR、95%CI和P值,和单因素的有些区别 OR_CI&lt;-exp(cbind(OR=coef(fit2),confint(fit2))) colnames(OR_CI) &lt;- c(&quot;OR&quot;,&quot;Lower&quot;, &quot;Higher&quot;) P &lt;- (pnorm(abs( coef(summary(fit2))[,&quot;t value&quot;]),lower.tail = FALSE)*2) P #TI P&lt;-P[1:3] ###提取前三个 out&lt;- as.data.frame(cbind(OR_CI, P)) out ## 计算多因素逻辑回归方法中的OR、95%CI和P值 ## 另外一种方法是使用questionr包中的odds.ratio函数。 round(odds.ratio(fit),2) ## 结果解读： # 以性别为例(仅解释结果，先暂时忽略掉其没有意义)，我们可以说：“和女性相比， # 男性对自身的健康状况评价更低 （OR=0.82, 95%CI=0.65-1.02）”， # 或者是“男性认为自身健康好的OR值是女性的0.82倍 （OR=0.82, 95%CI=0.65-1.02）”。 4.2.2.2.2.2 2.3.2.2 无序多分类逻辑回归 library(tidyverse) library(caret) library(nnet) ## 本质上多因素无序逻辑回归的是给出参照，然后进行组内比较的二分类多元逻辑回归： ## 如果想修改指定参照需要使用以下函数代码： train.data$Species &lt;- relevel(train.data$Species, ref = &quot;virginica&quot;) # 导入数据 library(datasets) data(&quot;iris&quot;) # 将数据分成训练集和测试集 set.seed(123) training.samples &lt;- iris$Species %&gt;% createDataPartition(p = 0.8, list = FALSE) train.data &lt;- iris[training.samples, ] test.data &lt;- iris[-training.samples, ] model &lt;- nnet::multinom(Species ~., data = train.data) # 查看结果 summary(model) # install.packages(&#39;questionr&#39;) library(questionr) # 用questionr包中的odds.ratio函数。 round(odds.ratio(model),2) ## 模型预测： predicted.classes &lt;- model %&gt;% predict(test.data) 4.2.2.2.3 2.3.3 计数变量回归： 4.2.2.2.3.1 2.3.3.1 泊松分布回归 生物学数据中很多都是计数型数值，通常具有这些特点：（1）数值是离散的，并且只能是非负整数；（2）数值分布倾向于在特定较小范围内聚集，并具有正偏态的分布特征；（3）通常会出现很多零值；（4）方差随均值而增加。 泊松或负二项分布都是离散的概率分布，具有两个重要的属性：（1）数值仅包含非负整数；（2）方差是均值的函数。在早期，计数数型变量常通过数据变换或通过非参数假设检验进行分析，现如今更普遍使用广义线性模型方法的主要原因是可以获得可解释的参数估计。 泊松分布的典型例子是呈现出偏左态分布； fit_poisson &lt;- glm(fish~acre+do2+depth+no3+so4+temp, data = dat, family = &#39;poisson&#39;) summary.glm(fit_poisson) #展示拟合回归的简单统计 4.2.2.2.3.2 2.3.3.1 负二项分布回归 ## 负二项分布是泊松分布形式的一种： library(MASS) ## 实现方法1： m1 &lt;- glm.nb(freq~test,data = target,weights = weight) summary(m1) confint(m1) ## 实现方法2： m2 &lt;- glm(freq~test+age+sex+高血压+血脂异常+: 噻唑烷二酮类+`α-糖苷酶抑制剂`+`SGLT-2`+胰岛素+acei+arb+B受体阻断剂+ccb+freq_menzhen+all_cost, family=negative.binomial(theta = 1),data = target,weights = weight) ## 实现方法3： fit_glm &lt;- manyglm(Diplo_intensity~Treatment, data = worm, family = &#39;negative.binomial&#39;) #基于 9999 次自举的 Wald 统计量估计 p 值，其它参数直接使用默认值 summary_fit &lt;- summary.manyglm(fit_glm, test = &#39;wald&#39;, nBoot = 9999) summary_fit # 查看建模的合理性，以残差图分析： plot(summary_fit) 4.2.2.3 加性模型(GA) 4.2.2.3.1 一般加性模型： # 加性模型是常被用来探索响应变量与自变量之间函数形式的一种较为灵活的工具。在一般加性模型中，假定响应变量服从正态分布，并试图建立自变量与响应变量条件均值的非参数函数的可能形式。 # 与上述这种常见的参数回归相比，在非参数化的加性模型中，只设定了可加和性，而并没有对变量关系的函数形式作出假设。 总的来说，加性模型放宽了对响应关系加和形式的限制，允许任意函数之和来建模结果，自变量和响应变量之间的关系可以为任意线性或非线性。 # mgcv包是执行加性模型的最常见R包之一 #示例数据集，详情加载 agridat 包后 ?lasrosas.corn library(agridat) data(lasrosas.corn) head(lasrosas.corn) ### 加和效应： fit2_k3_k5 &lt;- gam(yield~s(nitro, k = 3) + s(bv, k = 5), data = lasrosas.corn.select) ## 注意这种加和的方法； summary(fit2_k3_k5) #检验自变量的显著性以及评估回归整体的方差解释率 ### 交互效应: fit2_inter &lt;- gam(yield~s(nitro, bv, k = 5), data = lasrosas.corn.select) summary(fit2_inter) 4.2.2.3.2 广义加性模型： ## 案例学习： 一般加性模型一般化为广义加性模型（GAM），代表了一类服务于一组来自指数分布族（如正态分布、指数分布、泊松分布、二项分布、负二项分布等）的响应变量的非参数化平滑回归框架，概括形式为： 此时fn(X)仍是非参数的函数，而响应变量Y服从指数分布族中的某种分布（不局限于正态性）。g(μY)代表了响应变量Y条件均值的函数（指数、泊松、二项、负二项等），又称连接函数，与在广义线性模型（GLM）中的理解相似，目的是将各类非正态的指数分布族响应变量的条件均值转化为正态形式的条件均值，以建立和自变量的非参数加和响应关系。 连接函数根据响应变量Y的实际分布而具体为不同公式。例如，当响应变量为泊松分布时，连接函数g(μY) = loge(Y)。一般加性模型事实上属于广义加性模型在正态响应变量时的特殊形式，此时g(μY) = Y。 # R - 广义加性模型（GAM）构建详解：以归一化植被指数NDVI为例 df = as.data.frame(df) train = df[1:115,] test = df[116:165,] library(mgcv) ## 构建一般模型： fit = mgcv::gam(ndvi ~s(soil1)+s(soil2)+s(soil3)+s(soil4)+s(gpp)+ s(rain)+s(longwave)+s(shortwave)+s(root_sm)+s(evap)+s(temper),data = train, trace = TRUE) summary(fit) 4.2.2.4 分段线性回归 ## 分段线性回归及对分段断点的评估 ## 分段线性回归有专门的R包来做这个工作，可以在一定程度上去除人为干扰的影响； 5.3.1 library(SiZer) library(SiZer) #示例数据集，详情 ?Arkansas data(Arkansas) head(Arkansas) model &lt;- piecewise.linear(x = Arkansas$year, y = Arkansas$sqrt.mayflies, CI = TRUE, bootstrap.samples = 1000, sig.level = 0.05) model #简单作图查看分段线性回归图 plot(model, xlab = &#39;year&#39;, ylab = &#39;sqrt.mayflies&#39;) model ## 会返回一个线性模型或者模型的中断点； 5.3.2 segmented 与上述方法相比，segmented包中函数segmented()更加灵活： （1）它可以从一个给定的已知线性回归或广义线性回归模型出发，寻找可能存在的断点，不局限于简单的纯线性关系，适用情况更广泛； （2）允许考虑多变量响应的情况，不局限于简单的一元响应关系； （3）允许出现多个断点的情况，不局限于只能识别单个断点； （4）关于断点位置的确定，即可以通过指定断点数量自动评估位置，但如果您大致确定了断点可能出现的数值坐标，也可以将其输入至函数中，此时函数将在指定位置两侧确定最佳的断点。 library(segmented) #例如，先拟合一个简单的线性回归模型 fit_lm &lt;- lm(sqrt.mayflies~year, data = Arkansas) summary(fit_lm) #第一种，通过 npsi 指定断点数量 #例如 1 个断点，将自动在全范围内寻找可能的断点位置 lm_seg1 &lt;- segmented(fit_lm, seg.Z = ~year, npsi = 1) summary(lm_seg1) plot(lm_seg1, xlab = &#39;year&#39;, ylab = &#39;sqrt.mayflies&#39;) points(sqrt.mayflies~year, data = Arkansas) #第二种，通过 psi 指定断点可能存在的大致初始位置 #例如 x=1997 是可能的断点位置，将它指定，将自动在 x=1997 附近寻找最佳的断点位置 lm_seg2 &lt;- segmented(fit_lm, seg.Z = ~year, psi = 1997) summary(lm_seg2) plot(lm_seg2, xlab = &#39;year&#39;, ylab = &#39;sqrt.mayflies&#39;) points(sqrt.mayflies~year, data = Arkansas) 4.2.2.5 结构方程模型 7.1 路径分析 路径分析（Path Analysis）是目前使用的主要SEM模型之一，是没有潜在变量的SEM应用。 路径分析的优点在于，它包含了在一个模型中充当预测变量的变量之间的关系。一个典型的例子是中介模型。 ## 7.2 验证性因子分析（CFA） 验证性因子分析（Confirmatory Factor Analysis，CFA）是一种降维方法，在SEM中也称为测量模型，CFA意在描述潜在因子（ε1和ε2，在SEM中等同于潜在变量）与观测变量（x1-x8）的关系。 7.3 潜变量结构模型 潜变量结构模型（Latent Variable Structural Model）主要在路径分析框架内使用测得的潜在变量。 例如，潜变量结构模型的一种常见形式是因子分析和路径分析的组合，因子分析挖掘潜在因子（潜在变量），之后可将潜在变量代入路径分析，假设并测试它们之间的关系。 7.4 分段结构方程建模 分段SEM通过引入一个灵活的数学框架，合并各种类型的模型结构、分布和假设，扩展了传统的SEM。分段SEM中，每组关系都是独立（或局部）估计的，此过程将整体关系分解为与每个响应对应的简单或多个（一般为线性）回归，分别对每个响应进行评估，最后合并以生成有关全局SEM的推论。即分别在各个模型中估计路径，然后将它们拼凑起来以构建因果模型。假定的变量关联模式，包括交互作用和非正态响应、随机效应和层次模型以及其它相关结构（包括系统发育、空间和时间）等。 偏最小二乘路径模型（PLS-PM） ################# 关于结构方程模型的总结： ################## 常规的SEM有两个主要目标： （1）了解一组变量之间的相关/协方差模式； （2）用指定的模型尽可能解释它们的方差。 因此常规SEM也有人直接称为协方差SEM（下文允许我也使用这一称呼，尽管可能不贴切，因为分段SEM也基于协方差，只是情况比常规SEM复杂一些）。 观测协方差矩阵（原始变量观测值的协方差矩阵）与预测协方差矩阵（模型预测值的协方差矩阵）之间的差异量化了模型的拟合优度。 ## 模型评估： 可用于反映模型拟合优度的指标有很多，例如卡方值（CMIN）、卡方自由度比（CMIN/DF）、比较拟合指数（CFI）、近似值的均方根误差（RMSEA）、Akaike信息准则（AIC）、贝叶斯信息标准（BIC）等，它们均以比较两个协方差矩阵的差异为准 ## 协方差SEM的局限性 协方差SEM假定所有变量均来自正态分布，即数据服从多元正态分布。 协方差SEM假设所有观察结果都是独立的，换句话说，假设数据没有底层结构。例如在生态学研究中，这些假设经常被违反，变量间的空间、时间等相关关系普遍存在；尽管实际中通常忽略该假设。 SEM通常需要相当大的样本量，每个估计参数至少需要5个样本，更普遍在10个以上。如果变量是嵌套的，则此问题可能会更为棘手，此时通常只能在层次结构的最高层考虑变量，会极大降低分析的能力。 4.2.2.6 混合效应模型 ## 参见资料： https://zhuanlan.zhihu.com/p/60528092 https://zhuanlan.zhihu.com/p/50048784 https://blog.csdn.net/qq_27056805/article/details/87462542 https://blog.csdn.net/fjsd155/article/details/88360671 ## 使用说明： 混合效应模型：使用lmer包； lme4和lmerTest包的lmer函数 ## carn中提供的metTools包；用于交互式查看那lme4的结果； https://cran.r-project.org/web/packages/merTools/vignettes/merToolsIntro.html #将个体当做随机因素，这样由于个体有m个水平(m个个体),因此会发现会产生m个截距值 #这里的1代表随机截距， | 后面表示分组变量； model=lmer(pitch~sex+place+(1|subject)，data=data.csv) #查看结果 model #查看系数 coef(model) # 复杂的例子，这里的place表示place中使用随机斜率； model=lmer(pitch~sex+place+(place|subject)+(1|subject) ## 表示在给定随机斜率的情况下，固定截距；加上参数化的交互作用； lmer(Y ~ 1 + X1*W + X2 + (1 + X1 | group), ...) ## 固定截距（fixed intercept）：固定截距其实并不存在于HLM的模型中，而是“降级”到了一般的最小二乘法回归（OLS），也就是我们最常用的GLM回归分析。 → lm(Y ~ 1 + X1 + X2, ...) ## 随机截距（random intercept）：在做HLM时，我们通常都会将截距设置为随机截距，也就是允许不同组具有各自的截距（基线水平）。可以理解为，“有的人出生就在终点，而你却在起点”。在R里面，只要你在回归表达式后面加上小括号（当然，这时就不能再用lm了，要用lme4和lmerTest包的lmer函数），括号里就定义了Level 1截距或斜率在Level 2的随机部分（Level 1的随机部分则是个体层面的残差residual，不用我们定义）。竖线“|”后面是分组变量（clustering/grouping variable，可以是省市、学校，而在重复测量、追踪设计中则是被试个体），竖线前面的1代表随机截距、具体变量名则代表这个变量对应的随机斜率。 → lmer(Y ~ 1 + X1 + X2 + (1 | group), ...) ## 固定斜率（fixed slope）：固定斜率的意思是，某个Level 1自变量的斜率在不同的group里面都是一致的。虽然实际情况未必真的一致，但研究者可以假设并检验斜率是否在组间保持一致而不存在显著差异。需要注意的是，Level 2截距或斜率并不存在固定和随机的区分（或者说都是固定的），除非还有Level 3。 → lmer(Y ~ 1 + X1 + X2 + (1 | group), ...) ## 随机斜率（random slope）：与固定斜率相反，随机斜率意味着某个Level 1自变量的斜率在不同的group之间存在差异，或者说“依组而变”。可以理解为，“有的人花两个小时就能赚10000，而你却只能挣个10块钱被试费”。你既可以只纳入随机斜率成分而不对斜率的差异作出具体解释，也可以再纳入一个Level 2的自变量与这个Level 1自变量发生交互作用（即跨层交互），从而解释为什么X的效应依组而变、是什么因素导致了这种变化。 → lmer(Y ~ 1 + X1 + X2 + (1 + X1 | group), ...) # 这里的1可以省略，默认都纳入截距（但只有随机截距时则不能省） → lmer(Y ~ 1 + X1*W + X2 + (1 + X1 | group), ...) # W表示一个Level 2解释变量，X1*W即为一个跨层交互作用 preview ## 简单R代码实现： library(nlme) mod_lmm1 &lt;- lme(raw_lab_result.x~ test*time +raw_lab_result.y+age+sex+高血压+血脂异常+心肌梗塞+心肌缺血+心绞痛+房颤+卒中+心力衰竭+外周血管疾病+糖尿病足+神经病变+ +all_score+磺脲类+格列奈类+ ccb+ 利尿剂+抗凝药+抗血小板药+调脂药+freq_zhuyuan+ all_days+freq_menzhen+all_cost,random = ~1|patient,data = target,weights = c(&#39;weight&#39;)) summary(mod_lmm1) intervals(mod_lmm1,which = &#39;fixed&#39;) 4.2.3 方差检验 ## 代码更新：anova： set.seed(0) #创建数据框 data &lt;- data.frame(program = rep(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), each = 30), weight_loss = c(runif(30, 0, 3),runif(30, 0, 5),runif(30, 1, 7))) ## 方差检验前提： # 即数据集应该符合正态和各组的方差相等， # 可以分别用shapiro.test和bartlett.test检验从P值观察到这两个假设是符合的。 # 这两个检验的结局是p值大于0.05，则数据正态，方差相等。 ## 或者使用： library (car) #conduct Levene&#39;s Test for equations of variances leveneTest(weight_loss ~ program, data = data) #fit the one-way ANOVA model ## var.equal=TRUE满足方差齐性的需要： result2 &lt;- oneway.test(value~variable,data=anova1,var.equal=TRUE) # 各水平的总体方差不相等(var.equal=FALSE)，则使用Welch的近似方法 result2 &lt;- oneway.test(value~variable,data=anova1,var.equal=FALSE) #view the model output summary(model) ## 将统计结果整理成表格形式： library(broom) tidy(model) ## 模型检验： ## qq图： # 理想情况下，标准化残差将沿着图中的直线对角线下降。 # 然而，在上图中，我们可以看到残差在开始和结束时偏离了这条线。 # 这表明我们的正态性假设可能被违反。 plot(model) ## 事后检验： # 0.05 显着性水平上，每个程序的平均体重损失之间存在统计学上的显着差异。 # Tukey的HSD法要求各样本的样本相等或者接近，在样本量相差很大的情况下还是建议使用其他方法。 TukeyHSD(model, conf.level=.95) plot(TukeyHSD(model, conf.level=.95), las = 2) ## 参数事后比较： pairwise.t.test(x, g, p.adjust.method=”bonferroni”) ## 非参数的方差检验： kruskal.test(DEPDC1 ~ molecular_group, data=myeloma) ## 有以下两种算法来进行事后检验： # 区别在于： # 参见：https://blog.csdn.net/jbb0523/article/details/109990924 # 参见：https://baijiahao.baidu.com/s?id=1731958531513315048&amp;wfr=spider&amp;for=pc # DunnTest:处理组与对比组样本均数之间差别有无统计学意义； # NemenyiTest:是完全随机设计多样本间多重比较秩和检验的方法； # 相对来说，当分组较少时，使用DunnTest比使用NemenyiTest更合适；’ ans &lt;- kwAllPairsDunnTest(count ~ spray, data = InsectSprays, p.adjust.method = &quot;bonferroni&quot;) ans &lt;- kwAllPairsNemenyiTest(count ~ spray, data = InsectSprays) summary(ans) 4.2.4 相关统计R包资源： 4.2.4.1 BruceR-模型检验和模型整合输出 PROCESS()：致敬Hayes (2013, 2018)开发的SPSS PROCESS宏程序，更方便地进行各种中介效应和调节效应分析，支持一般/广义的线性/线性混合模型。 HLM_ICC_rWG()：计算多层线性模型的ICC(1)、ICC(2)、rWG指标。 lavaan_summary()：整理、汇总lavaan包的结构方程模型结果，同时可以更方便地计算各种类型的bootstrap置信区间。 granger_causality()：多元时间序列数据的格兰杰因果检验。 show_colors()：展示不同颜色或配色方案。 %^%：paste0()的管道函数版本，更方便地拼接字符串。 ## 其他资料参见； https://zhuanlan.zhihu.com/p/376007591 https://zhuanlan.zhihu.com/p/281150493 # install.packages(&quot;bruceR&quot;)#### library(bruceR) # 基础R编程（自动设置文件夹路径、一站式数据导入导出、数据匹配拼接等） # 多变量计算（极简化计算多题项平均分/总分及反向计分、数值重新编码等） # 信度与因素分析（量表信度分析、探索性因素分析EFA、主成分分析PCA、验证性因素分析CFA） # 描述与相关分析（描述统计、频数统计、相关分析、相关系数差异性检验） # t检验与方差分析（单样本/独立样本/配对样本t检验、多因素被试间/被试内/混合设计方差分析ANOVA、简单效应检验与多重比较） # 普通与多层回归分析（多种回归模型结果的完整报告与表格输出、多层线性模型HLM补充分析） # 中介与调节效应分析（普通与多水平的中介效应、调节效应、有调节的中介效应等，包括简单斜率分析、条件中介作用、链式中介作用、跨层调节作用等） # 统计与绘图辅助工具（总均值/组均值中心化处理、时间序列交叉相关分析与格兰杰因果检验、ggplot2绘图主题theme_bruce、颜色卡/配色方案展示等） ## 数据清理与导入： # 导入 data=import(&quot;MyData.csv&quot;) export(data, file=&quot;NewData.csv&quot;) # 一次同时导出两个数据集到Excel export(list(airquality, npk), sheet=c(&quot;air&quot;, &quot;npk&quot;), file=&quot;Two_Datasets.xlsx&quot;) ## 打印三线表(表格或者统计模型)到word或者控制台； dt=as.data.table(psych::bfi) dt[, `:=`( E=MEAN(dt, &quot;E&quot;, 1:5, rev=c(&quot;E1&quot;, &quot;E2&quot;), likert=1:6), A=MEAN(dt, &quot;A&quot;, 1:5, rev=&quot;A1&quot;, likert=1:6), C=MEAN(dt, &quot;C&quot;, 1:5, rev=c(4, 5), likert=1:6), N=MEAN(dt, &quot;N&quot;, 1:5, likert=1:6), O=MEAN(dt, &quot;O&quot;, 1:5, rev=c(2, 5), likert=1:6) )] dt print_table(dt) ## 打印到控制台 ## 打印到word： print_table(head(dt), file=&quot;Results2.doc&quot;) ## 打印模型结果输出： names(iris) model = lm(Sepal.Length ~ Sepal.Width,data = iris) print_table(model, file=&quot;Results23.doc&quot;) ### 优化现有模型输出 summary(model) model_summary(model) GLM_summary(model) ## 描述性统计： ## 参数中plotw = T 时出相关关系图： ## N Mean SD | Median Min Max Skewness Kurtosis install.packages(&quot;GGally&quot;) Describe(iris, plot=F, upper.triangle=TRUE, upper.smooth=&quot;lm&quot;) ## 排序：带sort() &quot;-&quot;（按频数从大到小排列）或&quot;+&quot;（按频数从小到大排列） Freq(iris$Sepal.Length,sort = &quot;-&quot;) ## 带P值矫正的相关性统计： ## 直接输出，不需要再进行一步协方差转换： Corr(airquality) ## 带出图和相关性统计结果：（输出相关系数及其95%置信区间） Corr(airquality, p.adjust=&quot;bonferroni&quot;) ## TTEST()：单样本/独立样本/配对样本t检验 ## （输出效应量Cohen&#39;s d及其95%置信区间、贝叶斯因子BF10） # 参数：test.sided：假设检验的方向，默认是双尾（一般用不到这个参数） # 参数：file：保存的Word文档，默认输出到R控制台 # d1=between.3 d1$Y1=d1$SCORE # 复制一下 d1$Y2=rnorm(32) # 随机数 d1$B=factor(d1$B, levels=1:2, labels=c(&quot;Low&quot;, &quot;High&quot;)) d1$C=factor(d1$C, levels=1:2, labels=c(&quot;M&quot;, &quot;F&quot;)) d2=within.1 ## 单样本t检验 ## TTEST(d1, &quot;SCORE&quot;) TTEST(d1, &quot;SCORE&quot;, test.value=5) ## 独立样本t检验 ## TTEST(d1, &quot;SCORE&quot;, x=&quot;A&quot;) TTEST(d1, &quot;SCORE&quot;, x=&quot;A&quot;, var.equal=FALSE) ## 批量分析多个自变量和因变量： TTEST(d1, y=&quot;Y1&quot;, x=c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;)) TTEST(d1, y=c(&quot;Y1&quot;, &quot;Y2&quot;), x=c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), mean.diff=FALSE, # 不保存原始均值差异 file=&quot;t-result.doc&quot;) ## 配对样本t检验 ## TTEST(d2, y=c(&quot;A1&quot;, &quot;A2&quot;), paired=TRUE) TTEST(d2, y=c(&quot;A1&quot;, &quot;A2&quot;, &quot;A3&quot;, &quot;A4&quot;), paired=TRUE) ## MANOVA()：多因素被试间/被试内/混合设计方差分析ANOVA ## （输出效应量partial η²、generalized η²） # dvs.pattern=&quot;A(.)B(.)&quot;会自动提取&quot;A1B1&quot;、&quot;A1B2&quot;……中的数字作为因素水平； # between：被试间因素的名称 # within：被试内因素的名称 # covariate：协变量的名称 ## sph.correction：重复测量ANOVA中，违背球形假设后的校正方法，续下： ## 默认是&quot;none&quot;，可选择&quot;GG&quot;（Greenhouse-Geisser）或&quot;HF&quot;（Huynh-Feldt） # 补充：P&lt;P0.05 差异有统计学意义 假设成立 两方差的相差有显著意义。 ## 被试间设计 ## between.2 MANOVA(between.2, dv=&quot;SCORE&quot;, between=c(&quot;A&quot;, &quot;B&quot;)) ## 被试内设计 ## within.3 MANOVA(within.3, dvs=&quot;A1B1C1:A2B2C2&quot;, dvs.pattern=&quot;A(.)B(.)C(.)&quot;, within=c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;)) ## 混合设计 ## 这里的混合设计，应该是指组件和组内的设计水平同时检测： mixed.3_2b1w install.packages(&quot;afex&quot;) MANOVA(mixed.3_2b1w, dvs=&quot;B1:B2&quot;, dvs.pattern=&quot;B(.)&quot;, between=c(&quot;A&quot;, &quot;C&quot;), within=&quot;B&quot;) ## 换个变量名 ## data.new=mixed.3_1b2w names(data.new)=c(&quot;Group&quot;, &quot;Cond_01&quot;, &quot;Cond_02&quot;, &quot;Cond_03&quot;, &quot;Cond_04&quot;) MANOVA(data.new, dvs=&quot;Cond_01:Cond_04&quot;, dvs.pattern=&quot;Cond_(..)&quot;, between=&quot;Group&quot;, within=&quot;Condition&quot;) ## 长数据也行 ## ## 这个很有用，典型案例： # 同时比较组内、组件、协方差内数据间的方差波动水平： library(afex) ?afex::obk.long MANOVA(afex::obk.long, subID=&quot;id&quot;, dv=&quot;value&quot;, between=c(&quot;treatment&quot;, &quot;gender&quot;), within=c(&quot;phase&quot;, &quot;hour&quot;), cov=&quot;age&quot;, sph.correction=&quot;GG&quot;) ## EMMEANS()：简单效应检验与多重比较 ## （输出效应量Cohen&#39;s d及其95%置信区间） # effect：待检验的“简单主效应”（如果输入多个，还会报告“简单交互作用”和“简单简单效应”） # by：简单效应检验中的“调节变量”（也可以输入多个，则检验“简单简单效应”） # 在这里管道符传递以后还会继续传递结果，并保存下一个结果： # between.2 MANOVA(between.2, dv=&quot;SCORE&quot;, between=c(&quot;A&quot;, &quot;B&quot;)) %&gt;% EMMEANS(&quot;A&quot;, by=&quot;B&quot;) %&gt;% EMMEANS(&quot;B&quot;, by=&quot;A&quot;) # within.3 MANOVA(within.3, dvs=&quot;A1B1C1:A2B2C2&quot;, dvs.pattern=&quot;A(.)B(.)C(.)&quot;, within=c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;)) %&gt;% EMMEANS(&quot;A&quot;, by=&quot;B&quot;) %&gt;% EMMEANS(c(&quot;A&quot;, &quot;B&quot;), by=&quot;C&quot;) %&gt;% EMMEANS(&quot;A&quot;, by=c(&quot;B&quot;, &quot;C&quot;)) 4.2.4.2 rstatix使用指南 ### 描述统计量 get_summary_stats(): Compute summary statistics for one or multiple numeric variables. Can handle grouped data. freq_table(): Compute frequency table of categorical variables. get_mode(): Compute the mode of a vector, that is the most frequent values. identify_outliers(): Detect univariate outliers using boxplot methods. mahalanobis_distance(): Compute Mahalanobis Distance and Flag Multivariate Outliers. shapiro_test() and mshapiro_test(): Univariate and multivariate Shapiro-Wilk normality test. ### 比较均值 t_test(): perform one-sample, two-sample and pairwise t-tests wilcox_test(): perform one-sample, two-sample and pairwise Wilcoxon tests sign_test(): perform sign test to determine whether there is a median difference between paired or matched observations. anova_test(): an easy-to-use wrapper around car::Anova() to perform different types of ANOVA tests, including independent measures ANOVA, repeated measures ANOVA and mixed ANOVA. kruskal_test(): perform kruskal-wallis rank sum test tukey_hsd(): performs tukey post-hoc tests. Can handle different inputs formats: aov, lm, formula. dunn_test(): compute multiple pairwise comparisons following Kruskal-Wallis test. emmeans_test(): pipe-friendly wrapper arround emmeans function to perform pairwise comparisons of estimated marginal means. Useful for post-hoc analyses following up ANOVA/ANCOVA tests. get_comparisons(): Create a list of possible pairwise comparisons between groups. get_pvalue_position: autocompute p-value positions for plotting significance using ggplot2. ### 促进R的ANOVA计算 factorial_design(): build factorial design for easily computing ANOVA using the car::Anova() function. This might be very useful for repeated measures ANOVA, which is hard to set up with the car package. anova_summary(): Create beautiful summary tables of ANOVA test results obtained from either car::Anova() or stats::aov(). The results include ANOVA table, generalized effect size and some assumption checks, such as Mauchly’s test for sphericity in the case of repeated measures ANOVA. ### 比较方差 levene_test(): Pipe-friendly framework to easily compute Levene’s test for homogeneity of variance across groups. Handles grouped data. box_m(): Box’s M-test for homogeneity of covariance matrices ### 效应值 cohens_d(): Compute cohen’s d measure of effect size for t-tests. eta_squared() and partial_eta_squared(): Compute effect size for ANOVA. cramer_v(): Compute Cramer’s V, which measures the strength of the association between categorical variables. ### 相关分析 计算相关性: cor_test(): correlation test between two or more variables using Pearson, Spearman or Kendall methods. cor_mat(): compute correlation matrix with p-values. Returns a data frame containing the matrix of the correlation coefficients. The output has an attribute named “pvalue”, which contains the matrix of the correlation test p-values. cor_get_pval(): extract a correlation matrix p-values from an object of class cor_mat(). cor_pmat(): compute the correlation matrix, but returns only the p-values of the correlation tests. as_cor_mat(): convert a cor_test object into a correlation matrix format. ## 重塑相关矩阵: cor_reorder(): reorder correlation matrix, according to the coefficients, using the hierarchical clustering method. cor_gather(): takes a correlation matrix and collapses (or melt) it into long format data frame (paired list) cor_spread(): spread a long correlation data frame into wide format (correlation matrix). ## 取子集: cor_select(): subset a correlation matrix by selecting variables of interest. pull_triangle(), pull_upper_triangle(), pull_lower_triangle(): pull upper and lower triangular parts of a (correlation) matrix. replace_triangle(), replace_upper_triangle(), replace_lower_triangle(): replace upper and lower triangular parts of a (correlation) matrix. ## 可视化相关矩阵: cor_as_symbols(): replaces the correlation coefficients, in a matrix, by symbols according to the value. cor_plot(): visualize correlation matrix using base plot. cor_mark_significant(): add significance levels to a correlation matrix. #### 矫正p值和添加显著性标记 adjust_pvalue(): add an adjusted p-values column to a data frame containing statistical test p-values add_significance(): add a column containing the p-value significance level ### 其他 doo(): alternative to dplyr::do for doing anything. Technically it uses nest() + mutate() + map() to apply arbitrary computation to a grouped data frame. sample_n_by(): sample n rows by group from a table convert_as_factor(), set_ref_level(), reorder_levels(): Provides pipe-friendly functions to convert simultaneously multiple variables into a factor variable. make_clean_names(): Pipe-friendly function to make syntactically valid column names (for input data frame) or names (for input vector). 4.2.4.3 eazystat 体系 ## 网站主页; https://www.r-bloggers.com/2022/01/happy-birthday-easystats-a-retrospective/ ##insight:： 这个无依赖的低级帮助程序包提供了一个统一的门户来提取有关各种统计模型的信息。如果您（作为用户或开发人员）曾经对 R 中对象的多样性及其提供的 API 以及如何从这种疯狂中创建 (S3) 方法感到沮丧，那么这正是适合您的软件包！ ## datawizard : 生态系统的最新成员！这个包构成了整个生态系统的数据整理后端，如果您想进行类似 tidyverse的数据整理，但希望避免向您的项目添加依赖，那么它可能正是您正在寻找的 ### 然后，我们有专门的软件包来从统计模型中提取额外的信息。 bayestestR： 一个包，提供用于操作和可视化贝叶斯统计模型的实用工具。 effectize： 一个一站式解决方案，用于计算人类已知的几乎所有效果大小 correlation：专用于计算和可视化的软件包，可能是最全面的相关性套件。 modelbased：一个实用程序包，用于处理统计模型的预测，可用于计算和可视化边际均值或对比分析。 ### 统计模型的信息通过以下两个包进行提取和聚合： performance：计算、分析和测试统计模型的性能。 parameters：提取几乎所有统计模型参数的综合数据框，并提供帮助以优雅的表格和图表呈现它们。 ### 最后，这两个高级软件包利用所有其他软件包以图形或文本方式传达统计分析的结果。 see： 这个包构成了整个生态系统的可视化后端，并与ggplot2接口以支持各种easystats对象的绘图方法。 report：  这个包提供了一种自动化的方式来创建文本报告，详细说明统计分析的结果。 4.2.4.4 finalfit 辅助批运行回归模型： ## 参见资料： ## 类型1： library(finalfit) dependent &lt;- &quot;mort_5yr&quot; explanatory &lt;- c(&quot;ulcer.factor&quot;, &quot;age&quot;, &quot;sex.factor&quot;, &quot;t_stage.factor&quot;) fit2 = melanoma %&gt;% finalfit(dependent, explanatory, metrics = TRUE) ## 类型2： library(finalfit) dependent &lt;- &quot;mort_5yr&quot; explanatory &lt;- c(&quot;ulcer.factor&quot;, &quot;I(age^2)&quot;, &quot;age&quot;) melanoma %&gt;% finalfit(dependent, explanatory, metrics = TRUE) ## 绘制逻辑回归的OR森林图： dependent &lt;- &quot;mort_5yr&quot; explanatory_multi &lt;- c(&quot;ulcer.factor&quot;, &quot;t_stage.factor&quot;) melanoma %&gt;% or_plot(dependent, explanatory_multi, breaks = c(0.5, 1, 2, 5, 10, 25), table_text_size = 3.5, title_text_size = 16) ## 补充说明： finalfit包中函数支持导出的结果到rmarkdown体系中，再经由markdown体系导出到word函数中。 4.3 临床统计模型 4.3.1 平衡匹配 ## 进行平衡匹配的主要原因： # 辛普森悖论： https://www.zhihu.com/people/wang-jiang-yuan-59/posts 4.3.1.1 逆概率加权： # Propensity score IPTW library(MatchIt) library(ggplot2) library(dplyr) library(twang) dt1 &lt;- read.csv(&quot;~/Documents/96 R/riskfactor.csv&quot;) head(dt1) # Pre-requirements for Dataset and Variables # 1. Treatment variable should be binary with value of (0, 1) dt1$DLSTYPN &lt;- ifelse(dt1$DLSTYP==1, 1, 0) # 2. Covariates should be no missing value dt2 &lt;- dt1[complete.cases(dt1$LOGCACBSAVAL,dt1$LOGAACBSAVAL, dt1$CVCBSAVAL), ] nrow(dt2) # 3. All categorical string covariates should be re-coded with numeric values. dt2$DIABMIN &lt;- ifelse(dt2$DIABMI==&quot;Y&quot;, 1, 0) dt2$HYPMIN &lt;- ifelse(dt2$HYPMI==&quot;Y&quot;, 1, 0) dt2$CPBCMN &lt;- ifelse(dt2$CPBCM==&quot;Y&quot;, 1, 0) dt2$NCPBCMN &lt;- ifelse(dt2$NCPBCM==&quot;Y&quot;, 1, 0) dt2$STNCMN &lt;- ifelse(dt2$STNCM==&quot;Y&quot;, 1, 0) dt2$VITDCMN &lt;- ifelse(dt2$VITDCM==&quot;Y&quot;, 1, 0) head(dt2) # Step 1: Run logistic regression to get PS param &lt;- matchit(DLSTYPN ~ DLAGE+AGE+SEXN+BMIMI+MAPMI+PPMI+SMOKE1N+DIABMIN +HYPMIN+BSTGMI+BSHDLMI+BSLDLMI+LOGBSCRPMI +AVGCACAT+AVGP+AVGPTH+LOGAVGFGFMI+AVGOHDMI +CPBCMN+NCPBCMN+STNCMN+VITDCMN +LOGCACBSAVAL+LOGAACBSAVAL+CVCBSAVAL, data=dt2) dt2$param_ps &lt;- param$distance head(dt2) nrow(dt2) # Step 2: Check the distribution of PS ggplot(data=dt2, aes(param_ps, fill=factor(DLSTYPN))) + geom_histogram(binwidth=0.01, alpha=0.5, position=&quot;identity&quot;) # Step 3 (Optional): Trim &lt;2.5 percentiles and &gt;97.5 percentiles of PS p2_5_cutoff &lt;- quantile(dt2$param_ps, 0.025) p97_5_cutoff &lt;- quantile(dt2$param_ps, 0.975) dt3 &lt;- dt2 %&gt;% filter(param_ps &gt;=p2_5_cutoff, dt2$param_ps &lt;= p97_5_cutoff) nrow(dt3) # Step 4: Calculate IPTW with stabilization dt3$iptw &lt;- ifelse(dt3$DLSTYPN==1, (mean(dt3$param_ps))/dt3$param_ps, (mean(1-dt3$param_ps))/(1-dt3$param_ps)) # Step 5: Check the balance of ASD before and after IPTW iptw &lt;- dx.wts(dt3$iptw, data=dt3, vars=c(&quot;DLAGE&quot;, &quot;AGE&quot;, &quot;SEXN&quot;, &quot;BMIMI&quot;,&quot;MAPMI&quot;,&quot;PPMI&quot;,&quot;SMOKE1N&quot;, &quot;DIABMIN&quot;,&quot;HYPMIN&quot;,&quot;BSTGMI&quot;,&quot;BSHDLMI&quot;,&quot;BSLDLMI&quot;, &quot;LOGBSCRPMI&quot;,&quot;AVGCACAT&quot;,&quot;AVGP&quot;,&quot;AVGPTH&quot;,&quot;LOGAVGFGFMI&quot;, &quot;AVGOHDMI&quot;, &quot;CPBCMN&quot;,&quot;NCPBCMN&quot;,&quot;STNCMN&quot;,&quot;VITDCMN&quot;, &quot;LOGCACBSAVAL&quot;,&quot;LOGAACBSAVAL&quot;,&quot;CVCBSAVAL&quot;), treat.var=&quot;DLSTYPN&quot;,estimand=&quot;ATT&quot;) bal.table(iptw) # install.packages(&quot;twang&quot;) library(twang) # Step 6: Include IPTW as a weighting variable to perform outcome analysis # 1. Logistic regression logistic_iptw &lt;- glm(outcome ~ DLSTYPN+DLAGE+AGE+SEXN+BMIMI+MAPMI+PPMI+SMOKE1N +DIABMIN+HYPMIN+BSTGMI+BSHDLMI+BSLDLMI+LOGBSCRPMI +AVGCACAT+AVGP+AVGPTH+LOGAVGFGFMI+AVGOHDMI +CPBCMN+NCPBCMN+STNCMN+VITDCMN +LOGCACBSAVAL+LOGAACBSAVAL+CVCBSAVAL, data=dt3, weights=iptw, family=binomial) summary(logistic_iptw) exp(coef(logistic_iptw)) # 2. Cox regression cox_iptw &lt;- coxph(outcome = Surv(time, status) ~ DLSTYPN+DLAGE+AGE+SEXN+BMIMI +MAPMI+PPMI+SMOKE1N +DIABMIN+HYPMIN+BSTGMI+BSHDLMI+BSLDLMI +LOGBSCRPMI+AVGCACAT+AVGP+AVGPTH+LOGAVGFGFMI+AVGOHDMI +CPBCMN+NCPBCMN+STNCMN+VITDCMN +LOGCACBSAVAL+LOGAACBSAVAL+CVCBSAVAL, data=dt3, weights=iptw) summary(cox_iptw) 4.3.1.2 psm倾向性评分匹配： ## 补充说明： ## 倾向于评分平衡匹配： ## 构造数据集： baseline_group2 &lt;- read_csv(&quot;/workspace/dev/glp1/data/analysisdata/baseline_group2.csv&quot;) %&gt;% dplyr::select(-X1) baseline_group2&lt;- as.data.frame(baseline_group2) baseline_group2[,c(7:49)] &lt;- lapply(baseline_group2[,c(7:49)],as.integer) covarsname &lt;- colnames(baseline_group2) x &lt;- baseline_group2[,covarsname] w &lt;- x[,&#39;test&#39;] ## 分组变量： x &lt;- x[,-c(1:4,9,7)] ## 建模变量： ################# 倾向性得分匹配： ############### library(&quot;MatchIt&quot;) library(&quot;tableone&quot;) library(&#39;survival&#39;) library(Matching) ## 方法1：Matching方法： library(MASS) library(Matching) bout.nm &lt;- MatchBalance(w~x, match.out = NULL,ks=FALSE) bal.nm &lt;- baltest.collect(matchbal.out = bout.nm,var.names = colnames(x),after = FALSE) round(bal.nm,3) ## 方法2：matchit方法： ## 构建普通的psm： m.out &lt;- matchit(treat ~ educ + age + race, data = lalonde, method = &quot;nearest&quot;,ratio=1,caliper=0.01) m.data &lt;- match.data(m.out) ## 参数介绍： # method =&#39;exact&#39;： 精确匹配；或者使用 &quot;nearest&quot;最近邻匹配； # ratio=1 : 比例1:1 ## 卡钳值： # caliper=0.001：设置的越小，越能够精确匹配； # caliper = c(.1, age = 2, educ = 1) ##针对具体的参数来定义； ## 熵平衡： install.packages(&#39;ebal&#39;) library(ebal) # create toy data: treatment indicator and three covariates X1-3 w &lt;- c(rep(0,50),rep(1,30)) x &lt;- rbind(replicate(3,rnorm(50,0)),replicate(3,rnorm(30,.5))) %&gt;% data.frame() colnames(x) &lt;- paste(&quot;x&quot;,1:3,sep=&quot;&quot;) # entropy balancing eb.out &lt;- ebalance(Treatment=w,X=x) # 结果提取： x$weight &lt;- c(eb.out$w,w[w==1]) # 平衡评估： bout.eb &lt;- MatchBalance(w~x, weights =c(eb.out$w,w[w==1]),ks=FALSE) bal.eb &lt;- baltest.collect(matchbal.out = bout.eb,var.names = colnames(x),after = FALSE) round(bal.eb,3) ################## 应用权重与检测平衡可靠性： library(Matrix) library(grid) library(survival) library(survey) xx = cbind(w,x) des1 &lt;- svydesign(ids = ~1, weights = ~weight,data =xx) vars = paste(&quot;x&quot;,1:3,sep=&quot;&quot;) tab &lt;- svyCreateTableOne(vars = vars,,strata = &#39;w&#39;,data = des1) ## 出具smd print(tab, showAlllevels = TRUE,catDigits = 2, contDigits = 2,smd = TRUE) 4.3.1.3 熵平衡匹配 install.packages(&#39;ebal&#39;) library(ebal) ### 熵平衡不改变1，只改变0 的权重值： # create toy data: treatment indicator and three covariates X1-3 w &lt;- c(rep(0,50),rep(1,30)) x &lt;- rbind(replicate(3,rnorm(50,0)),replicate(3,rnorm(30,.5))) %&gt;% data.frame() colnames(x) &lt;- paste(&quot;x&quot;,1:3,sep=&quot;&quot;) # entropy balancing eb.out &lt;- ebalance(Treatment=w,X=x) # 结果提取： x$weight &lt;- c(eb.out$w,w[w==1]) # 平衡评估： bout.eb &lt;- MatchBalance(w~x, weights =c(eb.out$w,w[w==1]),ks=FALSE) bal.eb &lt;- baltest.collect(matchbal.out = bout.eb,var.names = colnames(x),after = FALSE) round(bal.eb,3) # 应用权重： library(Matrix) library(grid) library(survival) library(survey) xx = cbind(w,x) des1 &lt;- svydesign(ids = ~1, weights = ~weight,data =xx) vars = paste(&quot;x&quot;,1:3,sep=&quot;&quot;) tab &lt;- svyCreateTableOne(vars = vars,,strata = &#39;w&#39;,data = des1) ## 出具smd print(tab, showAlllevels = TRUE,catDigits = 2, contDigits = 2,smd = TRUE) 4.3.1.4 多组倾向得分匹配 install.packages(&quot;twang&quot;) library(twang) data(AOD) mnps.AOD &lt;- mnps(treat ~ illact + crimjust + subprob + subdep + white, data = AOD, estimand = &quot;ATE&quot;, verbose = FALSE, stop.method = c(&quot;es.mean&quot;, &quot;ks.mean&quot;), n.trees = 3000) bal.table(mnps.AOD, digits = 2) summary(mnps.AOD) AOD$w &lt;- get.weights(mnps.AOD, stop.method = &quot;es.mean&quot;) design.mnps &lt;- svydesign(ids=~1, weights=~w, data=AOD) 4.3.2 生存分析 ## 参考资料: (1) https://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html#Part_3:_Competing_Risks (2) https://www.drizopoulos.com/courses/emc/survival%20analysis%20in%20r%20companion (3) 分布族描述： https://devinincerti.com/code/survival-distributions.html （4） 这本书把所有的生存分析与R都讲的清清楚楚： http://ehar.se/r/ehar2/parametric.html#the-piecewise-constant-proportional-hazards-model image-20220903133057292 4.3.2.1 KM曲线 4.3.2.1.1 2.1.1 常见比较检验方法： 对KM生存函数有三种常见检验方法：log-rank、breslow、tarone。这三种都属于卡方检验的方法。KM法根据观察时点的顺序，把生存资料从小到大排列，依次计算实际死亡数和预期死亡数。并根据公式计算卡方统计量，复合（自由度=组数-1）的卡方分布。不同的是，不同的方法在计算统计量的时候，赋予了不同的权重。 # 对数秩（Log-Rank）检验 各时点的权重均为“1”。就是不考虑各观察时点开始时存活的人数对统计模型的影响。也就是每个时点死亡情况的变化对整个模型的贡献是一样的。 # Breslow检验 在Log Rank检验的基础上增加了权重，并设置权重为各时点开始时存活的人数。也就是开始存活人数多的时点死亡情况的变化对整个模型的贡献较大，而开始存活人数少的时点死亡情况的变化对整个模型的贡献较小。 # Tarone-Ware检验 是权重的取值方法介于以上两种方法之间，设置权重为各时点开始时存活的人数的平方根。同样是开始存活人数多的时点死亡情况的变化对整个模型的贡献较大，而开始存活人数少的时点死亡情况的变化对整个模型的贡献较小。只是开始存活人数多的时点对整个模型的贡献不如Breslow检验大。 简而言之，log-rank法侧重于远期差别，breslow法侧重于近期差别，tarone法介于两者之间。 Kaplan-Meier曲线采用Log-rank 检验方法对组间差异进行比较，采用Possion分布计算事件发生率及其置信区间。Log-rank检验的零假设是指两组的生存时间分布完全一致，当p&lt;0.05时，就可以认为两组的生存时间分布存在统计学差异。 其中生存曲线的组间比较常采用Log-rank检验（对远期差异敏感）和Wilcoxon检验（对近期敏感）。 library(survival) library(survminer) # Fit survival curves fit &lt;- survfit(Surv(time, status) ~ sex, data = lung) # log-rank检验： # 本质上是时间节点下的列联表卡方检验： # 当满足比例风险 (PH) 假设时，对数秩检验是最有效的检验 # 即检验不同组受试者的生存函数是否在统计学上有显着差异的假设 surv_diff &lt;- survdiff(Surv(time, status) ~ sex, data = lung) ## 模型提取的p值为log-rank检验结果： p.value &lt;- 1 - pchisq(surv_diff$chisq, length(surv_diff$n) -1) p.value &lt;- p.value %&gt;% format(round(.,digits =3),nsmall=3) ## Peto &amp; Peto Gehan-Wilcoxon 检验 peto_peto &lt;- survdiff(Surv(time, status == 2) ~ sex, data = lung, rho = 1) peto_peto # Breslow检验： survdiff(Surv(time, status) ~ sex, rho = 1) ## 使用surv_pvalue的method包含了多种方法： surv_pvalue(fit, method = &quot;Gehan-Breslow&quot;) ## 另外一种实现方法： Br_fit &lt;- survfit(Surv(time, status) ~ 1, data = stanford2, type = &quot;fleming-harrington&quot;) Br_fit 4.3.2.1.2 2.1.2 RR值计算及可视化： ## 续上（单变量）： require(&quot;survival&quot;) fit &lt;- survfit(Surv(time, status) ~ sex, data = lung) ## 写一个计算RR的函数： # 举例：ac为一组，bd为一组，ab为有；cd为无； # RR = (a/(a+b)) / (c/(c+d)) options(scipen = 3) rr_ca = function(fit){ n.event = fit$n.event stra = fit$strata %&gt;% data.frame() data = cbind(c(rep(1,stra$.[1]),rep(2,stra$.[2])),n.event) %&gt;% data.frame() %&gt;% rename(&quot;cla&quot; = V1) %&gt;% mutate(z01 = ifelse(n.event&gt;0,1,0)) a = data %&gt;% filter(cla==1 &amp; z01==1) %&gt;% dim() %&gt;% .[1] c = data %&gt;% filter(cla==1 &amp; z01==0) %&gt;% dim() %&gt;% .[1] b = data %&gt;% filter(cla==2 &amp; z01==1) %&gt;% dim() %&gt;% .[1] d = data %&gt;% filter(cla==2 &amp; z01==0) %&gt;% dim() %&gt;% .[1] rr = (a/(a+b)) / (c/(c+d)) ## 计算上下限： rr_mat = matrix(c(a,b,c,d),byrow=TRUE,nrow=2) # install.packages(&quot;epitools&quot;) library(&quot;epitools&quot;) rr_ur = epitab(rr_mat,method=&quot;riskratio&quot;) rr_ur_out = rr_ur$tab %&gt;% data.frame() %&gt;% select(lower,upper,p.value) %&gt;% rows_delete(tibble(lower=NA)) %&gt;% mutate(lower = format(round(lower,digits =2),nsmall=2), upper = format(round(upper,digits =2),nsmall=2)) %&gt;% data.frame() RR = paste0(&quot;RR=&quot;,round(rr,2),&quot;,&quot;,&quot;95%CI[&quot;,rr_ur_out$lower,&quot;,&quot;,rr_ur_out$upper,&quot;]&quot;) return(RR) } rr = rr_ca(fit) ## 添加p值和RR: fig = ggsurvplot(fit, fun=&quot;event&quot;, palette=c(&quot;#B47846&quot;,&quot;#4682B4&quot;), legend.title = &quot;Sex&quot;, legend.labs=c(&quot;baseline&quot;,&quot;followup&quot;), legend= c(&quot;right&quot;), risk.table = F, conf.int = F, risk.table.height=0.32, ## 参数配置： title=&quot;Kaplan-Meier Curve for MI&quot;, # legend.title=&quot;Period&quot;, xlab=&quot;Time(Days)&quot;, ylab=&quot;Patients with an event(%)&quot;) fig$plot + ggplot2::annotate(&quot;text&quot;,x = 200, y = 0.95 ,size =5,label = paste0(&#39;P=&#39;,p.value)) + ggplot2::annotate(&quot;text&quot;,x = 200, y = 0.90 ,size =5,label = rr ) + theme(legend.title = element_text(size=15), legend.text = element_text(size = 15)) ## 多变量生成组图可视化： library(&quot;survival&quot;) library(&quot;survminer&quot;) require(&quot;survival&quot;) fit2 &lt;- survfit( Surv(time, status) ~ sex + rx + adhere, data = colon ) ggsurv &lt;- ggsurvplot(fit2, fun = &quot;event&quot;, conf.int = TRUE, ggtheme = theme_bw()) ggsurv$plot +theme_bw() + theme (legend.position = &quot;right&quot;)+ facet_grid(rx ~ adhere) 4.3.2.1.3 2.1.3 landmark分析 landmark分析关注的是事件结局中当KM曲线相交时的解决办法： 具体实施： 将cox针对time进行分组组合数据，然后分段拟合和求RR值和p值； 再之后将分段km曲线拟合到一张图中即可； 4.3.2.1.4 2.1.4 生成中位生存时间 ## 计算中位生存时间： lung %&gt;% filter(status == 1) %&gt;% summarize(median_surv = median(time)) ## 可视化报表： survfit(Surv(time, status) ~ 1, data = lung) %&gt;% tbl_survfit( probs = 0.5, label_header = &quot;**Median survival (95% CI)**&quot; ) 4.3.2.2 COX回归 ## 参见资料： # https://rviews.rstudio.com/2017/09/25/survival-analysis-with-r/ https://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html#(x)-year_survival_and_the_survival_curve # 论文资料： https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4645729/ https://ekja.org/journal/view.php?number=8532 4.3.2.2.1 构建cox回归模型检验（比例） 4.3.2.2.1.1 cox回归构建方法原理 ## 参见资料： https://zhuanlan.zhihu.com/p/53847644 cox回归用到的参数变量 image-20220905103709180 cox回归基于偏似然回归求解 β值： （1）回归求参数值，原理的h(t)是一个不指定分布族的非参数变量； （2）求每个协变量的 β值不需要基于非参数h(t)就可以求解，因为在对数转换过程中被消去了。 （3）从对数转换后的cox回归解释中，可以看到【风险比值的对数与时无关，只与胁逼哪里的线性组合有关。如果该假设不成立，就需要使用时依cox回归】 (4) 参见： https://zhuanlan.zhihu.com/p/97501833 如果我们认为h(t)是常数c，那么存活时间就满足参数为c的指数分布。如果认为logh(t) = a + bt，那么存活时间满足Gompertz分布，如果认为logh(t) = a + log(t)。那么存活时间满足Weibull分布。 （5）关于利用最大似然方法求解 β值时补充解释： 最大似然依赖于先验背景和θ值。而已有的推导表明三种删失方法都不会影响θ的界定，也即无论哪种删失（限定在常用的三种删失）所推导得到的基于似然估计得到的总体概率分布是相对固定的，也即求解β值的解并不受到删失类型的影响。 image-20220905104115551 回归参数的解读 ## 基于上述求对数消除的方法就可以获得每个协变量的 β值，则利用h(t)函数求解 HR = eβ( β为指数)，则ln(HR) = β; 因此，如果 β&gt;0,则HR&gt;1为危险因素；反之，当 β&lt;0,则HR&lt;1，为保护性因素。 image-20220905105320020 4.3.2.2.1.2 参数解释和先验假设 ## 1前提配置：----- ## 1、状态结局： # 一般结局为0-1，其中1的另外一种可能来自于删失； # 所谓删失是指患者在中途因意外因素退出试验； # Cox回归主要探讨什么样的患者死亡的更快，什么因素影响了患者死亡的速度。 # 删失定义； 生存结局（status）一般分为「死亡」和删失两类。「死亡」指的是我们感兴趣的终点事件（如白血病患者死亡、冠心病患者第二次发病）。除此之外的结局或生存结局则归类为删失（censoring），也称为截尾或终检。 删失的一般原因有： 1. 研究截至日期时，感兴趣终点事件仍未出现 2. 失访，不知道感兴趣终点事件何时发生或是否会发生 3. 因各种原因中途退出 4. 死于其它「事件」，如交通意外或其他疾病； 删失分为左删失、右删失和中间删失： ## 右删失分为三种类型： # 一型删失： 在一型删失方案中，试验中收集失效部件的失效时间，一旦试验进程到达提前设计的停止时间T，则删失所有剩余未失效部件，并停止试验进程。但此时收到的失效数据的数据量k是一个随机数。 # 二型删失： 在二型删失方案中，试验会提前给定收集到的失效数据量n，则试验会在收集到提前给定数据的数据量时停止。相较于一型删失方案，二型的优点是能够收集到固定数量的删失数据，但由于试验时间不可控制，为收集大足够多的数据会导致试验时间过长和成本大大提高。 # III型删失（Type III censoring）： 不同起点，无固定终点 在实际的研究过程中，往往不能保证所有研究对象在同一时间同时进入研究，在研究开始后，随着研究对象的陆续招募进入研究，不同研究对象的观察起始时间有先有后。同时，在研究结束前，有些研究对象已经发生终点事件，可以记录其准确的生存时间，但也有些研究对象中途退出研究，或者在研究结束时仍然未发生终点事件，他们的生存时间无法明确。这种观察起始时间和删失时间均不相同的类型，称之为III型删失，也是临床研究中最为常见的类型。由于删失数据往往是随机发生的，因此III型删失也称为随机删失（Random censoring）。 ## 2、模型参数解释 # 1、HR值： # HR值：Cox模型用来衡量某个因素对于生存的影响程度，HR就是影响程度的量化。 # 如果HR=1，则X是0还是1对于生存状况无影响，如果HR&gt;1,则X为1时会增加死亡的风险， # 如果HR&lt;1，则X为1是会减少死亡的风险。 # 当自变量增大一个单位时，对于COX回归，改变了风险比HR的自然对数值。 # 2、相关性分析： # 3）相关性分析：Cox的单因素回归分析时p&lt;0.05，但是用于多因素回归时会出现p&gt;0.05的情况。 # 这个时候往往是该因素和其他因素有一定的相关性， # 需要去做进一步的相关性分析。 ## 3、模型构建的先验假设： #（3.1）比例风险假设：又称PH假设，指模型自变量系数为固定值，不随时间T的变化而变化。 vet &lt;- mutate(veteran, AG = ifelse((age &lt; 60), &quot;LT60&quot;, &quot;OV60&quot;), AG = factor(AG), trt = factor(trt,labels=c(&quot;standard&quot;,&quot;test&quot;)), prior = factor(prior,labels=c(&quot;N0&quot;,&quot;Yes&quot;))) ## 观测协变量如何随时间变化： ## 适合删失数据的Aalen加性回归模型 aa_fit &lt;-aareg(Surv(time, status) ~ trt + celltype + karno + diagtime + age + prior , data = vet) ## 使用autoplot的前提是需要配置好分类变量和连续变量的属性： cox %&gt;% autoplot() ## 使用cox.zph测试比例风险假设（针对每个变量） fit &lt;- coxph(Surv(time, status) ~ trt + celltype + karno + diagtime + age + prior , data = veteran) test.ph = cox.zph(fit) ## p值小于0.05时该变量不符合使用Schoenfeld 假设； ## 通过可视化方法来校验Schoenfeld分布： ggcoxzph(test.ph) ## 当变量违反该假设时，使用分层或者交互来解决： # 添加covariate×time相互作用。 # 分层（对令人讨厌的混杂因素有用）。 # 还有有一种方法是使用时间加速失效模型（Accelerated Failure Time Models，AFT） #（3.2）对数线性假设： 模型中对数风险比值Ln(HR)与自变量X呈线性关系。 # 这个假设检验通常被忽略； ## 4 通过可视化偏差的方法来检查纳入数据集的异常值： ggcoxdiagnostics(fit, type = &quot;deviance&quot;, linear.predictions = FALSE, ggtheme = theme_bw()) 4.3.2.2.1.3 构建cox模型的两种方法 ## 构建模型方法1： library(survey) library(survival) library(survminer) cox &lt;- coxph(Surv(time, status) ~ trt + celltype + karno + diagtime + age + prior , data = veteran) summary(cox) cox_fit &lt;- survfit(cox) plot(cox_fit, main = &quot;cph model&quot;, xlab=&quot;Days&quot;) ## 构建模型方法2:： #用 cph()函数来拟合Cox比例风险回归模 res.cox &lt;- cph(Surv(time, status) ~ age + sex, data = lung) #构建生存函数 med &lt;- Quantile(res.cox) # 计算中位生存时间 surv &lt;- Survival(res.cox) # 构建生存概率函数 #绘图：预测中位生存时间 nom &lt;- nomogram(res.cox, fun=function(x) med(lp=x), funlabel=&quot;Median Survival Time&quot;) plot(nom) ## 计算中位生存时间和和生存概率： med &lt;- Quantile(res.cox) # 计算中位生存时间 surv &lt;- Survival(res.cox) # 构建生存概率函数 ## 计算指定时间的生存概率： summary(res.cox, times = c(1000, 2000)) ## 计算指定生存概率累计和的天数： # 在probs参数中quantile()我们必须指定一个减去我们的目标生存概率；这是因为该 # 函数在累积分布函数 (CDF) 约定下工作，并且 CDF 等于 1 减去生存概率 # 找出生存概率等于 0.7 的天数和等于 0.6 的天数 quantile(KM_fit, probs = 1 - c(0.7, 0.6)) ## 预测多年持续生存概率： # 根据模型预测患者1年，3年和5年的生存概率。 t &lt;- c(1*365,3*365,5*365) survprob &lt;- predictSurvProb(cox1,newd=test,times=t) 4.3.2.2.1.4 模型结果解读 # 2、z值代表Wald统计量，其值等于回归系数coef除以其标准误se(coef)，即z = coef/se(coef)；有统计量必有其对应的假设检验的显著性P值，其说明bata值是否与0有统计学意义上的显著差别。 # 补充来说： 标记为“z”的列给出了Wald统计值。它对应于每个回归系数与其标准误差的比率（z = coef / se（coef））。wald统计评估是否beta（ββ）系数在统计上显着不同于0。 # 3、coef(-0.5310)值小于0说明HR值小于1，而这里的Cox模型是group two相对于group one而言的，那么按照测试数据集来说：male=1，female=1，即女性的死亡风险相比男性要低 # # 4、exp(coef)[等价于HR]等于0.59，即风险比例等于0.59，说明女性(male=2)减少了0.59倍风险，女性与良好预后相关 # # 5、Likelihood ratio test，Wald test，Score (logrank) test则是给出了3种可选择的P值，这三者是asymptotically equivalent；当样本数目足够大时，这三者的值是相似的；当样本数目较少时，这三者是有差别的，但是Likelihood ratio test会比其他两种在小样本中表现的更优 4.3.2.2.1.5 模型评估(c-index) # c-index：可以看到Cox回归中还有一个值是Concordance； # 它反映模型的区分能力，考察模型正确预测的能力。 # c-index等于0.5时，表示模型的预测结果是随机的，没有任何预测作用，0.5-0.7为低准确度， # 0.7-0.9表示中等准确度，0.9以上表示高准确度。 ## base-R中：rcorrcens已被删除： rcorrcens(Surv(time,status) ~ predict(res.cox), data =lung) ## Hmisc::rcorr.cens() data &lt;- read.csv(&quot;survivaldta.csv&quot;) library(Hmisc) library(survival) f &lt;- cph(Surv(time,death)~x1+x2+x3,data=survivldata) fp &lt;- predict(f) cindex.orig=1-rcorr.cens(fp,Surv(time,death)) ## survival::survConcordance data &lt;- read.csv(&quot;survivaldta.csv&quot;) library(survival) fit &lt;- coxph(Surv(time,death)~x1+x2+x3,data=survivldata) sum.surv &lt;- summary(fit) c_index &lt;-sum.surv$concordance c_index ## concordance.concordant se.std(c-d) ## 0.54469239 0.02788881 # or using the function survConcordance to get c-index c_index &lt;- survConcordance(Surv(time,death)~predict(fit,survivldata))$concordance 4.3.2.2.1.6 绘制多变量列线图 Cox比例风险模型也是多因素回归模型的一种，在考虑结局时，还加入了时间因素的影响。 列线图（Alignment Diagram），又称诺莫图（Nomogram图），用来把多因素回归分析结果（logistic回归和cox回归）用图形方式表现出来，将多个预测指标进行整合，然后采用带有刻度的线段，按照一定的比例绘制在同一平面上，从而用以表达预测模型中各个变量之间的相互关系。 根据模型中各个影响因素对结局变量的贡献程度（回归系数的大小），给每个影响因素的每个取值水平进行赋分，然后再将各个评分相加得到总评分，最后通过总评分与结局事件发生概率之间的函数转换关系，从而计算出该个体结局事件的预测值。 ## 列线图解读： 就那上图来说。我们想知道年龄45岁 (age=45) 的女性（sex=1）的患病风险，只需要将age=45岁向“ points（评分）”轴投射，则：points=50；同理 sex=1 时，points≈37。两者相加则“ Total points=87”；将此数值在“ Total points ”轴上向“ Risk ”概率轴投射，则可知风险大概在0.4和0.5之间。（参见图中红线）。 ## 方法1： library(survival) library(rms) dd &lt;- datadist(lung) options(datadist = &quot;dd&quot;) coxfit &lt;- cph(Surv(time, status) ~ age + sex + ph.ecog + ph.karno + pat.karno, data = lung, x=T,y=T,surv = T ) # 构建生存函数，注意你的最大生存时间 surv &lt;- Survival(coxfit) surv1 &lt;- function(x) surv(365,x) # 1年OS surv2 &lt;- function(x) surv(365*2,x) # 2年OS nom &lt;- nomogram(coxfit, fun = list(surv1,surv2), lp = T, funlabel = c(&#39;1-year survival Probability&#39;, &#39;2-year survival Probability&#39;), maxscale = 100, fun.at = c(0.95,0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1)) ## 绘制列线图： plot(nom, lplabel=&quot;Linear Predictor&quot;, xfrac = 0.2, # 左侧标签距离坐标轴的距离 #varname.label = TRUE, tcl = -0.2, # 刻度长短和方向 lmgp = 0.1, # 坐标轴标签距离坐标轴远近 points.label =&#39;Points&#39;, total.points.label = &#39;Total Points&#39;, cap.labels = FALSE, cex.var = 1, # 左侧标签字体大小 cex.axis = 1, # 坐标轴字体大小 col.grid = gray(c(0.8, 0.95))) # 竖线颜色 ## 方法2：动态弹出： library(DynNom) coxfit &lt;- cph(Surv(time, status) ~ age + sex + ph.ecog + ph.karno + pat.karno, data = lung, x=T,y=T,surv = T) DynNom(coxfit, DNxlab = &quot;Survival probability&quot;, KMtitle=&quot;Kaplan-Meier plot&quot;, KMxlab = &quot;Time (Days)&quot;, KMylab = &quot;Survival probability&quot;) ## 方法3： library(regplot) coxfit &lt;- cph(Surv(time, status) ~ age + sex + ph.ecog + ph.karno + pat.karno,data = lung, x=T,y=T,surv = T) regplot(coxfit, plots = c(&quot;violin&quot;, &quot;boxes&quot;), ##连续性变量形状，可选&quot;no plot&quot; &quot;density&quot; &quot;boxes&quot; &quot;ecdf&quot; &quot;bars&quot; &quot;boxplot&quot; &quot;violin&quot; &quot;bean&quot; &quot;spikes&quot;；分类变量的形状，可选&quot;no plot&quot; &quot;boxes&quot; &quot;bars&quot; &quot;spikes&quot; observation = lung[1,], #用哪行观测，或者T F center = T, # 对齐变量 subticks = T, droplines = T,#是否画竖线 title = &quot;nomogram&quot;, points = T, # 截距项显示为0-100 odds = T, # 是否显示OR值 showP = T, # 是否显示变量的显著性标记 rank = &quot;sd&quot;, # 根据sd给变量排序 interval=&quot;confidence&quot;, # 展示可信区间 clickable = F # 是否可以交互) ## 方法4： library(VRPM) library(survival) cox_fit &lt;- coxph(Surv(time, status) ~ age + sex + ph.ecog + ph.karno + pat.karno, data = lung,model = T) colplot(cox_fit,coloroptions = 3,filename = &quot;cox.png&quot;) 4.3.2.2.1.7 可视化多变量条件下连续变量与累计风险的关系： ## 参见资料： https://zhuanlan.zhihu.com/p/370963253 library(ggplot2) library(titanic) library(rms) library(survival) rm(list = ls()) data &lt;- lung[, c(&quot;time&quot;, &quot;status&quot;, &quot;age&quot;, &quot;sex&quot;, &quot;ph.karno&quot;)] data &lt;- na.omit(data) data$sex &lt;- factor(data$sex, levels = c(1, 2)) fit &lt;- coxph(Surv(time, status) ~ age + sex + ph.karno, data = data) fit dd &lt;- datadist(data) options(datadist = &quot;dd&quot;) dd$limits$age[2] &lt;- 0 # 指定参考点 ## 使用rms提供的样条样本采样方法获得： fit &lt;- update(fit) HR &lt;- Predict(fit, age = seq(40, 80, 1), fun = exp, ref.zero = T) ggplot(HR) + geom_line(data = dt, aes(x = age, y = HR, color = &quot;HR&quot;), linetype = 2) + geom_line(data = dt, aes(x = age, y = HR_lwr, color = &quot;HR_lwr&quot;), linetype = 2) + geom_line(data = dt, aes(x = age, y = HR_upr, color = &quot;HR_upr&quot;), linetype = 2) + labs(color = &quot;&quot;) + theme(legend.position = c(0.8, 0.8)) 4.3.2.2.2 构建时依cox回归模型检验（非比例） 4.3.2.2.2.1 时依协变量 类型 时依协变量（Time-Dependent Covariate），所谓时依协变量，顾名思义指的就是随时间变化而变化解释变量，也有翻译成时变解释变量、时变协变量，我觉得也很不错。 大体时变协变量分为几个情况： - 内在时依协变量：时依协变量是指随时间变化自变量本身发生变化的那些变量，比如有些患者原来是吸烟的，但在随访过程中戒烟了，这种时依协变量被称为内在时依协变量。 - 外在时依协变量：还有一种情况，随着时间的变化，模型中自变量本身取值并未发生改变，但其效应却在发生变化，这种时依协变量被称为外在时依协变量。 - 交互项纳入：有时候我们也会刻意构建一种时依协变量，比如当违背比例风险假定时，我们可以将变量与时间的相乘作为将互项纳入（即使变量本身不一定会随时间变化而变化），这样就可以进行COX回归了。当然我们也可以利用这个原理考察比例风险假定是否满足。 一个典型的例子就是多疗程治疗下用户的死亡时间，如果以用户接受的药剂量来做协变量，则属于一个经典时变变量。 因为用户活得越久，接受大疗程越多，注入的要药剂也越多。换而言之，药剂量在用户的生存期内，是随时间变化的，不像性别这些因素一样保持不变。 这样的问题在用户流失中同样存在，如优惠券的累计发放量，同样与药剂量类似。 对于这种变量，我们需要对原始数据做split。根据变化的时间节点，把原始数据打断为多行。 以用户流失为例，假设某用户在一年内35日、186日、303日分别接受了优惠券： 4.3.2.2.2.2 R 时依COX # 先进行PH检验： library(survival) fit &lt;- coxph(Surv(time, status) ~ factor(trt) + factor(prior) + karno, data = veteran) ## 时依模式下的变量是否符合比例风险假设： zp &lt;- cox.zph(fit, transform = function(time) log(time + 20)) zp ## 如果不符合需要根据需要来修改时间变量的方法： zp2 &lt;- cox.zph(fit, transform = function(time) log(time)) zp2 ## 构建时依变量模型 -- 方法1: ## tt这里是构架了一种处理函数，来转换时间尺度的变化： # 常用转换有三种形式： x*t ; x*log(t); x*log(t+20) fit2 &lt;- coxph(Surv(time, status) ~ factor(trt) + factor(prior) + karno + tt(karno), data = veteran, tt = function(x, t, ...) x*log(t+20)) summary(fit2) ## 构建时依变量模型 -- 方法2: # 注意不能通过添加交互项的方法来为cox回归添加协变量，因为那种方法相当于给指定协变量添加了一个静态变量： 先以unique生存时间为分割点，将每个id拆分成多条记录，然后在拆分的数据集中构建协变量*时间的交互项。拆分数据集可通过survival包的survSplit()函数或Greg包timeSplitter()函数实现。 这里的实现逻辑本质上是放宽了cox回归的假设要求； ## 方法1：使用survive包的survSplit()函数： # id 一般默认为患者id； veteran2 &lt;- survSplit(Surv(time, status) ~., cut = unique(veteran$time), data = veteran, id = &quot;id&quot;) fit3_1 &lt;- coxph(Surv(tstart, time, status) ~ trt + prior + karno + karno : log(time + 20), data = veteran2) summary(fit3_1) ## 方法2：使用Greg包的timeSplitter： veteran$id &lt;- 1:nrow(veteran) veteran4 &lt;- timeSplitter(data = veteran, by = unique(veteran$time), time_var = &quot;time&quot;, event_var = &quot;status&quot;) ## 总结： 1. survSplit()函数和timeSplitter()函数拆分的数据集完全一样，差别在于：1）起止时间变量的命名不同，survSplit()函数为tstart和time，timeSplitter()函数为Start_time和Stop_time；2）survSplit()函数可以通过参数id在拆分数据集中生成变量id，而timeSplitter()函数则需在拆分前手动生成变量id。 2. 以unique生存时间为分割点和以unique事件发生的生存时间为分割点，回归结果是一样的，只不过以unique生存时间为分割点的拆分数据集会更大。 3、tt参数构建的模型占用的内存相对更大（fit5），所以在数据量较大的情况下可以优先选择先拆分数据集再构建模型的方式。 4、最后要说明一点，在不考虑时依协变量的情况下，是否拆分数据集以及分割点的选择，对回归结果没有影响。 4.3.2.2.3 构建分段cox回归模型检验（非比例） 虽然风险比例假定在整个随访时间内不成立，但在一个较短的时间段内则可能是成立的，分段模型的逻辑就是把整个生存时间拆分成多个时间段，每一段拟合一个比例风险模型。 ## 第一种情况： 很多案例常从中位生存时间处下刀，SPSS中进行含时依协变量的COX回归时，会给出模型的-2 Log Likelihood值，根据模型“smaller-is-better”的信息准则格式，可以采用尝试法，选用-2LL值最小的模型对应的时间点为分割点，当然我们还需要兼顾样本量的问题【EVP原则：10 outcome events per variable】。 ## 参见资料： https://cran.r-project.org/web/packages/eha/vignettes/tpchreg.html https://cran.r-project.org/web/packages/eha/vignettes/tpchreg.html ## 分段cox回归： ## 本质上是一种中间删失的cox数据组： ## 使用surv函数来定义：t oldmort$birthyear &lt;- floor(oldmort$birthdate) - 1800 ## Surv(time, time2, event,type=c(&#39;right&#39;,&#39;interval&#39;)) om &lt;- toTpch(Surv(enter, exit, event) ~ sex + civ + birthyear, cuts = seq(60, 100, by = 2), data = oldmort) head(om) oldpar &lt;- par(mfrow = c(1, 2), las = 1, lwd = 1.5) plot(fit3, fn = &quot;haz&quot;, log = &quot;y&quot;, main = &quot;log(hazards)&quot;, xlab = &quot;Age&quot;, ylab = &quot;log(Deaths / Year)&quot;, col = &quot;blue&quot;) plot(fit3, fn = &quot;haz&quot;, log = &quot;&quot;, main = &quot;hazards&quot;, xlab = &quot;Age&quot;, ylab = &quot;Deaths / Year&quot;, col = &quot;blue&quot;) 4.3.2.3 竞争风险研究（Fine-gray） 4.3.2.3.1 竞争风险统计背景 ## 定义； 竞争风险模型( Competing Risk Model ) : 指的是在观察队列中，存在某种已知事件可能会影响另一种事件发生的概率或者是完全阻碍其发生,则可认为前者与后者存在竞争风险。 所谓竞争风险模型（Competing Risk Model）是一种处理多种潜在结局生存数据的分析方法，早在1999年Fine和Gray就提出了部分分布的半参数比例风险模型，通常使用的终点指标是累积发生率函数（Cumulative incidence function，CIF）。 作者：咩咩羊不是羊 https://www.bilibili.com/read/cv14611796 出处：bilibili # 当受试者在事件发生时间设置中有多个可能事件时 例子：复发、死于疾病等 例如，可以想象复发的患者更有可能死亡，因此复发时间和死亡时间不会是独立事件。 # 竞争风险的背景： 1、给定事件的特定原因危害：这表示事件在没有因其他事件而失败的事件中的每单位时间发生率 2、给定事件的累积发生率：这表示事件单位时间的发生率以及竞争事件的影响 ## 竞争风险模型分析原因： 竞争风险模型:适用于多个终点的生存数据，是一种处理多种潜在结局生存数据的分析方法,通过计算每个结局的累积发生率函数( Cumulative Incidences Function , CIF )进行分析。 单一结局的生存分析,主要关注研究对象的生存概率或者死L _风险，这里的生存概率仅仅是这一个结局的生存的概率，而竞争风险中,可能是多个结局共同的总的生存概率或者总的死L _风险,研究的兴趣点的生存概率是这个总生存概率的一部分。 ## 竞争风险模型也分为单因素多终点和多因素多终点两种： # 单因素多终点： 使用CIF（见上）意为各自的关心事件累积发生函数、竞争事件累积发生函数。CIF假设事件每次发生有且仅有一种，具有期望属性,即各类别CIF之和等于复合事件CIF。 # 多因素多终点： CS :表示时刻未发生任何事件的观察个体发生第k类事件的瞬时概率强度。 SD :表示时刻未发生第k类事件的观察个体发生第k类事件的瞬时概率强度。 其中： CS解决了比例风险模型中不能同时较准确地考虑多个终点事件,适合病因学研究。 缺点: 1、要求观察量两两之间及协变量之间独立; 2、对结果的解释不是很直观。 ## 何时选择竞争风险模型： 临床生存数据如果有多个结局,当存在竞争结局时,不满足”删失独立”的假设，则不能用Cox比例风，险模型进行多因素分析,否则会出现错误的HR。竞争事件比例&gt; 10%采用传统方法可造成严重偏倚,而&lt;10%可能出现假阳性或假阴性。 单因素分析 单因素分析 不存在竞争风险 存在竞争风险 发生率 Kaplan-Meier CIF 生存曲线 Kaplan-Meier曲线 Nelson-Aalen累积风险曲线 差异性检验 Log-rank Gray’s检验 4.3.2.3.2 R 代码实现竞争风险回归 # 此案例数据是从 http://www.stat.unipg.it/luca/R/ # 单因素竞争风险： library(foreign) bmt &lt;-read.csv(‘bmtcrr.csv’) library(cmprsk) bmt$D &lt;- as.factor(bmt$D) fit1 &lt;- cuminc(ftime,Status,D) plot(fit1,xlab = ‘Month’, ylab = ‘CIF’,lwd=2,lty=1, col = c(‘red’,’blue’,’black’,’forestgreen’)) # 多因素竞争风险 -方法1:： ## 注意cov的构建应尽可能展开数据，并分类成0-1类的数字编码形式： cov &lt;- data.frame(age = bmt$Age, sex_F = ifelse(bmt$Sex==‘F’,1,0), dis_AML = ifelse(bmt$D==‘AML’,1,0), phase_cr1 = ifelse(bmt$Phase==‘CR1’,1,0), phase_cr2 = ifelse(bmt$Phase==‘CR2’,1,0), phase_cr3 = ifelse(bmt$Phase==‘CR3’,1,0), source_PB = ifelse(bmt$Source==‘PB’,1,0)) 1fit2 &lt;- crr(bmt$ftime, bmt$Status, cov , failcode=1, cencode=0) 2summary(fit2) # 多因素竞争风险 -- 方法2： # 竞争风险图表： ## 竞争分享评估； ## 参见资料： # https://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html library(cmprsk) data(Melanoma, package = &quot;MASS&quot;) ## cencode=2表示删失的数据类型： ci_fit &lt;- cuminc(Melanoma$time, Melanoma$status, cencode = 2) plot(ci_fit, xlab = &quot;Days&quot;) ggcompetingrisks(ci_fit, xlab = &quot;Days&quot;) ## 比较组间竞争差异： ci_fit &lt;- cuminc(Melanoma$time, Melanoma$status, group = Melanoma$ulcer,cencode = 2) ci_ulcer[[&quot;Tests&quot;]] ggcompetingrisks( fit = ci_ulcer, multiple_panels = FALSE, xlab = &quot;Days&quot;, ylab = &quot;Cumulative incidence of event&quot;, title = &quot;Death by ulceration&quot;, ylim = c(0, 1)) ## 绘制竞争风险图表：生存曲线（不同竞争风险因素 ）： ciplotdat1 &lt;- ci_ulcer %&gt;% # 可以识别数据组内的列表 list_modify(&quot;Tests&quot; = NULL) %&gt;% # 使用浅拷贝方法从列表直接考虑一层数据，并转置成数据框： map_df(`[`, c(&quot;time&quot;, &quot;est&quot;), .id = &quot;id&quot;) %&gt;% filter(id %in% c(&quot;0 1&quot;, &quot;1 1&quot;)) %&gt;% mutate(Ulceration = recode( id, &quot;0 1&quot; = &quot;Not ulcerated&quot;, &quot;1 1&quot; = &quot;Ulcerated&quot;) ) mel_plot &lt;- ggplot(ciplotdat1, aes(x = time, y = est, color = Ulceration)) + geom_step(lwd = 1.2) + ylim(c(0, 1)) + coord_cartesian(xlim = c(0, 5000)) + scale_x_continuous(breaks = seq(0, 5000, 1000)) + theme_classic() + theme(plot.title = element_text(size = 14), legend.title = element_blank(), legend.position = &quot;bottom&quot;) + labs(x = &quot;Days&quot;, y = &quot;Cumulative incidence&quot;, title = &quot;Melanoma death by ulceration status&quot;) + annotate(&quot;text&quot;, x = 0, y = 1, hjust = 0, label = paste0( &quot;p-value = &quot;, ifelse(ci_ulcer$Tests[1, 2] &lt; .001, &quot;&lt;.001&quot;, round(ci_ulcer$Tests[1, 2], 3)))) mel_fit &lt;- survfit( Surv(time, ifelse(status != 2, 1, 0)) ~ ulcer, data = Melanoma) num &lt;- ggsurvplot( fit = mel_fit, risk.table = TRUE, risk.table.y.text = FALSE, ylab = &quot;Days&quot;, risk.table.fontsize = 3.2, tables.theme = theme_survminer(font.main = 10), title = &quot;Test&quot;) cowplot::plot_grid( mel_plot, num$table + theme_cleantable(), nrow = 2, rel_heights = c(4, 1), align = &quot;v&quot;, axis = &quot;b&quot;) ## 竞争风险评估的协变量回归评估： # 使用crr函数：并需要将协变量指定为矩阵； # 如果感兴趣的事件结局不止一个，可以使用failcode来请求不同事件的结果； shr_fit &lt;- crr(ftime = Melanoma$time, fstatus = Melanoma$status, cov1 = Melanoma[, c(&quot;sex&quot;, &quot;age&quot;)], cencode = 2) View(Melanoma) ## 如果性别和年龄未被编码为数值变量，crr不能处理字符变量： #需要使用model.matrix()来进行编码定义： chardat &lt;- Melanoma %&gt;% mutate( sex_char = ifelse(sex == 0, &quot;Male&quot;, &quot;Female&quot;)) ## 这里的covs1对应使用如上： covs1 &lt;- model.matrix(~ sex_char + age, data = chardat)[, -1] 4.3.2.3.3 R 代码实现竞争风险回归列线图： ## 参见资料： https://cloud.tencent.com/developer/article/1666334 ## 资料来自： http://www.stat.unipg.it/luca/R/ library(foreign) bmt &lt;-read.csv(‘bmtcrr.csv’) bmt$id&lt;-1:nrow(bmt) bmt$sex &lt;- as.factor(ifelse(bmt$Sex==‘F’,1,0)) bmt$D &lt;- as.factor(ifelse(bmt$D==‘AML’,1,0)) bmt$phase_cr &lt;- as.factor(ifelse(bmt$Phase==‘Relapse’,1,0)) bmt$source = as.factor(ifelse(bmt$Source==‘PB’,1,0)) ## 注意事项： 列线图中不建议使用哑变量，应避免在列线图中使用哑变量； regplot包中的regplot()函数可以绘制更多美观的列线图。但是，它目前仅接受由coxph()，lm()和glm()函数返回的回归对象。 因此，为了绘制竞争风险模型的列线图，我们需要对原始数据集进行加权，以创建用于竞争风险模型分析的新数据集。 mstate包中crprep()函数的主要功能是创建此加权数据集，如下面的R代码所示。然后，我们可以使用coxph()函数拟合加权数据集的竞争风险模型，再将其给regplot()函数以绘制列线图。对于特定的加权原理，读者可以参考Geskus等人发表的文章。此处不再详述。 library(mstate) df.w &lt;- crprep(“ftime”, “Status”, data=bmt, trans=c(1,2), cens=0, id=“id”, keep=c(“age”,”sex”,”D”,”phase_cr”,”source”)) df.w$T&lt;- df.w$Tstop - df.w$Tstart m.crr&lt;- coxph(Surv(T,status==1)~age+sex+D+phase_cr+source, data=df.w[df.w$failcode==1,], weight=weight.cens, subset=failcode==1) summary(m.crr) library(regplot) regplot(m.crr,observation=df.w[df.w$id==31&amp;df.w$failcode==1,], failtime = c(36, 60), prfail = T, droplines=T) # 为了便于比较，可以在原始数据集bmt中进一步构建Cox回归模型，将id=31的患者的协变 # 量的值计算为相应的得分，并计算总分，分别计算id=31的患者在36个月和60个月的累积复# 发概率。计算结果分别为：0.205和0.217(图33)。 library(survival) m.cph&lt;-coxph(Surv(ftime,Status==1)~age+sex+D+phase_cr+source, data=bmt) summary(m.cph) regplot(m.cph,observation=bmt[bmt$id==31,], failtime = c(36,60), prfail = TRUE,droplines=T) 4.3.2.4 COX模型的扩展 4.3.2.4.1 使用参数分布族来拟合cox回归模型： ## 如果基线风险是参数类型，则可以在某些条件下使用参数cox回归法方法更精准的解释cox回归结果； # 如果基线风险被认为是非参数的，则将获得 Cox 比例风险模型。 # Weibull 和exponential是仅有的同时具有比例风险和加速故障时间表示的参数回归模型。 # 关于不同分布族的选择方法： # 参考:https://blog.sciencenet.cn/blog-927304-876450.html # 如果我们认为h(t)是常数c，那么存活时间就满足参数为c的指数分布。如果认为logh(t) = a + bt，那么存活时间满足Gompertz分布，如果认为logh(t) = a + log(t)。那么存活时间满足Weibull分布。 ## survival提供了survreg函数用于参数分析cox回归； # 当survreg中的method为log形式时，则表示使用AFT模型： # 注意survreg计算的p值是基于似然比(LR)，而不是 Wald 检验。 ## Weibull 分布： survreg(Surv(futime, fustat) ~ ecog.ps + rx, ovarian, dist=&#39;weibull&#39;, scale=1) ## exponential指数分布： survreg(Surv(futime, fustat) ~ ecog.ps + rx, ovarian, dist=&quot;exponential&quot;) ## Gompertz 分布： # 这里是参数化的cox回归类型，需要注意AFT中的Gompertz还需要后向参数： library(eha) fit &lt;- phreg(Surv(enter + 40, exit + 40, event) ~ I(birthdate - 1810) + strata(ses), data = mort, dist = &quot;gompertz&quot;) plot(fit, fn = &quot;haz&quot;, col = c(&quot;red&quot;, &quot;blue&quot;), main = &quot;Male mortality by social class&quot;, ylab = &quot;Gompertz hazard&quot;, xlab = &quot;Age&quot;) 4.3.2.4.2 协变量分层cox回归： 如果某个分类性质的混杂因素不满足风险比例假定，可以考虑分层分析。分层分析一般用于混杂因素比较少的情况，且进行分层的变量须是分类变量（分层实际上就是按分层变量的各个水平分为不同的层），每一层内仍然要求保持比例风险假定。 也就是要求在进行分层分析时，允许基线风险率在分层因素的各个层上完全不同，但其他因素的相对危险度RR在各个时点及层内保持不变。由于无法估计基线风险函数，进行分层的变量对生存结局的影响强度就无法分析，也就无法获得分层因素的统计学检验，因此分层因素只适合用于控制混杂。 直接将混杂因素进行多因素回归分析也可以控制混杂，不同的是这种在Cox回归模型中，要求基线风险率在混在因素不同的水平间是一样的，其他因素的相对危险度RR在混杂因素的各个水平中保持一致： 在某些情况下，我们在数据中存在固有的异质性（例如，多中心试验），或者我们有一个不满足比例风险假设的分类变量，我们可以使用分层。为了拟合分层 Cox 模型，我们在使用strata()函数的模型公式中包含分层因子。 ## 两种形式： ## 形式（1）：直接生成每个风险模型具有不同的基线风险sex; fit &lt;- coxph(Surv(years, status2) ~ age + drug + log(serBilir) + strata(sex), data = pbc2.id) ## 形式（2）： 如果希望获得分层和其他协变量之间的交互： fit_int &lt;- coxph(Surv(years, status2) ~ age * strata(sex) + drug + log(serBilir), data = pbc2.id) ## 可视化： # 需要使用expand.grid生成分组后的扩展数据集： ND &lt;- with(pbc2.id, expand.grid( age = 60, drug = factor(&quot;placebo&quot;, levels = levels(drug)), serBilir = 15, sex = factor(&quot;male&quot;, levels = levels(sex)))) sfit &lt;- survfit(fit_int, newdata = ND) plot(sfit, col = c(&quot;black&quot;, &quot;red&quot;)) legend(&quot;topright&quot;, levels(pbc2.id$sex), lty = 1, lwd = 2, col = c(&quot;black&quot;, &quot;red&quot;), bty = &quot;n&quot;) 4.3.2.4.3 集群cox回归 当分层cox回归的每个中心条件下都有相对充足的人群数据时，使用分层cox回归是合适的。但是当分层后人群数不够，则需要处理集群数据来合理外推： ## 第一种方法：跨集群合并： # 使用cluster()方法： 例如，在多中心试验中，我们想要估计跨中心的合并治疗效果。为了考虑 Cox 模型中的聚类并获得合并效应，我们使用分组折刀方差估计调整结果估计的标准误差。这是我们使用函数时自动实现的cluster(). marginal_model &lt;- coxph(Surv(time, status) ~ age + cluster(inst), data = lung) # summary(marginal_model) 在p值检验中的z-score似然估计中的会输出一个robust结果错误矫正以及返回的p值； ## 第二种方法：观测集群数据的异质性： 在这种方法中，我们假设集群内的所有成员共享一个未观察到的变量。由于这个变量的共享产生了相关性。 # 使用frailty() frailty_model &lt;- coxph(Surv(time, status) ~ age + frailty(inst), data = lung) summary(frailty_model) 4.3.2.4.4 加速失效时间模型（AFT） image-20220905143823100.png 4.3.2.4.4.1 2.4.4.1 模型假设检验 # 在加速失效时间回归模型中，评估协变量对生存时间对数的影响。在这种情况下获得的模型包括广义 gamma、Log-logistic、Log-normal、威布尔和指数。 # 使用替代的时间加速模型的原因： 比例风险模型不需要考虑生存时间的特定概率分布；因此，它是分析生存数据最有用的模型。但是模型的效率严重依赖于比例风险假设，因此，Cox 模型通常被称为比例风险模型。在比例风险模型不可接受的情况下，从 Cox 模型得出的估计值将导致模型拟合不当和推论不正确 ( 16 – 22 )。在这种情况下，加速故障时间模型尤其重要。这些模型——由于具有生存时间的参数分布——使统计推断更加准确，并导致模型的正确拟合 4.3.2.4.4.2 2.4.4.2 模型拟合 ## 方法来自survivel：最常用为对数正态和逻辑正态： # 也支持gamma、指数和威布尔分布； fit_lnorm &lt;- survreg(Surv(years, status2) ~ drug + sex + age, data = pbc2.id,dist = &quot;lognormal&quot;) fit_llogis &lt;- survreg(Surv(years, status2) ~ drug + sex + age, data = pbc2.id,dist = &quot;loglogistic&quot;) summary(fit_llogis) ## 使用方法来自：eha包： library(eha) fit.gaft &lt;- aftreg(Surv(enter - 60, exit - 60, event) ~ sex + region, id = id, param = &quot;lifeExp&quot;, dist = &quot;gompertz&quot;, data = oldmort) summary(fit.gaft) 4.3.2.4.4.3 2.4.4.3 交互项检查效果图 ## 参见： https://www.drizopoulos.com/courses/emc/survival%20analysis%20in%20r%20companion 4.3.2.4.4.4 2.5.4 复杂与简单模型的交互检验 fit_alt &lt;- survreg(Surv(years, status2) ~ (drug + sex) * (age + I(age^2)), data = pbc2.id) fit_null &lt;- survreg(Surv(years, status2) ~ drug + sex + age, data = pbc2.id) anova(fit_null, fit_alt) 4.3.2.4.4.5 2.4.4.5 残差分析-模型拟合效果检验 fit_weib &lt;- survreg(Surv(Time, death) ~ drug * gender, data = aids.id) fitted_values &lt;- fit_weib$linear.predictors resids &lt;- (log(fit_weib$y[, 1]) - fitted_values) / fit_weib$scale resKM &lt;- survfit(Surv(resids, death) ~ 1, data = aids.id) plot(resKM, mark.time = FALSE, xlab = &quot;AFT Residuals&quot;, ylab = &quot;Survival Probability&quot;) xx &lt;- seq(min(resids), max(resids), length.out = 35) yy &lt;- exp(- exp(xx)) lines(xx, yy, col = &quot;red&quot;, lwd = 2) legend(&quot;bottomleft&quot;, c(&quot;KM estimate&quot;, &quot;95% CI KM estimate&quot;, &quot;Survival function of Extreme Value distribution&quot;), lty = c(1,2,1), col = c(1,1,2), bty = &quot;n&quot;) 4.3.2.4.5 finalfit批运行cox回归 4.3.2.4.5.1 2.4.5.1 COX回归 # install.packages(&quot;finalfit&quot;) library(finalfit) library(survival) melanoma &lt;- boot::melanoma names(melanoma) ## 构造可读参数模型： dependent_os &lt;- &quot;Surv(time, status)&quot; explanatory &lt;- c(&quot;age&quot;, &quot;sex&quot;, &quot;thickness&quot;, &quot;ulcer&quot;) ## 单次运行模型： melanoma %&gt;% ## 输出每个参数变量的单变量HR和多变量HR; finalfit(dependent_os, explanatory) ## 标签调整： melanoma %&gt;% finalfit(dependent_os, explanatory, add_dependent_label = FALSE) %&gt;% rename(&quot;Overall survival&quot; = label) %&gt;% rename(&quot; &quot; = levels) %&gt;% rename(&quot; &quot; = all) ## 指定变量(基于条件筛选后，直接输出指定变量的多变量HR和p值)： explanatory_multi &lt;- c(&quot;age&quot;, &quot;thickness&quot;, &quot;ulcer&quot;) melanoma %&gt;% finalfit(dependent_os, explanatory, ## explanatory_multi指定保留的变量（为变量筛选后） explanatory_multi, keep_models = TRUE) ## 变量的PH检验： explanatory &lt;- c(&quot;age&quot;, &quot;sex&quot;, &quot;thickness&quot;, &quot;ulcer&quot;, &quot;year&quot;) melanoma %&gt;% coxphmulti(dependent_os, explanatory) %&gt;% cox.zph() %&gt;% ## 内部函数进行全局参数赋值： {zph_result &lt;&lt;- .} %&gt;% ## 输出cox.zph()函数运行结果： plot(var=2) ## 分层cox回归模型实现： explanatory &lt;- c(&quot;age&quot;, &quot;sex&quot;, &quot;ulcer&quot;, &quot;thickness&quot;, &quot;strata(year)&quot;) melanoma %&gt;% finalfit(dependent_os, explanatory) ## 使用cluster和frailty聚类分析： # Simulate random hospital identifier melanoma &lt;- melanoma %&gt;% mutate(hospital_id = c(rep(1:10, 20), rep(11, 5))) # Cluster model explanatory &lt;- c(&quot;age&quot;, &quot;sex&quot;, &quot;thickness&quot;, &quot;ulcer&quot;, &quot;cluster(hospital_id)&quot;) melanoma %&gt;% finalfit(dependent_os, explanatory) # Frailty model explanatory &lt;- c(&quot;age&quot;, &quot;sex&quot;, &quot;thickness&quot;, &quot;ulcer&quot;, &quot;frailty(hospital_id)&quot;) melanoma %&gt;% finalfit(dependent_os, explanatory) ## 风险比例森林图： melanoma %&gt;% hr_plot(dependent_os, explanatory) ## 补充： explanatory = c(&quot;age.factor&quot;, &quot;sex.factor&quot;, &quot;obstruct.factor&quot;, &quot;perfor.factor&quot;) dependent = &quot;Surv(time, status)&quot; colon_s %&gt;% coxphmulti(dependent, explanatory) %&gt;% ## 提取整洁的coxphmulti结果： fit2df() 4.3.2.4.5.2 2.4.5.2 竞争风险模型： explanatory &lt;- c(&quot;age&quot;, &quot;sex&quot;, &quot;thickness&quot;, &quot;ulcer&quot;) dependent_dss &lt;- &quot;Surv(time, status_dss)&quot; dependent_crr &lt;- &quot;Surv(time, status_crr)&quot; melanoma %&gt;% # Summary table # 这里是将分类变量进行参数化分解，保证每个列名下的变量都有独特分类用于计算 # HR和P值； summary_factorlist(dependent_dss, explanatory, column = TRUE, fit_id = TRUE) %&gt;% # CPH univariable ff_merge( melanoma %&gt;% # 计算 每个变量的HR; coxphmulti(dependent_dss, explanatory) %&gt;% fit2df(estimate_suffix = &quot; (DSS CPH univariable)&quot;) ) %&gt;% # CPH multivariable ff_merge( melanoma %&gt;% coxphmulti(dependent_dss, explanatory) %&gt;% fit2df(estimate_suffix = &quot; (DSS CPH multivariable)&quot;) ) %&gt;% # Fine and Gray competing risks regression ff_merge( melanoma %&gt;% ## 注意这种写法：提取的是竞争风险模型的参数： crrmulti(dependent_crr, explanatory) %&gt;% fit2df(estimate_suffix = &quot; (competing risks multivariable)&quot;) ) %&gt;% select(-fit_id, -index) %&gt;% dependent_label(melanoma, &quot;Survival&quot;) 4.3.2.4.6 pec包提高cox回归模型的交叉时间点验证 install.packages(&quot;pec&quot;) library(pec) ##验证模型 library(rms) ##拟合生存分析模型 library(survival) ##生存分析包 library(glmnet) ##Lasso回归包 set.seed(1234) x &lt;- nrow(dt) %&gt;% runif() ## 随机生成449个生成从0到1区间范围内的服从正态分布的随机数 dt &lt;- transform(dt,sample= order(x))%&gt;% arrange(sample) ###随机排列数据集样本 ###拆分数据集 train &lt;- dt[1:((nrow(dt)-1)/2),-45] test &lt;- dt[((nrow(dt)+1)/2):nrow(dt),-45] ## 全模型： ## 注意cph来自rms包，并对原有的coxph回归做进一步拓展： cox1 &lt;- cph(Surv(live.time,outcome==1)~.,x=T,y=T,data=train, surv=TRUE) ## cox2 筛选变量搭建的模型 cox2 &lt;- cph(Surv(live.time,outcome==1)~relapse+BCLC+tumor.single.double+AFP,x=T,y=T,data=train,surv=TRUE) ## 模型区分度对比和验证 ## eval.times 输入评价模型区分能力的时间点向量，缺少的话系统认为是最大生存时间 ## 此时相当于单个时间点评估，同之前文章中求的模型concordance ## 这个就是优于rms的一个点，可以对每个时间点比较。 ## cindex来自pec包，专门用于时序回归模型之间的比较检验： ## 官网定义： # 该函数提供了审查一致性概率的加权估计的概率的倒数，以调整正确的审查。 # 基于自举重采样或自举二次抽样的交叉验证可用于评估和 # 比较各种回归建模策略对同一组数据的判别能力。 c_index &lt;- cindex(list(&quot;Cox(43 variables)&quot;=cox1, &quot;Cox(4 variables)&quot;=cox2), formula=Surv(live.time,outcome==1)~., data=test, eval.times=seq(365,5*365,36.5)) ## 设置画图参数 ##mar 以数值向量表示的边界大小，顺序为“下、左、上、右”，单位为英分*。 ##默认值为c(5, 4, 4, 2) + 0.1 ##mgp 设定标题、坐标轴名称、坐标轴距图形边框的距离。默认值为c(3,1,0)， ##其中第一个值影响的是标题 ## cex.axis 坐标轴刻度放大倍数 ## cex.main 标题的放大倍数 ## legend.x，legend.y 图例位置的横坐标和纵坐标 ## legend.cex 图例文字大小 par(mgp=c(3.1,0.8,0),mar=c(5,5,3,1),cex.axis=0.8,cex.main=0.8) plot(c_index,xlim = c(0,2000),legend.x=1,legend.y=1,legend.cex=0.8) ## 采用bootstrap重抽样法进行交叉验证，提高结果稳定性。 ## calPlot也来自pec包： calPolt2 &lt;- calPlot(list(&quot;Cox(43 variables)&quot;=cox1, &quot;Cox(4 variables)&quot;=cox2), time=3*365,#设置想要观察的时间点 data=test,legend.x=0.5, legend.y=0.3,legend.cex=0.8, splitMethod = &quot;BootCv&quot;, B=1000） 4.3.3 补充R包 ## survMS R 包：生存模型模拟 ## 参见： https://github.com/mathildesautreuil/survMS 生存管理系统是一个 R 包，它从具有不同复杂程度的不同生存模型生成生存数据。包中实现了三个生存模型来模拟生存数据：Cox 模型、加速故障时间 (AFT) 模型和加速危险 (AH) 模型。这些模型有其特殊性。事实上，Cox 模型是比例风险模型，而不是 AFT 和 AH 模型。比例风险模型意味着具有不同解释变量的两个人之间的风险比不依赖于时间。在 AFT 模型中，解释变量对个体的生存有加速或减速的影响。在 Cox 模型和 AFT 模型这两个模型中，生存函数的曲线从不相交。例如，当人们对具有相交生存曲线的更复杂的数据感兴趣时，为了使方法更难预测生存，包中实现了两种方法。第一种方法包括修改 AFT 模型以具有相交的生存曲线。第二种方法涉及使用 AH 模型（对于Accelerated Hazards ) 以生成生存数据。AH模型比上面提到的两种模型更灵活。在 AH 模型中，变量加速或减速危险风险。因此，AH 模型的生存曲线可以相互交叉。从上面提到的不同模型中产生的生存时间。假设模型的基线风险函数是已知的并且遵循特定的概率分布。对于这些不同的模型，我们使用基线风险的参数分布来生成生存时间。 请注意，我们停留在没有交互的线性依赖框架中。我们将在第二步中将包调整为非线性框架以模拟交互。通过用解释变量的非线性函数替换解释变量的线性部分，该包的实现很容易适应具有交互作用的非线性框架。包中的所有函数都经过编码，以便轻松引入解释变量的这种非线性函数。 4.4 构造模型的辅助函数或参数 4.4.1 常用参数 ## 正则运算符： &amp;&amp; 和 || 是短路运算，即遇到TRUE （FALSE）则返回TRUE（false）而不继续往下计算； 而&amp; 和 | 是向量运算符，对向量中所有元素分别进行运算； ## 数据类型知识： R中使用NA表示缺失值，null表示空值，NaN表示非数，Inf表示无穷大； 4.4.2 查询线性模型结果 ## 函数 summary() 展示拟合模型的详细结果 # coefficients 列出拟合模型的模型参数(截距项和斜率) # confine(): 提供模型参数的置信区间(默认95%) # fitted() 列出拟合模型的预测值 # residuals ( 列出拟合模型的残差值 # anova() 生成一个拟合模型的方差分析表,或者比较两个或更多拟合模型的方差分析表 此外，anova还可以将模型的结果求出，解出每个协变量的截距值； # vcov() 列出模型参数的协方差矩阵 # AIC() 输出赤池信息统计量 ## 另外一种方法是使用questionr包中的odds.ratio函数。 ## 计算模型回归后拟合的p、OR和95%CI值等； round(odds.ratio(fit),2) ## 提取模型输出结果： summary(step.model) ## 结合逐步回归给出的变量参考建议： drop1(step.model) ## 可视化交互效应模型变量间的强弱关系： library(effects) plot(effect(&#39;prey_num*temperature&#39;, fit2), multiline=TRUE) ## 整理检验结果： #view the model output summary(model) ## 将统计结果整理成表格形式： library(broom) tidy(model) 4.4.3 R语言笔记.formula 4.4.3.0.1 常用公式构建参数： image-20220902144627876 image-20210708152116483 ## 补充： 1、I()：使用I()包含二次项和交互项； model.frame( y ~ x + x^2, data = data.frame(x = rnorm(5), y = rnorm(5))) y~x+x^2 首先先在外面算好x^2的值,然后再放进formula 用I(): `y~x + I(x^2) 2、*：在公式中*表示交互式，同时包含参数间的交互作用和不同参数各自的作用； 而：表示仅提取交互项的作用； 3、y ~ .：其中&quot;.&quot;表示将所有的环境变量全部纳入到其中； 4、使用poly()函数来直接简写交互项： ## 直接写公式： m1 &lt;- lm(y~x+I(x^2),data = swiss) ## 使用poly 函数 m2 &lt;- lm(y ~ poly(x, 2, raw=TRUE),data = swiss) # raw 参数表示使用正交多项式 5、lm()函数释义： 此时再看lm(y~x)、lm(y~x+1)、lm(y~-1)三者的区别便可发现： +1表示有截距项与-1相对应， -1指没有截距项， 而x表示默认有截距项。 image-20210708152742317 ## 补充： ## 模型中的“1”代表什么？ #将个体当做随机因素，这样由于个体有m个水平(m个个体),因此会发现会产生m个截距值。不仅仅是这样，还是作为做随机效应 #这里的1代表随机截距， | 后面表示分组变量； model=lmer(pitch~sex+place+(1|subject)，data=data.csv) ## # 复杂的例子，这里的place表示place中使用随机斜率； model=lmer(pitch~sex+place+(place|subject)+(1|subject) ## 表示在给定随机斜率的情况下，固定截距；加上参数化的交互作用； lmer(Y ~ 1 + X1*W + X2 + (1 + X1 | group), ...) 4.4.3.0.2 手动构造交互变量： 在R语言中，两个变量相乘可以用“*”表示，若任一个变量为分类变量，R语言会警告“‘*’ not meaningful for factors”。回归模型中如果需要添加交互项，可以直接在formula中通过“*”或“:”实现（连续变量和分类变量均适用）。但在某些情况下，可能需要拆解分类变量的交互项而后使用，那么在R语言中如何实现手动构建交互项呢？ ## “连续变量*连续变量” dat &lt;- within(dat, { gender &lt;- factor(gender, labels = c(&quot;male&quot;, &quot;female&quot;)) prog &lt;- factor(prog, labels = c(&quot;jog&quot;, &quot;swim&quot;, &quot;read&quot;))}) fit1 &lt;- lm(loss ~ hours * effort, data = dat) summary(fit1) 等价于： fit1 &lt;- lm(loss ~ hours + effort + hours : effort, data = dat) summary(fit1) ## “连续变量*分类变量”、 ## 方法1: dat$gender &lt;- relevel(dat$gender, ref = &quot;female&quot;) fit2 &lt;- lm(loss ~ hours * gender, data = dat) summary(fit2) 等价于： dat$hours_gender_male &lt;- dat$hours * (dat$gender == &quot;male&quot;) fit2 &lt;- lm(loss ~ hours + gender + hours_gender_male, data = dat) summary(fit2) ## 方法2： dat$prog &lt;- relevel(dat$prog, ref = &quot;read&quot;) fit3 &lt;- lm(loss ~ hours * prog, data = dat) summary(fit3) 等价于： dat$hours_prog_jog &lt;- dat$hours * (dat$prog == &quot;jog&quot;) dat$hours_prog_swim &lt;- dat$hours * (dat$prog == &quot;swim&quot;) fit3 &lt;- lm(loss ~ hours + prog + hours_prog_jog + hours_prog_swim, data = dat) summary(fit3) ## “分类变量*分类变量” ## 方法1： dat$gender &lt;- relevel(dat$gender, ref = &quot;female&quot;) dat$prog &lt;- relevel(dat$prog, ref = &quot;read&quot;) fit4 &lt;- lm(loss ~ gender * prog, data = dat) summary(fit4) 等价于： dat$gender_male_prog_jog &lt;- (dat$gender == &quot;male&quot;) * (dat$prog == &quot;jog&quot;) dat$gender_male_prog_swim &lt;- (dat$gender == &quot;male&quot;) * (dat$prog == &quot;swim&quot;) fit4 &lt;- lm(loss ~ gender + prog + gender_male_prog_jog + gender_male_prog_swim, data = dat) summary(fit4) ## 方法2： dat$effort_cat &lt;- cut(dat$effort, breaks = c(0, 25, 35, Inf), labels = c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;)) dat$effort_cat &lt;- relevel(dat$effort_cat, ref = &quot;low&quot;) dat$prog &lt;- relevel(dat$prog, ref = &quot;read&quot;) fit5 &lt;- lm(loss ~ effort_cat * prog, data = dat) summary(fit5) 等价于： dat$effort_cat_medium_prog_jog &lt;- (dat$effort_cat == &quot;medium&quot;) * (dat$prog == &quot;jog&quot;) dat$effort_cat_high_prog_jog &lt;- (dat$effort_cat == &quot;high&quot;) * (dat$prog == &quot;jog&quot;) dat$effort_cat_medium_prog_swim &lt;- (dat$effort_cat == &quot;medium&quot;) * (dat$prog == &quot;swim&quot;) dat$effort_cat_high_prog_swim &lt;- (dat$effort_cat == &quot;high&quot;) * (dat$prog == &quot;swim&quot;) fit5 &lt;- lm(loss ~ effort_cat + prog + effort_cat_medium_prog_jog + effort_cat_high_prog_jog + effort_cat_medium_prog_swim + effort_cat_high_prog_swim, data = dat) summary(fit5) 4.4.3.0.3 分组回归和交互项回归的区别 # 参见：https://zhuanlan.zhihu.com/p/429103359 例如以mtcar中的数据为例： 研究wt对mpg的影响是否因为am而异？ ## 经比较发现： # 当添加额外协变量时，分组回归和交互式回归的结果不同： 举例俩说： fit1_1 &lt;- lm(mpg ~ wt + hp, mtcars, subset = (am == 0)) summary(fit1_1) fit1_2 &lt;- lm(mpg ~ wt + hp, mtcars, subset = (am == 1)) summary(fit1_2) 不等价于： fit1 &lt;- lm(mpg ~ wt * factor(am) + hp, mtcars) summary(fit1) # 当没有添加额外协变量时，分组回归和交互式回归的结果相同： fit2_1 &lt;- lm(mpg ~ wt, mtcars, subset = (am == 0)) summary(fit2_1) fit2_2 &lt;- lm(mpg ~ wt, mtcars, subset = (am == 1)) summary(fit2_2) 等价于： fit1 &lt;- lm(mpg ~ wt * factor(am) , mtcars) summary(fit1) ## 总结： 1. 亚组分析主要关注不同亚组的疗效是否有统计学差异，而不是亚组的疗效是否有统计学意义。 2. 亚组分析需注意：（1）分亚组后样本量减少，检验效能降低（Ⅱ类错误）；（2）对同一效应进行多次检验增加假阳性率（Ⅰ类错误）；（3）在随机分组试验中，亚组分析会破坏随机化的作用，结果解释需谨慎。 3. 预先定义的亚组分析和事后亚组分析▪ 预先定义的亚组分析（prespecified subgroup analysis），一般作为验证性分析，需要在临床试验方案中提前声明，并在计算样本量和随机化分组时考虑亚组变量的影响，确定研究的假设检验和统计方法。▪ 预事后亚组分析（post hoc subgroup analyses），一般作为探索性分析。 4.4.3.0.4 构建GLM分布族和链接函数 逻辑回归、一般线性回归等都是广义线性回归的一种分布族下的子集函数；广义线性模型通过拟合响应变量的条件均值的一个函数（不是响应变量的条件均值），并假设响应变量服从指数分布族中的某个分布（不限于正态分布），从而极大地扩展了标准线性模型。模型参数估计的推导依据是极大似然估计，而非最小二乘法。 此外，在glm模型中可以选择使用链接函数log或者identity，其中log函数会对因变量做log转换处理，而identity对原始因变量不做修正。 ## 举例如下： # 二项式分布： glm(Y ~ X1 + X2 + X3, family=binomial(link=&quot;logit&quot;), data=data) # 正态：n趋于无穷大则为高斯分布 glm(y~x,family=gaussian(link=&quot;identity&quot;),data=base) glm(y~x,family=gaussian(link=&quot;log&quot;),data=base) # 反正态 glm(y~x,family=inverse.gaussian(link=&quot;identity&quot;),data=base) # 泊松回归适用于在给定时间内响应变量为事件发生数目的情形，其假设Y服从泊松分布，连接函数为log(λ)，概率分布为泊松分布： glm(y~x,family=poisson(link=&quot;identity&quot;),data=base) # 连续概率变量函数， glm(y~x,family=Gamma(link=&quot;identity&quot;),data=base) 4.4.4 特征工程 1、避免无意义的信息； ## 这里所指 的无意义信息，包含两种，第一种是指因果联系十分薄弱的非相关信息； 第二种是无法区分数据类型的数据； 2、避免重复性新信息， ## 简言之，就是在处理去除掉存在强共线性的信息； 3、避免复杂的信息： ## 简言之，去除那些多阶信息，保留尽可能的单阶信息。但可能部分情况下，这个问题 并不 绝对。 preview 4.4.4.0.1 构造哑变量 library(caret) library(ISLR) dummies &lt;- dummyVars(~League+Division+NewLeague, data = Hitters) dummies &lt;- predict(dummies, newdata = Hitters) head(dummies) 4.4.4.0.2 构造训练集和测试集 set.seed(1) index &lt;- sample(1:nrow(Hitters_dummy), size = 0.8*nrow(Hitters_dummy)) train &lt;- Hitters_dummy[index,] test &lt;- Hitters_dummy[-index,] 4.5 深度学习 4.5.1 R 中执行keras library(tensorflow) library(keras) ## 第五步：在R中学习使用keras软件分析：#### ######### 5.1数据可视化： iris_tar &lt;- iris[,5] plot(iris$Petal.Length, iris$Petal.Width, pch=21, bg=c(&quot;red&quot;,&quot;green3&quot;,&quot;blue&quot;)[unclass(iris$Species)], xlab=&quot;Petal Length&quot;, ylab=&quot;Petal Width&quot;) ######## 5.2 数据相关性分析： # Store the overall correlation in `M` M &lt;- cor(iris[,1:4]) # Plot the correlation plot with `M` library(corrplot) corrplot(M, method=&quot;circle&quot;) ######## 5.3 数据处理及建模数据准备： ## 将类型数据转为向量分类型数据： iris[,5] &lt;- as.numeric(iris[,5]) -1 iris &lt;- as.matrix(iris) # Set iris `dimnames` to `NULL` dimnames(iris) &lt;- NULL # 转为矩阵： iris &lt;- as.matrix(iris ) # 第三步：构建训练集和测试集： ind &lt;- sample(2, nrow(iris), replace=TRUE, prob=c(0.67, 0.33)) # Split the `iris` data iris.training &lt;- iris[ind==1, 1:4] iris.test &lt;- iris[ind==2, 1:4] # Split the class attribute iris.trainingtarget &lt;- iris[ind==1, 5] iris.testtarget &lt;- iris[ind==2, 5] # One hot encode training target values iris.trainLabels &lt;- to_categorical(iris.trainingtarget) # One hot encode test target values iris.testLabels &lt;- to_categorical(iris.testtarget) ######## 5.4 构建模型： #### 5.4.1 构建初级模型： # Initialize a sequential model model &lt;- keras_model_sequential() # Add layers to the model model %&gt;% ## 参数意义：&#39;relu&#39;用于激活隐藏层；&#39;softmax&#39;表示输出层； ## 输出值为为3个分类单元； ## 假设输入的隐藏层为8个； ## 这里的input_shape为4是因为仅有4列参与建模； layer_dense(units = 8, activation = &#39;relu&#39;, input_shape = c(4)) %&gt;% layer_dense(units = 3, activation = &#39;softmax&#39;) #### 5.4.2 模型评价： # 模型的评价： summary(model) # 获取模型参数； get_config(model) # 获取模型的名称： get_layer(model, index = 1) #### 5.4.3 编译和拟合模型： # Compile the model # 该过程需要两个函数;优化器和损失函数； # 如果是二元分类的问题应该使用binary_crossentropy model %&gt;% compile( loss = &#39;categorical_crossentropy&#39;, optimizer = &#39;adam&#39;, metrics = &#39;accuracy&#39; ) #### 5.4.4 可视化模型拟合过程： # Store the fitting history in `history` history &lt;- model %&gt;% fit( iris.training, iris.trainLabels, epochs = 200, ## 200次迭代； batch_size = 5, ## 每次5个样本；通过这样做，您可以优化效率， ## 因为您可以确保不会同时将太多输入模式加载到内存中。 validation_split = 0.2 ) # 可视化在训练过程中准确性和损失函数的变化过程： plot(history) # 5.4.4.1 可视化损失函数的变化过程： # Plot the model loss of the training data plot(history$metrics$loss, main=&quot;Model Loss&quot;, xlab = &quot;epoch&quot;, ylab=&quot;loss&quot;, col=&quot;blue&quot;, type=&quot;l&quot;) # Plot the model loss of the test data lines(history$metrics$val_loss, col=&quot;green&quot;) # Add legend legend(&quot;topright&quot;, c(&quot;train&quot;,&quot;test&quot;), col=c(&quot;blue&quot;, &quot;green&quot;), lty=c(1,1)) # 5.4.4.3 可视化准确性函数的变化过程： # Plot the accuracy of the training data plot(history$metrics$acc, main=&quot;Model Accuracy&quot;, xlab = &quot;epoch&quot;, ylab=&quot;accuracy&quot;, col=&quot;blue&quot;, type=&quot;l&quot;) # Plot the accuracy of the validation data lines(history$metrics$val_acc, col=&quot;green&quot;) # Add Legend legend(&quot;bottomright&quot;, c(&quot;train&quot;,&quot;test&quot;), col=c(&quot;blue&quot;, &quot;green&quot;), lty=c(1,1)) ##### 5.4.5 基于测试数据进行数据预测并构建混淆矩阵： # Predict the classes for the test data classes &lt;- model %&gt;% predict_classes(iris.test, batch_size = 128) # Confusion matrix table(iris.testtarget, classes) # Evaluate on test data and labels score &lt;- model %&gt;% evaluate(iris.test, iris.testLabels, batch_size = 128) # Print the score print(score) ##### 5.4.6 模型调参： # 通过添加层、增加隐藏单元的数量以及通过您自己的优化compile()函数的参数。 # Initialize the sequential model model2 &lt;- keras_model_sequential() # Add layers to model model2 %&gt;% ## 通过增加二级隐藏层的数量来增加拟合精度： layer_dense(units = 28, activation = &#39;relu&#39;, input_shape = c(4)) %&gt;% layer_dense(units = 5, activation = &#39;relu&#39;) %&gt;% layer_dense(units = 3, activation = &#39;softmax&#39;) # Compile the model model2 %&gt;% compile( loss = &#39;categorical_crossentropy&#39;, optimizer = &#39;adam&#39;, ## 还可以使用sgd； metrics = &#39;accuracy&#39; ) # Fit the model to the data model2 %&gt;% fit( iris.training, iris.trainLabels, epochs = 200, batch_size = 5, validation_split = 0.2 ) # Evaluate the model score2 &lt;- model2 %&gt;% evaluate(iris.test, iris.testLabels, batch_size = 128) # Print the score print(score2) ##### 5.4.7 模型保存和读取： save_model_hdf5(model, &quot;my_model.h5&quot;) model &lt;- load_model_hdf5(&quot;my_model.h5&quot;) ## 第六步：在R的pyhton中使用keras软件分析：#### ~~~ 4.6 机器学习 4.6.1 机器学习 -mlr3包 https://zhuanlan.zhihu.com/p/112845336 过去：整合机器学习算法的包： - mlr 包 - caret 包 现在：新一代整合机器学习算法的包，也是上面两个的进化版： - mlr3verse 包（首推） - tidymodels 包 ## 机器学习书籍在线： https://mlr3book.mlr-org.com/ ## Tidy Modeling with R https://www.tmwr.org/index.html ## Hands-On Machine Learning with R Bradley Boehmke &amp; Brandon Greenwell https://bradleyboehmke.github.io/HOML/ ## 构造R包： R Packages (2e) https://r-pkgs.org/ ## 高效R编程： Efficient R programming https://csgillespie.github.io/efficientR/ ## bookdown with R： bookdown: Authoring Books and Technical Documents with R Markdown https://bookdown.org/yihui/bookdown/ ## Engineering Production-Grade Shiny Apps https://engineering-shiny.org/ 4.6.2 常规机器学习 4.6.2.1 支持向量机（SVM） setwd(&#39;/Users/jerseyshen/Documents/JianShu_Project/20191230&#39;) ## 加载数据： data = read.csv(&quot;data.csv&quot;, header = T) data = data[,-1] date = seq(as.Date(&#39;2002-01-01&#39;), as.Date(&#39;2015-12-01&#39;), &#39;1 month&#39;)[1:165] ## 分割数据： train = data[1:115,] test = data[116:165,] date_train = date[1:115] date_test = date[116:165] ## 构建模型： model_init = svm(shortwave ~ temper+evap, data = train, kernel = &#39;radial&#39;) ## 5. 模型训练结果与实测数据相关性及RMSE分析（图1） train_simu = as.numeric(model_init$fitted) pl_point_df = data.frame(SIMU = as.numeric(train_simu), TRAIN = train$shortwave) label_df = data.frame( x = c(-0.5,-0.5,-0.5), y = c(0.3,0.25,0.2), label = c(paste0(&quot;COR = &quot;, round(cor(train_simu,train$shortwave),2)), paste0(&quot;p = &quot;, formatC(cor.test(train_simu,train$shortwave)$p.value)), paste0(&quot;RMSE = &quot;, round(rmse(train_simu,train$shortwave),3))) ) p1_cor = ggplot()+ geom_point(data = pl_point_df, aes(x = SIMU, y= TRAIN), size = 5, color = &#39;blue&#39;, shape = 1)+ geom_abline(intercept = 0,slope = 1,size = 1)+ geom_text(data = label_df,aes(x = x,y = y,label = label),size = 6,color = &#39;black&#39;, hjust = 0)+ theme_bw()+ theme( axis.text = element_text(face = &#39;bold&#39;,color = &#39;black&#39;,size = 12), axis.title = element_text(face = &#39;bold&#39;,color = &#39;black&#39;,size = 14, hjust = .5) )+ xlab(&#39;SIMULATION RESULT&#39;)+ ylab(&#39;REAL SHORTWAVE&#39;) png(&#39;plot1.png&#39;, height = 20, width = 20, units = &#39;cm&#39;, res = 800) print(p1_cor) dev.off() ## 6、剩下还包含两步，第一是与时间序列分析一致； ## 7、还需要对测试集进行相同的分析操作，观察数据建模的一致性关系； ## 8、模型模拟残差分析(图5) TRAIN_RES = model_init$fitted - train$shortwave TEST_RES = test_simu - test$shortwave df_res1 = data.frame(RES = TRAIN_RES, Type = &quot;TRAIN_RES&quot;) df_res2 = data.frame(RES = TEST_RES, Type = &quot;TEST_RES&quot;) df_res = rbind(df_res1,df_res2) label_df_res = data.frame( x = c(0.3,0.3,0.3,0.3), y = c(500,450,400,350), labels = c( paste0(&#39;TRAIN_MEAN = &#39;,round(mean(TRAIN_RES),2)), paste0(&#39;TRAIN_SD = &#39;,round(sd(TRAIN_RES),2)), paste0(&#39;TEST_MEAN = &#39;,round(mean(TEST_RES),2)), paste0(&#39;TEST_SD = &#39;,round(sd(TEST_RES),2)) ) ) p5 = ggplot()+ geom_density(data = df_res,aes(x = RES, y = ..count..,fill = Type))+ geom_text(data = label_df_res,aes(x = x, y = y, label = labels), color = &#39;black&#39;,fontface= &#39;bold&#39;,size = 5,hjust= 1)+ theme_bw()+ theme( axis.text = element_text(face = &#39;bold&#39;,color = &#39;black&#39;,size = 12), axis.title = element_text(face = &#39;bold&#39;,color = &#39;black&#39;,size = 14, hjust = .5), legend.text = element_text(face = &#39;bold&#39;,color = &#39;black&#39;,size = 12), legend.title = element_blank(), legend.position = &#39;bottom&#39;, legend.direction = &#39;horizontal&#39; )+ xlab(&#39;RES&#39;)+ ylab(&#39;Count&#39;) png(&#39;plot5.png&#39;, height = 20, width = 20, units = &#39;cm&#39;, res= 800) print(p5) dev.off() 4.6.2.2 随机森林 gx.rf&lt;-randomForest(gdp~.,data=gxdata_without_x,importance=TRUE, ntree=1000) importance(gx.rf) varImpPlot(gx.rf) 4.6.3 朴素贝叶斯 待添加 "],["Model-performance.html", "第 5 章 模型评估 5.1 模型评估辅助： 5.2 常用模型辅助评估R包：", " 第 5 章 模型评估 5.1 模型评估辅助： 5.1.1 purrr包的pluck()函数字符提取 ## 使用purrr包的pluck()函数帮助快速提取统计结果： ## 与传统代码提取结果相比，传统提取往往需要在统计结果中使用列表查询或者子查询； ## 而使用purrr::pluck() 可以指定查询的字符串（实现多级查询），然后输出结果，提高索引效率。 t.test_results %&gt;% pluck(&quot;age&quot;) # alternatively, use pluck(1) t.test_results %&gt;% pluck(&quot;age&quot;, &quot;p.value&quot;) # 多级索引，age列表下的p.value() ## 使用map()函数来辅助批量提取： t.test_results %&gt;% map(pluck, &quot;p.value&quot;) # return every p-value ### 更简单的辅助提取方法，是直接使用purrr族的相关函数： t.test_results %&gt;% {tibble( variables = names(.), p = map_dbl(., &quot;p.value&quot;), ## 不用经过pluck()函数，可以直接提取对应统计值； means = map(., &quot;estimate&quot;))} 5.2 常用模型辅助评估R包： performance：计算、分析和测试统计模型的性能。 parameters：提取几乎所有统计模型参数的综合数据框，并提供帮助以优雅的表格和图表呈现它们。 "],["id_06-Model-release-and-application.html", "第 6 章 模型发布与应用 6.1 flexboard 6.2 shinydashboard 6.3 Rmarkdown使用进阶", " 第 6 章 模型发布与应用 6.1 flexboard 6.1.1 安装工作环境 install.packages(&quot;flexdashboard&quot;) library(flexdashboard) 6.1.2 多级导航模块 6.1.2.1 构建一级导航 --- title: &quot;Multiple Pages&quot; output: flexdashboard::flex_dashboard --- # Page 1 ## 使用等号来赋值新的导航页。默认为纵向； # `===================================== ` ### Chart 1 # 修改默认页为纵向 # Page 2 {data-orientation=rows} # `===================================== ` 6.1.2.2 构建二级下拉菜单 # --- # title: &quot;Page Navigation Menus&quot; # output: flexdashboard::flex_dashboard # --- # # Page 1 {data-navmenu=&quot;Menu A&quot;} # `=====================================` # Page 2 {data-navmenu=&quot;Menu A&quot;} # `=====================================` # Page 3 {data-navmenu=&quot;Menu B&quot;} # `=====================================` # Page 4 {data-navmenu=&quot;Menu B&quot;} # `=====================================` 6.1.3 超级链接表 6.1.3.1 提供显著引用 # Page 1 # `===================================== ` # You can link to a dashboard page with either of the following syntaxes: # # 方法1 # [Page 2] # # 方法2可以修改指引的名称； # [Page Two](#page-2) 6.1.3.2 使用图标提供显著引用 # Page 1 {data-icon=&quot;fa-list&quot;} # `=====================================` 6.1.3.3 隐藏最后一级的引用 # 暂时没想到这个怎么用； # Page 3 {.hidden} -- 就不能直接引用这个； 6.1.4 给导航栏添加Social Links --- title: &quot;Social Links&quot; output: flexdashboard::flex_dashboard: social: [ &quot;twitter&quot;, &quot;facebook&quot;, &quot;menu&quot; ] --- 6.1.5 给导航栏添加Source Code --- title: &quot;Source Code&quot; output: flexdashboard::flex_dashboard: ## 源代码嵌入 source_code: embed ## GITHUB:url嵌入； source_code: &quot;http:/&quot; --- 6.1.6 给导航栏添加一个about --- title: &quot;Navigation Bar&quot; output: flexdashboard::flex_dashboard: navbar: - { title: &quot;About&quot;, href: &quot;https://example.com/about&quot;, align: left } --- 6.1.7 布局参数 6.1.7.1 行列布局参数配置 ## 默认为行构建； ## 按列指定构建； Column -- 单列 - ### Chart 1 Column -- 多列 - ### Chart 2 ### Chart 3 Column {.tabset} - 形成合并框构建； Column{.tabset .tabset-fade} - 渐隐合并框； - ### Chart 2 ### Chart 3 ## 直接指定数据展示的高度，有时候比定义图片高度更好用； Row {data-height=600} - 6.1.7.2 图形参数配置 # 常用通常修改图形方式； {r, fig.width=5, fig.height=5} # 默认情况下，flexdashboard 在图表边缘放置 8 个像素的填充。 # 添加.no-padding属性以指定完全没有填充，也可以添加属性data-padding以指定特定数量的像素。 ### Chart 1 {.no-padding} ### Chart 2 {data-padding=10} 6.1.7.3 表格参数配置 ## 默认静态配置 knitr::kable(mtcars) DT::datatable(mtcars, options = list( bPaginate = FALSE )) ## 动态参数配置--更新； renderTable({ head(mtcars, n = input$rows) }) DT::renderDataTable({ data &lt;- head(mtcars, n = input$maxrows) DT::datatable(data, options = list( bPaginate = FALSE )) }) 6.1.7.4 文字描述 正常可以在一页的任意位置写入即可，表示单独的块；可以给他用###起名； 也可以在新的一页中定义； 6.1.7.5 排除标题 All Lung Deaths {.no-title} —不展示标题； dygraph(ldeaths) 6.1.8 功能块参数 6.1.8.1 图标网站来源 https://fontawesome.com/icons/twitter?s=solid&amp;f=brands 引用方式”fa-comments” #### 计数box ### Comments per Day --内置到box中的数字标签； comments &lt;- computeComments() # --输出数字； valueBox(comments, icon = &quot;fa-comments&quot;) ## shiny样式；--即时输入； renderValueBox({ articles &lt;- computeArticles(input$types) valueBox(articles, icon = &quot;fa-pencil&quot;, color = ifelse(articles &gt; 100, &quot;success&quot;, &quot;info&quot;)) }) 6.1.8.2 仪表盘（比例动态） rate &lt;- computeContactRate() gauge(rate, min = 0, max = 100, symbol = &#39;%&#39;, gaugeSectors( success = c(80, 100), warning = c(40, 79), danger = c(0, 39) )) renderGauge({ rate &lt;- computeContactRate(input$region) gauge(rate, min = 0, max = 100, symbol = &#39;%&#39;, gaugeSectors( success = c(80, 100), warning = c(40, 79), danger = c(0, 39) )) }) 6.1.9 故事版 6.1.9.1 主题故事版构建 ## 在titile中提供storyboard: true； ## 其他部分正常些即可； --- title: &quot;Storyboard&quot; output: flexdashboard::flex_dashboard: storyboard: true --- 此时page1将被当做导航标题： ## page1 ` # 讲述一个故事 6.1.9.2 插入某一页面为故事版 --- title: &quot;Storyboard Page&quot; output: flexdashboard::flex_dashboard --- Analysis {.storyboard} ========================================= 6.1.10 辅助dashboard展示参数 6.1.10.1 去除手机显示中的参数展示 --- title: &quot;Storyboard Commentary&quot; output: flexdashboard::flex_dashboard: storyboard: true --- 6.1.10.2 指定展示的主题 --- title: &quot;Themes&quot; output: flexdashboard::flex_dashboard: theme: bootstrap --- ## 可以使用的主题列表 default cosmo bootstrap cerulean journal flatly readable spacelab united lumen paper sandstone simplex yeti 6.1.10.3 给展示网站提供logo ## 图片控制在(48 pixels high for the default “cosmo --- title: &quot;Logo and Favicon&quot; output: flexdashboard::flex_dashboard: logo: logo.png favicon: favicon.png --- 6.2 shinydashboard 6.2.1 基本框架和流程 6.2.1.1 基本组成框架 ## ui.R library(shinydashboard) dashboardPage( dashboardHeader(), dashboardSidebar(), ## 侧边栏 dashboardBody() ## 主体； ) ## app.R ## library(shiny) library(shinydashboard) ## 其中skin可用的颜色为The default is blue, also black, purple, green, red, and yellow； ui &lt;- dashboardPage(header, sidebar, body, skin = skin) server &lt;- function(input, output) { } shinyApp(ui, server) 6.2.1.2 基于ui和server来开发shiny的流程范式 1、侧边栏内部可以写`sliderInput`，body内部也可以写； 2、每个ui中的输入都需要指定输出的`name`,然后在 `output$name`接受ui的参数，然后再进行运算。 3、ui和server都会存在显示效果； 6.2.2 仪表板的结构 6.2.2.1 dashboardHeader()标题栏 # 默认形式 # 其中titleWidth = 450可以修改默认的标题长度，适用于特别长的标题选择； dashboardHeader(title = &quot;My Dashboard&quot;,titleWidth = 450) 6.2.2.1.1 消息菜单 6.2.2.1.2 静态消息 # 直接放置在dashboardHeader()内部即可； dropdownMenu(type = &quot;messages&quot;, messageItem( from = &quot;Sales Dept&quot;, message = &quot;Sales are steady this month.&quot; ), messageItem( from = &quot;New User&quot;, message = &quot;How do I register?&quot;, icon = icon(&quot;question&quot;), time = &quot;13:45&quot; )) 6.2.2.1.3 动态消息 ## 感觉可以用于统计访问患者数等； ## ui: dashboardHeader(dropdownMenuOutput(&quot;messageMenu&quot;)) ## server: output$messageMenu &lt;- renderMenu({ # Code to generate each of the messageItems here, in a list. This assumes # that messageData is a data frame with two columns, &#39;from&#39; and &#39;message&#39;. msgs &lt;- apply(messageData, 1, function(row) { messageItem(from = row[[&quot;from&quot;]], message = row[[&quot;message&quot;]]) }) # This is equivalent to calling: # dropdownMenu(type=&quot;messages&quot;, msgs[[1]], msgs[[2]], ...) dropdownMenu(type = &quot;messages&quot;, .list = msgs) }) 6.2.2.1.4 通知菜单 dropdownMenu(type = &quot;notifications&quot;, notificationItem( text = &quot;5 new users today&quot;, icon(&quot;users&quot;) ), notificationItem( text = &quot;12 items delivered&quot;, icon(&quot;truck&quot;), status = &quot;success&quot; )) 6.2.2.1.5 任务菜单 ## 用于显示项目进度； ---这个如果能做成动态的效果会很好； dropdownMenu(type = &quot;tasks&quot;, badgeStatus = &quot;success&quot;, taskItem(value = 90, color = &quot;green&quot;, &quot;Documentation&quot; ), taskItem(value = 17, color = &quot;aqua&quot;, &quot;Project X&quot; ), taskItem(value = 75, color = &quot;yellow&quot;, &quot;Server deployment&quot; ), taskItem(value = 80, color = &quot;red&quot;, &quot;Overall project&quot; )) 6.2.2.1.6 禁用标题 dashboardHeader(disable = TRUE) 6.2.2.2 dashboardSidebar()侧边栏 6.2.2.2.1 构建侧边栏 # 其中&quot;Dashboard&quot;为侧边栏的实际显示名，`tabName`为引用名； dashboardSidebar( ## 调整侧边栏的宽度； width = 350, sidebarMenu( menuItem(&quot;Dashboard&quot;, tabName = &quot;dashboard&quot;, icon = icon(&quot;dashboard&quot;)), menuItem(&quot;Widgets&quot;, tabName = &quot;widgets&quot;, icon = icon(&quot;th&quot;)) ) ) 6.2.2.2.2 menuItem() 参数补充 # badgeLabel 在侧边栏中提供提示，用于说明功能性； menuItem(&quot;Widgets&quot;, icon = icon(&quot;th&quot;), tabName = &quot;widgets&quot;, badgeLabel = &quot;new&quot;, badgeColor = &quot;green&quot;) 6.2.2.2.3 提供不同网站间的相互引用 menuItem(&quot;Source code&quot;, icon = icon(&quot;file-code-o&quot;), href = &quot;https://github.com/rstudio/shinydashboard/&quot;) 6.2.2.2.4 menuSubItem() 次级标签引入；- # 需要包裹在menuItem内部，也即 menuItem(&quot;Charts&quot;, icon = icon(&quot;bar-chart-o&quot;), menuSubItem(&quot;Sub-item 1&quot;, tabName = &quot;subitem1&quot;), menuSubItem(&quot;Sub-item 2&quot;, tabName = &quot;subitem2&quot;)) 6.2.2.2.5 sidebarUserPanel() 展示用户信息- # A panel displaying user information in a sidebar sidebarUserPanel(&quot;User Name&quot;, subtitle = a(href = &quot;#&quot;, icon(&quot;circle&quot;, class = &quot;text-success&quot;), &quot;Online&quot;) # Image file should be in www/ subdir # image = &quot;./code_review.Ruserimage.png&quot;) 6.2.2.2.6 提供多功能组件间的检索功能 ## 这个功能可以检索多个tib，需要参考 ?dashboardSidebar() sidebarSearchForm(label = &quot;Enter a number&quot;, &quot;searchText&quot;, &quot;searchButton&quot;), ## 用法实现还是用处不大； sidebarSearchForm(textId = &quot;searchText&quot;, buttonId = &quot;searchButton&quot;, label = &quot;Search dataset&quot;, icon = shiny::icon(&quot;search&quot;)) example_data &lt;- data.frame(ID = 1:7, word = c(&quot;random&quot;, &quot;words&quot;, &quot;to&quot;, &quot;test&quot;, &quot;the&quot;, &quot;search&quot;, &quot;function&quot;)) output$filtered_table &lt;- renderTable({ req(input$searchButton == TRUE) example_data[input$searchText,] 6.2.2.2.7 添加文字和换行 # 换行 br(),br(),br(),br(), # 添加文字方式1: # p(HTML(&quot;&lt;b&gt; Teague Tian&lt;/b&gt;&quot;),style=&quot;white-space: pre-wrap&quot;), # p(HTML(&quot;&lt;b&gt; 医学统计师&lt;/b&gt;&quot;),style=&quot;white-space: pre-wrap&quot;) # 添加文字方式2 # p(HTML(&#39;&amp;nbsp;&#39;),HTML(&#39;&amp;nbsp;&#39;),HTML(&#39;&amp;nbsp;&#39;),HTML(&#39;&amp;nbsp;&#39;),HTML(&#39;&amp;nbsp;&#39;),HTML(&#39;&amp;nbsp;&#39;), # HTML(&#39;&amp;nbsp;&#39;),strong(&quot;Teague Tian&quot;)), # p(HTML(&#39;&amp;nbsp;&#39;),HTML(&#39;&amp;nbsp;&#39;),HTML(&#39;&amp;nbsp;&#39;),HTML(&#39;&amp;nbsp;&#39;),HTML(&#39;&amp;nbsp;&#39;),HTML(&#39;&amp;nbsp;&#39;), # HTML(&#39;&amp;nbsp;&#39;),strong(&quot;医学统计师&quot;)) 6.2.2.2.8 给sider()内部提供ID,用于动态监测多级程序运行 - ## 如果不提供ID，由于无法知道程序运行的检测，可能导致部分分项无法打开运行； dashboardSidebar( sidebarMenu( # Setting id makes input$tabs give the tabName of currently-selected tab id = &quot;tabs&quot;, menuItem(&quot;Dashboard&quot;, tabName = &quot;dashboard&quot;, icon = icon(&quot;dashboard&quot;)), textOutput(&quot;res&quot;))) server &lt;- function(input, output, session) { output$res &lt;- renderText({ paste(&quot;You&#39;ve selected:&quot;, input$tabs) }) } ## 此外，在之前的版本中如果使用子项快速检索提取的位置，但不会返回来自哪一项； # 目前的版本中可以使用input$sidebarItemExpanded来返回总级别 server{ req(input$sidebarItemExpanded) paste(&quot;Expanded menuItem:&quot;, input$sidebarItemExpanded)} 6.2.2.2.9 在server内用renderMenu{}构建多级标签 dashboardSidebar( sidebarMenu( # Setting id makes input$tabs give the tabName of currently-selected tab id = &quot;tabs&quot;, menuItem(&quot;Dashboard&quot;, tabName = &quot;dashboard&quot;, icon = icon(&quot;dashboard&quot;)), menuItem(&quot;Widgets&quot;, icon = icon(&quot;th&quot;), tabName = &quot;widgets&quot;), menuItem(&quot;Charts&quot;, icon = icon(&quot;bar-chart-o&quot;), menuSubItem(&quot;Sub-item 1&quot;, tabName = &quot;subitem1&quot;), menuSubItem(&quot;Sub-item 2&quot;, tabName = &quot;subitem2&quot;)))) ## 等价于 ui &lt;- dashboardPage( dashboardHeader(), dashboardSidebar( sidebarMenuOutput(&quot;menu&quot;), textOutput(&quot;res&quot;)), dashboardBody( tabItems( tabItem(&quot;dashboard&quot;, &quot;Dashboard tab content&quot;), tabItem(&quot;widgets&quot;, &quot;Widgets tab content&quot;), tabItem(&quot;subitem1&quot;, &quot;Sub-item 1 tab content&quot;), tabItem(&quot;subitem2&quot;, &quot;Sub-item 2 tab content&quot;) ))) server &lt;- function(input, output, session) { output$res &lt;- renderText({ paste(&quot;You&#39;ve selected:&quot;, input$tabs)}) output$menu &lt;- renderMenu({ sidebarMenu( # Setting id makes input$tabs give the tabName of currently-selected tab id = &quot;tabs&quot;, menuItem(&quot;Dashboard&quot;, tabName = &quot;dashboard&quot;, icon = icon(&quot;dashboard&quot;)), menuItem(&quot;Widgets&quot;, icon = icon(&quot;th&quot;), tabName = &quot;widgets&quot;), menuItem(&quot;Charts&quot;, icon = icon(&quot;bar-chart-o&quot;), menuSubItem(&quot;Sub-item 1&quot;, tabName = &quot;subitem1&quot;), menuSubItem(&quot;Sub-item 2&quot;, tabName = &quot;subitem2&quot;)))})} 6.2.2.3 dashboardBody()内部主体构建 大多数仪表板的基本构建块是box. 盒子又可以包含任何内容。 box(title = &quot;Box title&quot;, height = 300, &quot;Box content&quot;) 6.2.2.3.1 在body内部引用多级侧边栏的方法 ## 使用tabItems()来获取侧边栏，然后利用tabItem()来捕获每个侧边栏的内容； dashboardBody( tabItems( # First tab content tabItem(tabName = &quot;dashboard&quot;, fluidRow( box(plotOutput(&quot;plot1&quot;, height = 250)), box( title = &quot;Controls&quot;, sliderInput(&quot;slider&quot;, &quot;Number of observations:&quot;, 1, 100, 50) ))), # Second tab content tabItem(tabName = &quot;widgets&quot;, h2(&quot;Widgets tab content&quot;) ))) 6.2.2.3.2 在body内部创建多个多级标签页 ## 使用 tabBox 来创建多级标签页 ## body &lt;- dashboardBody( fluidRow( tabBox( title = &quot;First tabBox&quot;, # The id lets us use input$tabset1 on the server to find the current tab id = &quot;tabset1&quot;, height = &quot;250px&quot;, tabPanel(&quot;Tab1&quot;, &quot;First tab content&quot;), tabPanel(&quot;Tab2&quot;, &quot;Tab content 2&quot;) ), tabBox( side = &quot;right&quot;, height = &quot;250px&quot;, selected = &quot;Tab3&quot;, tabPanel(&quot;Tab1&quot;, &quot;Tab content 1&quot;), tabPanel(&quot;Tab2&quot;, &quot;Tab content 2&quot;), tabPanel(&quot;Tab3&quot;, &quot;Note that when side=right, the tab order is reversed.&quot;) )), fluidRow( tabBox( # Title can include an icon title = tagList(shiny::icon(&quot;gear&quot;), &quot;tabBox status&quot;), tabPanel(&quot;Tab1&quot;, &quot;Currently selected tab from first box:&quot;, verbatimTextOutput(&quot;tabset1Selected&quot;) ), tabPanel(&quot;Tab2&quot;, &quot;Tab content 2&quot;)))) shinyApp( ui = dashboardPage( dashboardHeader(title = &quot;tabBoxes&quot;), dashboardSidebar(), body), server = function(input, output) { # The currently selected tab from the first box output$tabset1Selected &lt;- renderText({ input$tabset1})}) 6.2.3 布局参数 6.2.3.1 行列布局 6.2.3.1.1 fluidRow() 创造自适应的布局页面应用在body内部； # Create a page with fluid layout fluidRow( box(plotOutput(&quot;plot1&quot;, height = 250)), box( title = &quot;Controls&quot;, sliderInput(&quot;slider&quot;, &quot;Number of observations:&quot;, 1, 100, 50) )) # 此外，还有fluidPage,与此fluidRow类似，但fluidRow更适合管理多级页面； 6.2.3.1.2 column(width = 4,…box())纵向展示 body &lt;- dashboardBody( fluidRow( column(width = 4, box( title = &quot;Box title&quot;, width = NULL, status = &quot;primary&quot;, &quot;Box content&quot; ), box( title = &quot;Title 1&quot;, width = NULL, solidHeader = TRUE, status = &quot;primary&quot;, &quot;Box content&quot; ), box( width = NULL, background = &quot;black&quot;, &quot;A box with a solid black background&quot; )))) 6.2.3.1.3 混合模式布局 # 交叉使用fluidRow()和column()来构建参数分布图 body &lt;- dashboardBody( fluidRow( box( title = &quot;Box title&quot;, width = 6, status = &quot;primary&quot;, &quot;Box content&quot; ), box( status = &quot;warning&quot;, width = 6, &quot;Box content&quot; ) ), fluidRow( column(width = 4, box( title = &quot;Title 1&quot;, width = NULL, solidHeader = TRUE, status = &quot;primary&quot;, &quot;Box content&quot; ), box( width = NULL, background = &quot;black&quot;, &quot;A box with a solid black background&quot; )))) 6.2.4 在dashboard中使用css技巧 暂定，还没找到合适的资源学习； ### 功能块参数 #### infoBox和valueBox library(shinydashboard) ui &lt;- dashboardPage( dashboardHeader(title = &quot;Value boxes&quot;), dashboardSidebar(), dashboardBody( fluidRow( # A static valueBox valueBox(10 * 2, &quot;New Orders&quot;, icon = icon(&quot;credit-card&quot;)), # Dynamic valueBoxes ## 基本逻辑是输入任意一种命名的box； ## 然后在下面使用input函数来承接； valueBoxOutput(&quot;progressBox&quot;), valueBoxOutput(&quot;approvalBox&quot;)), fluidRow( # Clicking this will increment the progress amount ## 这里的actionButton就是承接用户输入，并反馈输出； box(width = 4, actionButton(&quot;count&quot;, &quot;Increment progress&quot;))))) server &lt;- function(input, output) { output$progressBox &lt;- renderValueBox({ valueBox( paste0(25 + input$count, &quot;%&quot;), &quot;Progress&quot;, icon = icon(&quot;list&quot;), color = &quot;purple&quot;)}) output$approvalBox &lt;- renderValueBox({ valueBox( &quot;80%&quot;, &quot;Approval&quot;, icon = icon(&quot;thumbs-up&quot;, lib = &quot;glyphicon&quot;), color = &quot;yellow&quot;)})} shinyApp(ui, server) 6.2.5 适配于shiny的特殊函数 req() --判断数据返回的真假； 如果数据返回不符合条件限制，则该运算停止； output$filtered_table &lt;- renderTable({ + req(input$searchButton == TRUE) ......} 6.3 Rmarkdown使用进阶 6.3.1 参考书目 ### 权威指南： https://bookdown.org/yihui/rmarkdown/ # 下面这本书非常有用；！！！ https://bookdown.org/yihui/rmarkdown-cookbook/multi-column.html ## 其他参考博客： 速查表：https://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf?_ga=2.258283010.941366294.1644810156-143414957.1644300675 全局代码设置； （https://yihui.org/knitr/options/） 6.3.2 title和auther/YMAL --- title: &quot;ters&quot; author: &quot;nicheerfeng&quot; date: &quot;2022/2/14&quot; output: html_document ## 另外还有pdf_document // word_document --- 复杂常用参数； --- title: &quot;Habits&quot; output: html_document: toc: ture #是否展示目录 toc_float: true #目录的形式，是否浮动 number_sections: true #各个标题的数字标记是否展示 df_print: paged #表格的形式，paged创建可分页的表 theme: cerulean #文档主题，来源于Bootswatch(https://bootswatch.com/) highlight: tango #指定语法高亮样式 #css: css/styles.css #加入额外的CSS，如果想从自己的css为文档提供所有样式，theme和highlight可设置为null #fig_width: 7 #图片宽度 #fig_height: 6 #图片高度 #fig_caption: TRUE #图片设置，控制图形是否带有标题 #code_folding: hide #是否隐藏代码块 #self_contained: false #在外部文件中保留依赖关系 #keep_md: true #是否在knitr处理，pandoc渲染之后保存一份markdown文件的副本 #template: quarterly_report.html #可以使用模板选项替换基础pandoc模板 --- include参数： 主要用于在输出文档中添加额外的内容（在文档标题中或文档正文之前/之后包含内容） --- title: &quot;Habits&quot; output: html_document: includes: in_header: header.html before_body: doc_prefix.html after_body: doc_suffix.html --- author选项的补充： --- title: &#39;This is the title: it contains a colon&#39; author: - name: Author One affiliation: University of Somewhere - name: Author Two affiliation: University of Nowhere tags: [nothing, nothingness] # 添加tag选项； abstract: | ## 添加abstract This is the abstract. It consists of two paragraphs. --- 输出结果的补充： --- title = data_view ##注意这里的``的作用是外循环中执行； author: &quot;Rsh&quot; date: &#39;2020/09/1&#39; params: name: = &#39;input you data name&#39; output: html_document: ## 输出html格式的 文件； theme:cerulean highlight:tango --- 6.3.3 字体与格式 ### 字体： **这是加粗的文字** *这是倾斜的文字*` ***这是斜体加粗的文字*** ~~这是加删除线的文字~~ ### 使用内嵌的html来编辑rmarkdown中字号和颜色； &lt;font face=&quot;宋体&quot;&gt;宋体&lt;/font&gt; &lt;font face=&quot;宋体&quot; color=red&gt;红色宋体&lt;/font&gt; &lt;font face=&quot;宋体&quot; color=red size=5&gt;5号红色宋体&lt;/font&gt; ## 修改字体颜色：使用html作为元素插入到代码块中； 我是\\textcolor{blue}{庄闪闪}呀！欢迎关注我的\\textcolor{red}{公众号}：\\textcolor{blue}{庄闪闪的R语言手册}。 ## 另外一种打印红色字体的方法; &lt;span style=&quot;color: red;&quot;&gt;**_DANGER:_** This is a warning.&lt;/span&gt; 6.3.4 辅助标记 ## 分割线： ---这是分割线 ***这也是分割线 ### 代码注释不被编号，仅适用于rmarkdwon，而不适用于markdown中： # Preface {-} ## 注释掉文本： ctrl+shift+c:富文本注释方法，可以将标记的markdown文本注释掉； 时间设置： # date: &quot;`r Sys.Date`&quot; # 就可以输出当前的时间了； 内联代码： 在Rmarkdown语法中使用内联代码 r函数+外部参数 的方法来运行； 主要用于脚本的 title修改等方面使用； 比如说：[date:“2022-09-29”] 添加浮动导航栏： --- title: &quot;FAF&quot; author: &quot;nicheerfeng&quot; date: &quot;2022-09-27&quot; output: html_document: ## 注意缩进很重要！ toc: true toc_float: true # toc_collapsed: true # toc_depth: 3 # number_sections: true # theme: lumen --- 6.3.5 常用代码标签 ### 关闭打印： (```){r example label, echo = FALSE, warning = FALSE} coding (```) # 解释： 注意：这里的代码即为使用R语言的代码，设置了代码块的名称为 example label，使代码块不包括在文档中，同时不输出警告信息。为了防止转译添加的小括号，正式代码中没有。注意代码行注释的意义在于格底部包含的代码块导航器中方便于查询； ### 主要常用代码标签: 注意这些标签仅在生成对应html文件时才有用； 1)echo = FALSE：隐藏代码，但运行代码并产生所有输出，曲线图，警告和消息。 2)eval = FALSE：显示代码，但不实际运行。这样的代码段如果有标签， 可以在后续代码段中被引用。 3)fig.show = &quot;hide&quot;：隐藏图。 4)eval=FALSE：运行代码，但不显示输出。这对于设置代码和注释条件很有用很有帮助。打开新的R Markdown文档时，您可以在第一个代码块中看到一个示例 5)message = FALSE：防止软件包在加载时打印消息。这也抑制了函数生成的消息。 6)results = &quot;hide&quot;：隐藏打印输出。 7)warning = FALSE：防止软件包和功能显示警告。 ### 非常用代码块： 1) collapse选项： 一个代码块的代码、输出通常被分解为多个原样文本块中， 如果一个代码块希望所有的代码、输出都写到同一个原样文本块中， 加选项collapse=TRUE。 2) results = 结果补充； hide, 运行了代码后不显示运行结果。 hold, 一个代码块所有的代码都显示完， 才显示所有的结果。 3) ### 全局代码设置：##### （https://yihui.org/knitr/options/） （注意其他所有参数都可以通过knitr::opts_knit$set()来设计；） ### 修改工作目录： knitr::opts_knit$set(root.dir = &#39;desired/directorypath&#39;) 6.3.6 非常用代码标签 6.3.6.1 解决重复标签序列的问题 options(knitr.duplicate.label = ‘allow’) 6.3.6.2 将R代码转为rmarkdown形式 在R脚本中使用注释时，使用 `#+` 的形式，而不是`#` ,然后就可以得到R脚本； 然后脚本在R中另外新建，进行命名运算，就可以得到一个新的Rmarkdown； knitr::spin(&quot;r_script.R&quot;, knit = FALSE, format = &quot;Rmd&quot;) # 例如： #&#39; noew1 ## 注意#&#39; 将代码块分行； #+ hh ## 构建新的代码块； print(&quot;he&quot;) #&#39; noew21 #+ fhaufh print(&quot;jj&quot;) 6.3.6.3 将rmarkdown转为R脚本的形式 ## 您必须指定documentation = 2返回#&#39;注释中的完整文档。如果您的文档是纯代码，请指定documentation = 0。 knitr::purl(&quot;r_script.Rmd&quot;, documentation = 2) 6.3.6.4 Rmarkdown中执行换行： 使用对应的脚本注释后，添加两个空格，在再下一行添加信息的注释信息就可以得到对应的显示输出； 6.3.6.5 Rmarkdown中添加空白行： 在对应的注释行中添加： 6.3.6.6 rmarkdown中添加分页： 插入： 6.3.6.7 中文正常显示： pdf.options(family=&quot;GB1&quot;) 或者 pdf.options(height=10/2.54, width=10/2.54, family=“GB1”) 6.3.6.8 代码整洁; 加选项tidy=TRUE可以自动重新排列代码段， 使得代码段格式更符合规范 6.3.6.9 缓存代码结果，减少反复运行代码的风险： cache=TRUE， {r cache=TRUE} 6.3.6.10 修改页边距： 6.3.6.11 在ymal中修改页边距：添加下面代码即可； geometry: “left=2cm,right=2cm,top=2cm,bottom=2cm” 6.3.6.12 文字缩进： | When dollars appear its a sign | that your code does not quite align ## 这行会自动缩进； | Ensure that your math | in xaringan hath | been placed on a single long line 6.3.6.13 控制文本输出的宽度： options(width = 300) ##这个越小，实际显示宽度越宽； matrix(runif(100), ncol = 20) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] ## [1,] 0.6960712 0.3069383 0.9819571 0.8552895 0.7653925 0.5024550 0.67082752 0.53488795 0.405182251 0.7797764 0.34720928 0.3540547 0.89839023 0.4740390 0.7280266 0.55356090 0.4618603 0.1577412 0.7340524 0.4677173 ## [2,] 0.6202972 0.4887921 0.5384296 0.2036990 0.4295990 0.3109159 0.18254373 0.19132696 0.754489071 0.2670022 0.78905704 0.5442485 0.93291664 0.7920058 0.5394844 0.55208703 0.5965311 0.4785389 0.6611132 0.6020849 ## [3,] 0.4730650 0.4431196 0.6209938 0.3665569 0.1121109 0.8334048 0.97565366 0.59504944 0.007302545 0.8437772 0.51490802 0.8921057 0.57208919 0.1267545 0.4708121 0.47503083 0.3709792 0.6230875 0.6867100 0.9836900 ## [4,] 0.1569810 0.8768297 0.6342785 0.6505393 0.8196403 0.7680294 0.07120192 0.08931203 0.250627395 0.5689549 0.04105461 0.5459946 0.99335892 0.5193698 0.6739556 0.05065583 0.4283386 0.9834827 0.3078525 0.2747295 ## [5,] 0.3466511 0.6871590 0.5980750 0.7980536 0.5086072 0.8790518 0.84841277 0.54083108 0.148621666 0.1266417 0.00790655 0.6887134 0.01653424 0.5588134 0.2882896 0.63822283 0.3312142 0.7579220 0.2892632 0.3921596 6.3.6.14 文档内的引用：这里就是根据代码块中的命名进行引用方法了； See Figure ?? 6.3.6.15 全局设置的一些案例： 案例1： 结尾的斜线Figs/很重要。如果您使用 fig.path=‘Figs’，那么这些数字将进入主目录，但Figs作为其名称的初始部分。可以指定图的输出位置； knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path=&#39;Figs/&#39;, echo=FALSE, warning=FALSE, message=FALSE) 案例2： 全局打印,是否打印对应的代码；选择FALSE则不打印代码，只输出结果； knitr: opts_chunk$set(echo = TRUE) 6.3.7 快捷键 ## 添加新的代码块： ctrl+alt + i ## 运行当前块上方的所有块 Ctrl + Alt + P ## 运行当前的代码块： Ctrl + Shift + Enter ## 运行所有块： Ctrl + A + Enter 6.3.8 标题修改 ## 参见https://zhuanlan.zhihu.com/p/435553617 ## 自动左对齐；添加多行标题；（适用导出文件类型：html、pdf (不适用word）） --- title: | | Veryyyyyyy ## 自动居中对齐（适用导出文件类型：html、pdf (不适用word）） --- title: | &lt;center&gt; Veryyyyyyy &lt;/center&gt; &lt;center&gt; yyyyyyyyyyyyyy Looooo &lt;/center&gt; ## 自动换行对齐: 换行符法（适用文件类型： word 、html） --- title: &quot;YYYY \\n mdmdmdmd&quot; 6.3.9 图片处理 6.3.9.1 修改默认形式 默认形式为html格式的svg； 需要将其修改为pdf格式或者png格式： knitr::opts_chunk$ set ( dev = “cairo_pdf” ) knitr::opts_chunk$ set ( dev = “png” , dev.args = list ( type = “cairo-png” )) 6.3.9.2 fig.show() 设置图片合适： fig.show=‘asis’：表示plot在产生他们的代码后面 fig.show=‘hold’：所有代码产生的图片都放在一个完整的代码块之后 fig.show=‘animate’：表示将所有生成的图片合成一个动画图片 可以参考：https://bookdown.org/yihui/rmarkdown-cookbook/animation.html fig.show=‘hide’：表示产生所有图片,但是并不展示 6.3.9.3 其他参数： fig.width：设置图片输出的宽度 fig.height：设置图片输出的高度 fig.align 设置图片位置排版格式，默认为left,可以为right或者center fig.cap ：设置图片的标题 fig.subcap：设置图片的副标题out.width和out.height选项指定在输出中实际显示的宽和高，如果使用如”90%“这样的百分数单位则可以自动适应输出的大小。 fig.keep = “all”, 会把低级图形函数修改后的结果单独保存； “last”, 仅保留最后一个图形； “first”, 仅保留第一个图； “none”, 所有图都不显示出来。 6.3.9.4 插入外界图片： ![图的标题](xxx.png){width=50%} ## 举例： {r fig.width=10/2.54, fig.height=10/2.54, out.width=&quot;80%&quot;} 6.3.9.5 优化PNG代码图显示： 注意需要安装OptiPNG 参见：https://bookdown.org/yihui/rmarkdown-cookbook/optipng.html knitr::knit_hooks$set(optipng = knitr::hook_optipng) 6.3.10 表格输出 6.3.10.1 美化表格输出 使用kableExtra kable()函数的digits=选项可以控制小数点后数字位数， caption=选项可以指定表的标题内容。 knitr::kable(co) 用kableExtra(Zhu 2020[7])、huxtable (Hugh-Jones 2020[8])等扩展包来美化表格。 library(knitr) library(kableExtra) kable(iris) %&gt;% kable_styling(latex_options = &quot;striped&quot;) 添加外边框： bootstrap_options = “bordered” x_html &lt;- knitr:: kable(head(rock), “html”) kableExtra::kable_styling(x_html,bootstrap_options = “bordered”) 设置表格的宽度： 使用full_width = F使得表格横向不会填满整个页面 x_html &lt;- knitr:: kable(head(rock), &quot;html&quot;) kableExtra::kable_styling(x_html,bootstrap_options = &quot;striped&quot;, full_width = F) 表格对齐： x_html &lt;- knitr:: kable(head(rock), &quot;html&quot;) kableExtra::kable_styling(x_html,bootstrap_options = &quot;striped&quot;, full_width = F, position = &quot;left&quot;) 设置表格字体： x_html &lt;- knitr:: kable(head(rock), &quot;html&quot;) kableExtra::kable_styling(x_html,bootstrap_options = &quot;striped&quot;, full_width = T, font_size = 20) ## 设置表格字体； 针对指定行或者列设置填充颜色： x_html &lt;- knitr:: kable(head(rock), &quot;html&quot;) x_html &lt;- kableExtra::kable_styling(x_html, bootstrap_options = &quot;striped&quot;, full_width = T) kableExtra::column_spec(x_html,1:2, ## 针对第1和2列； bold = T, color = &quot;white&quot;, # 内部线颜色； background = &quot;#D7261E&quot;) # 外部背景颜色； 针对指定单元格实行指定函数式运算： x_html &lt;- knitr:: kable(head(rock), &quot;html&quot;) x_html &lt;- kableExtra::kable_styling(x_html, bootstrap_options = &quot;striped&quot;, full_width = T) kableExtra::column_spec(x_html,1:2, bold = T, color = &quot;white&quot;, background = &quot;#D7261E&quot;) 使用滚动表格 DT::datatable(linelist, rownames = FALSE, options = list(pageLength = 5, scrollX = TRUE), class = &#39;white-space: nowrap&#39; ) 6.3.11 结果输出 在另外的R脚本中执行命令，然后输出输出rmd的结果： # rmarkdown::render(&#39;report.Rmd&#39;, &#39;html_document&#39;) 6.3.11.1 生成可重复性的报告 使用get()函数来调用外部命令函数或者参数接口，从而实现函数批循环调用： 例如： # ----- 在rmarkdown 中的 文件中执行这个文件 ------- mydata = get(params$name) summary(mydata) # ----- 在rmarkdown的ymal文件中加入这个参数 ----- --- title = data_view: ## 这种格式也称之为内联代码； author: &quot;Rsh&quot; date = &#39;2020/09/1&#39; params: name: = &#39;input you data name&#39; output: html_document: ## 输出html格式的 文件； theme:cerulean highlight:tango --- # ----- 在外界R环境中调用下面循环体 library(datasets)name_list &lt;-c(&quot;airquality&quot;,&quot;mtcars&quot;,&quot;LifeCycleSavings&quot;)for(name in name_list){ render(&quot;用rmarkdown定制你的数据分析报告/可重复分析报告/模版.Rmd&quot;, params = list(name=name), output_file = paste0(name,&#39;数据集概览&#39;), )} 6.3.12 补充知识点 6.3.12.1 使用params --- title: Parameterized reports output: html_document params: ## 这个参数可以提供的更多元的内置设置，这样就更方便引入外界参数； state: Nebraska year: 2019 midwest: true --- 上面的代码，还可以启动gui来提高代码的访问效率： rmarkdown::render(&quot;input.Rmd&quot;, params = &quot;ask&quot;) 6.3.12.2 代码块计时 knitr::knit_hooks$set(time_it = local({ now &lt;- NULL function(before, options) { if (before) { # record the current time before each chunk now &lt;&lt;- Sys.time() } else { # calculate the time difference after a chunk res &lt;- difftime(Sys.time(), now) # return a character string to show the time paste(&quot;Time for this code chunk to run:&quot;, round(res,3)) ## 或者只记录时间不打印出来： # all_times[[options$label]] &lt;&lt;- res } } })) 6.3.12.3 纸张翻转 该函数适用于rmd文件中； 当在rmd文件中输出超长宽表时，希望能够使用word纸张横转的范式： 使用方法如下（注意在rmd文件中使用）： 需要source()这个函数体； LANDSCAPE_STOP &lt;- block_section( prop_section(page_size = page_size(orient = &quot;landscape&quot;), type = &quot;continuous&quot; )) 而部分表格则希望有不横转的情况： [r LANDSCAPE_STOP] – 将此页横表转长表； "],["Visualization-Tables-Figures.html", "第 7 章 可视化（表与图） 7.1 表格可视化 7.2 图形可视化", " 第 7 章 可视化（表与图） 7.1 表格可视化 7.1.1 快速图表可视化 ## 这里是生成优化表格的三件套；-好用 iris %&gt;% flextable::flextable() %&gt;% # convert to pretty image flextable::autofit() %&gt;% # format to one line per row flextable::save_as_docx(path = &quot;tabyl.docx&quot;) ## 快速查看表格：- 好用 library(knitr) library(kableExtra) kable(iris) %&gt;% kable_styling(latex_options = &quot;striped&quot;) ## 快速生成数据报告：-- 好用 # install.packages(&quot;rrtable&quot;) library(rrtable) ## 自动化报表输出： ## 生成报表图片： df2flextable2( sampleData3 ,vanilla= FALSE ) ## 生成描述性统计图片： mytable2flextable( mytable(Dx~.,data=acs) ,vanilla= FALSE ) ## 生成对应统计分类的html格式： data2HTML(sampleData3) ## 快速统计建模结果可视化：- 模型； library(sjPlot) library(sjmisc) library(sjlabelled) tab_model(m1) ## 注意仅支持统计结果，不支持表格； #################### shiny表格的可视化: ################################# ## DT 查看：--复杂使用参见R-data-shiny.md library(DT) datatable(iris) ## reactable --复杂使用参见R-data-shiny.md library(reactable) reactable(iris) ## datacleanr -- 快速生成数据报告，需要联网环境 它可以处理嵌套的表格，以及空间和时间序列数据。 install.packages(&quot;datacleanr&quot;) library(datacleanr) dcr_app(iris) ## 第一个功能很好用，可以用可视化的方法展示数据内部的结构； 7.1.2 编辑表 方法1： 不仅能够编辑表，还可以实现表数据的增删，以及筛选后导出； mtcars_new &lt;- DataEditR::data_edit(mtcars, save_as = &quot;mtcars_new.csv&quot;) 方法2： edit() 方法3： # DT::datatable(head(iris), editable = &#39;cell&#39;) 7.1.3 静态表格优化展示 7.1.3.1 knitr:kableExtra ## knitr:kableExtra #### library(knitr) library(kableExtra) iris2 &lt;- head(iris) knitr::kable(iris2, col.names = gsub(&quot;[.]&quot;, &quot; &quot;, names(iris))) ## 指定对齐： knitr::kable(iris2, align = &quot;lccrr&quot;) ## 添加标题： knitr::kable(iris2, caption = &quot;An example table caption.&quot;) ## 添加范式： kable(iris) %&gt;% kable_styling(latex_options = &quot;striped&quot;) ## 设置字体： kable(head(iris, 5), booktabs = TRUE) %&gt;% kable_styling(font_size = 8) ## 分组行/列 iris2 &lt;- iris[1:5, c(1, 3, 2, 4, 5)] names(iris2) &lt;- gsub(&#39;[.].+&#39;, &#39;&#39;, names(iris2)) kable(iris2, booktabs = TRUE) %&gt;% add_header_above(c(&quot;Length&quot; = 2, &quot;Width&quot; = 2, &quot; &quot; = 1)) %&gt;% add_header_above(c(&quot;Measurements&quot; = 4, &quot;More attributes&quot; = 1)) ## 进阶应用： (ipip50_nested &lt;- ipip50_nested %&gt;% mutate(model = map(data, ~lm(o.value ~ t.value, data = .)))) (ipip50_nested &lt;- ipip50_nested %&gt;% mutate(tidy = map(model, broom::tidy))) (tab &lt;- ipip50_nested %&gt;% unnest(tidy, .drop = T) %&gt;% select(Trait:std.error) %&gt;% rename(b = estimate, SE = std.error) %&gt;% gather(key = tmp, value = value, b, SE) %&gt;% unite(tmp, outcome, tmp, sep = &quot;.&quot;) %&gt;% spread(key = tmp, value = value)) tab %&gt;% select(-Trait) %&gt;% kable(., &quot;html&quot;, booktabs = T, escape = F, digits = 2, col.names = c(&quot;Term&quot;, rep(c(&quot;b&quot;, &quot;SE&quot;), times = 3))) %&gt;% kable_styling(full_width = F) %&gt;% column_spec(2:7, width = &quot;2cm&quot;) %&gt;% group_rows(&quot;Agreeableness&quot;,1,2) %&gt;% ## 添加新行，分组，并重命名： group_rows(&quot;Conscientiousness&quot;,3,4) %&gt;% group_rows(&quot;Extraversion&quot;,5,6) %&gt;% group_rows(&quot;Neuroticism&quot;,7,8) %&gt;% group_rows(&quot;Openness&quot;,9,10) %&gt;% ## 指定标题行的分类位置；1和2表示占位符，按顺序向后推； add_header_above(c(&quot; &quot; = 1, &quot;BMI&quot; = 2, &quot;Exercise&quot; = 2, &quot;Log Median Income&quot; = 2)) ## 生成多表： knitr::kable( list( head(iris[, 1:2], 3), head(mtcars[, 1:3], 5) ), caption = &#39;A Tale of Two Tables.&#39;, booktabs = TRUE ) 7.1.3.2 flextable() 静态动态均支持 data6 %&gt;% tabyl(insurance_type,sex,) %&gt;% adorn_totals(where=&quot;row&quot;) %&gt;% adorn_totals(where=&quot;col&quot;) %&gt;% adorn_percentages(denominator = &quot;col&quot;) %&gt;% adorn_pct_formatting() %&gt;% adorn_ns(position=&quot;front&quot;) %&gt;% adorn_title( row_name = &quot;insurance_type&quot;, col_name = &quot;Gender&quot;, placement=&quot;combined&quot; )-&gt;table2 mytable2&lt;-flextable(table2) mytable2 7.1.3.3 高度自定义gt() ## 参见; https://themockup.blog/posts/2020-09-04-10-table-rules-in-r/ https://themockup.blog/posts/2020-09-26-functions-and-themes-for-gt-tables/#gt-tables ## 这个博客主人的博客原文代码： ## 很有意义，可以用来学习写日常的rmarkdown； https://github.com/jthomasmock/themockup-blog/blob/master/posts/2020-09-26-functions-and-themes-for-gt-tables/index.qmd 7.1.4 动态表格优化展示 7.1.4.1 DT-reactable ## 参见资料： https://thinkr.fr/tableaux-interactifs-avec-r-pour-shiny-et-vos-pages-web/ ## data-clean-table https://clarewest.github.io/blog/post/making-tables-shiny/ ## DT包：- - -专注于展示表格相关的数据结构-使用shiny； ## 参见：https://rstudio.github.io/DT/ ## 其他复杂用法参见： library(DT) datatable(iris) ## 使用论文线表来可视化数据结构； datatable(head(iris), class = &#39;cell-border stripe&#39;) ## 表格编辑： DT::datatable(head(iris), editable = &#39;cell&#39;) ## 表格筛选可视化 DT::datatable(head(iris), editable = list( target = &#39;row&#39;, disable = list(columns = c(1, 3, 4)) )) ## 结合shiny来使用datatale() library(shiny) ui &lt;- fluidPage(titlePanel(&quot;DT table in Shiny&quot;), mainPanel(width = 12, DT::dataTableOutput(&quot;mytable&quot;))) server &lt;- function(input, output) { output$mytable &lt;- DT::renderDataTable(villagers, options = list(scrollX = TRUE), rownames = FALSE) } # Run the application shinyApp(ui = ui, server = server) ## DT中相关参数： cell-border：用于单元格的实心边框 compact: 减少行间距 hover: 悬停在光标上时突出显示行 nowrap：删除单元格中的换行符 order-column：突出显示表格排序所依据的列 row-bordercell-border: 仅用于列顶部和底部的边框（出于明显原因，只能同时使用） stripe：用于“条纹”线，即两种交替的颜色 display: 对于集合stripe, hover,row-border和order-column # 组合使用： iris %&gt;% datatable(class = &quot;cell-border compact hover order-column&quot;) 7.1.4.2 reactable 包 – - - -专注于展示表格相关的数据结构-使用shiny； ## install.packages(&quot;reactable&quot;) ## 参见；https://glin.github.io/reactable/ library(reactable) ## 直接使用： reactable(iris) ## 结合shiny来使用reactable； library(shiny) library(reactable) ui &lt;- fluidPage( reactableOutput(&quot;table&quot;) ) server &lt;- function(input, output) { output$table &lt;- renderReactable({ reactable(iris) }) } shinyApp(ui, server) ## 在Rmarkdwon中使用reactable() library(reactable) reactable(iris) 7.1.4.3 sparkline包生成交互式小型图表： ## 这个功能很有用： library(sparkline) iris_spark &lt;- iris %&gt;% group_by(Species) %&gt;% summarise(Sepal.Length = list(Sepal.Length), Sepal.Width = list(Sepal.Width), Petal.Length = list(Petal.Length), Petal.Width = list(Petal.Width)) iris_spark %&gt;% reactable( columns = list( Sepal.Length = colDef(cell = function(values) { sparkline(values, type = &quot;bar&quot;) }), Sepal.Width = colDef(cell = function(values, index) { sparkline(iris_spark$Sepal.Width[[index]]) }), Petal.Length = colDef(cell = function(values, index) { sparkline(iris_spark$Petal.Length[[index]], type = &quot;box&quot;) }), Petal.Width = colDef(cell = function(values, index) { sparkline(iris_spark$Petal.Width[[index]]) }) ) ) ## 在表格的默认添加交互式图表： iris %&gt;% reactable( defaultPageSize = 5, ## 指定显图表行数； defaultColDef = colDef(footer = function(values) { if (!is.numeric(values)) return() ## 添加末尾的交互式图表： sparkline(values, type = &quot;box&quot;, width = 100, height = 30) }) ) 7.1.4.4 formattable()添加颜色 ## formattable ######## install.packages(&quot;formattable&quot;) library(formattable) df &lt;- data.frame( id = 1:10, name = c(&quot;Bob&quot;, &quot;Ashley&quot;, &quot;James&quot;, &quot;David&quot;, &quot;Jenny&quot;, &quot;Hans&quot;, &quot;Leo&quot;, &quot;John&quot;, &quot;Emily&quot;, &quot;Lee&quot;), age = c(28, 27, 30, 28, 29, 29, 27, 27, 31, 30), grade = c(&quot;C&quot;, &quot;A&quot;, &quot;A&quot;, &quot;C&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;A&quot;, &quot;C&quot;, &quot;C&quot;), test1_score = c(8.9, 9.5, 9.6, 8.9, 9.1, 9.3, 9.3, 9.9, 8.5, 8.6), test2_score = c(9.1, 9.1, 9.2, 9.1, 8.9, 8.5, 9.2, 9.3, 9.1, 8.8), final_score = c(9, 9.3, 9.4, 9, 9, 8.9, 9.25, 9.6, 8.8, 8.7), registered = c(TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, FALSE), stringsAsFactors = FALSE) ### 动态结合表筛选过程展示建模结果 formattable(df, list( age = color_tile(&quot;white&quot;, &quot;orange&quot;), grade = formatter(&quot;span&quot;, style = x ~ ifelse(x == &quot;A&quot;, style(color = &quot;green&quot;, font.weight = &quot;bold&quot;), NA)), area(col = c(test1_score, test2_score)) ~ normalize_bar(&quot;pink&quot;, 0.2), final_score = formatter(&quot;span&quot;, style = x ~ style(color = ifelse(rank(-x) &lt;= 3, &quot;green&quot;, &quot;gray&quot;)), x ~ sprintf(&quot;%.2f (rank: %02d)&quot;, x, rank(-x))), registered = formatter(&quot;span&quot;, style = x ~ style(color = ifelse(x, &quot;green&quot;, &quot;red&quot;)), x ~ icontext(ifelse(x, &quot;ok&quot;, &quot;remove&quot;), ifelse(x, &quot;Yes&quot;, &quot;No&quot;))) )) ## 补充，formattable() 还可以结合DT的结果来输出形成阅读性较高的动态表格； formattable( pic_items, list( `sell_value` = color_tile(&quot;white&quot;, &quot;pink&quot;), `buy_value` = color_bar(&quot;lightblue&quot;), orderable = true_false_formatter ) ) %&gt;% as.datatable(escape = FALSE, options = list(scrollX = TRUE), rownames = FALSE) ## 另外一种颜色梯度的表格用法： library(formattable) products &lt;- data.frame(id = 1:5, price = c(10, 15, 12, 8, 9), rating = c(5, 4, 4, 3, 4), market_share = percent(c(0.1, 0.12, 0.05, 0.03, 0.14)), revenue = accounting(c(55000, 36400, 12000, -25000, 98100)), profit = accounting(c(25300, 11500, -8200, -46000, 65000))) sign_formatter &lt;- formatter(&quot;span&quot;, style = x ~ style(color = ifelse(x &gt; 0, &quot;green&quot;, ifelse(x &lt; 0, &quot;red&quot;, &quot;black&quot;)))) products %&gt;% formattable(list( price = color_tile(&quot;transparent&quot;, &quot;lightpink&quot;), rating = color_bar(&quot;lightgreen&quot;), market_share = color_bar(&quot;lightblue&quot;), revenue = sign_formatter, profit = sign_formatter)) 7.1.5 描述性统计表格分析 7.1.5.1 table1 table1包： 参见网页：https://cran.r-project.org/web/packages/table1/vignettes/table1-examples.html 特色：既可以指定对应的函数，也可以利用现成的分类数据进行平局值和标准差计算； library(table1) library(boot) melanoma2 &lt;- melanoma # Factor the basic variables that # we&#39;re interested in melanoma2$status &lt;- factor(melanoma2$status, levels=c(2,1,3), labels=c(&quot;Alive&quot;, # Reference &quot;Melanoma death&quot;, &quot;Non-melanoma death&quot;)) melanoma2$sex &lt;- factor(melanoma2$sex, levels=c(1,0), labels=c(&quot;Male&quot;, &quot;Female&quot;)) melanoma2$ulcer &lt;- factor(melanoma2$ulcer, levels=c(0,1), labels=c(&quot;Absent&quot;, &quot;Present&quot;)) label(melanoma2$sex) &lt;- &quot;Sex&quot; label(melanoma2$age) &lt;- &quot;Age&quot; label(melanoma2$ulcer) &lt;- &quot;Ulceration&quot; label(melanoma2$thickness) &lt;- &quot;Thickness&quot; ## 这两个unit就很有用；在输出表格中很有用； units(melanoma2$age) &lt;- &quot;years&quot; units(melanoma2$thickness) &lt;- &quot;mm&quot; ## 注意这里的分组变量使用的|status 。另外，像sex之类的变量，还可以设计为factor(sex) table1(~ sex + age + ulcer + thickness | status, data=melanoma2, overall=&quot;Total&quot;) ## 还可以指定函数，用于表格中参数的计算： ## 指定函数另外参见：https://zhuanlan.zhihu.com/p/466024679 my.render.cont &lt;- function(x) { with(stats.apply.rounding(stats.default(x), digits=2), c(&quot;&quot;, &quot;Mean (SD)&quot;=sprintf(&quot;%s (&amp;plusmn; %s)&quot;, MEAN, SD))) } my.render.cat &lt;- function(x) { c(&quot;&quot;, sapply(stats.default(x), function(y) with(y, sprintf(&quot;%d (%0.0f %%)&quot;, FREQ, PCT)))) } table1(strata, labels, groupspan=c(1, 1, 2), render.continuous=my.render.cont, render.categorical=my.render.cat) ## table1还支持输出表格样式： table1(~ age + sex + wt | treat, data=dat, topclass=&quot;Rtable1-zebra&quot;) 7.1.5.2 janitor包tabyl()功能 ### janitor包提供了生成表格和交叉表格的tabyl()功能，可以“装饰”或使用辅助功能进行修改，以显示百分比、比例、计数等。 pacman::p_load( rio, # File import here, # File locator skimr, # get overview of data tidyverse, # data management + ggplot2 graphics gtsummary, # summary statistics and tests rstatix, # summary statistics and statistical tests janitor, # adding totals and percents to tables scales, # easily convert proportions to percents flextable # converting tables to pretty images ) linelist %&gt;% tabyl(age_cat, gender) %&gt;% ## 交叉统计 adorn_totals(where = &quot;row&quot;) %&gt;% ## 添加汇总列： adorn_percentages(denominator = &quot;row&quot;) %&gt;% ## 将计数转为比例；按行或列 adorn_pct_formatting(digits = 1) %&gt;% ## 将比例转为百分比； adorn_ns(position = &quot;front&quot;) %&gt;% # display as: &quot;count (percent)&quot; adorn_title( # adjust titles row_name = &quot;Age Category&quot;, ## 这里是汇总命名的方法； col_name = &quot;Gender&quot;, placement = &quot;combined&quot;) %&gt;% flextable::flextable() %&gt;% # convert to pretty image flextable::autofit() %&gt;% # format to one line per row flextable::save_as_docx(path = &quot;tabyl.docx&quot;) 7.1.5.3 tableone library(survival) library(tableone) library(skimr) ## Load data data(pbc) pbc=pbc %&gt;% as.tbl() %&gt;% mutate(trt=factor(trt,labels = c(&quot;yes&quot;,&quot;no&quot;)), status=factor(status, labels =c(&quot;status&quot;,&quot;edema&quot;,&quot;stage&quot;))) df=pbc %&gt;% as.tbl() %&gt;% mutate(trt=factor(trt,labels = c(&quot;yes&quot;,&quot;no&quot;)), status=factor(status, labels =c(&quot;status&quot;,&quot;edema&quot;,&quot;stage&quot;))) %&gt;% select(time,age,sex,status,trt,bili,platelet) df # A tibble: 418 x 7 time age sex status trt bili platelet &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; 1 400 58.8 f stage yes 14.5 190 2 4500 56.4 f status yes 1.1 221 # … with 408 more rows ## 构建统计展示： T1=CreateTableOne(data=df,strata = c(&quot;trt&quot;)) print(T1,showAllLevels = TRUE) ## 指定部分检验结果： # 有时候，并不是所有数据都是服从正态分布，频数分布有时候也需要用到Fisher 检验，这时候需要我们自定义了。 # 这里在Print需要用nonnormal来指定哪些变量为非正态分布及exact 来指定确切概率法检验 T3=print(T1,showAllLevels = TRUE,nonnormal = c(&quot;bili&quot;,&quot;platelet&quot;), exact = &quot;sex&quot;) 7.1.5.4 compareGroups包 ## 参见：https://www.jianshu.com/p/8879d49064ae ## 功能： 可支持多种数据导入，如haven、readxl、readr等，也接受Tibble类型数据集。 内置descrTable的新函数，只需一步就可以构建描述性表。 支持R-markdown文档，支持HTML的分层表。 内置strataTable的新功能，可以按层(变量的值或级别)构建描述性表。 日期变量被视为连续非正态，执行中位数、四分位数和非参数检验。 在compareGroups和descrTable中添加新的参数var.equal。这允许在比较两组以上的比较。 ## 基础函数数据值展示： library(compareGroups) library(tidyverse) data(predimed) head(predimed) # ALL data descrTable( ~ ., data = predimed) res &lt;- compareGroups(group ~ age + sex + smoke + waist + hormo, data = predimed) res createTable(res) ### 结果输出展示； ## 函数输出默认为正态分布： 如果有非正态分布；我们需要对非正态分布进行指定，使用下面方法进行指定。 这里method 变量=1表示比较使用正态分布， 变量=2表示使用四分位间距， 变量=3表示使用分类变量比较， 变量=NA表示自动根据Shapiro-Wilks检测，做出是正态还是非正态方法 createTable(res) ## add non-normal test res=compareGroups(group ~ age + smoke + waist + hormo, data = predimed, method = c(waist = 2, age = 2)) createTable(res) ## OR或HR的展示 res1 &lt;- compareGroups(htn ~ age + sex + bmi + smoke, data = predimed, ref = 1) createTable(res1, show.ratio = TRUE) # HR展示： library(survival) predimed$tmain &lt;- with(predimed, Surv(toevent, event == &quot;Yes&quot;)) createTable(compareGroups(tmain ~ group + age + sex, data = predimed), show.ratio = TRUE) ## 输出xlxs格式： export2xls(createTable(res), file=&#39;table1.xlsx&#39;) 7.1.5.5 gtsummary() 7.1.5.5.1 基础配置 ### 常用默认主题配置： reset_gtsummary_theme() theme_gtsummary_language( language = &quot;zh-cn&quot;, decimal.mark = NULL, set_theme = TRUE) 7.1.5.5.2 描述性统计 7.1.5.5.2.1 5.5.2.1 基础描述性统计 ### tbl_summary式样，使用by分组统计： ### 输出HTML格式的表格报告： t1 &lt;- data_table4 %&gt;% tbl_summary(by=druggroup, type = list(all_continuous() ~ &#39;continuous2&#39;, all_dichotomous() ~ &quot;dichotomous&quot;), statistic = all_continuous()~c( &quot;{mean} ({sd})&quot;, &quot;{median} ({p25}, {p75})&quot;, &quot;{min}, {max}&quot;), ## 设定表格中小数的位数 digits = list(c(sex,age_cat,insurance_type)~2), label=list( sex~&quot;性别&quot;, age~&quot;入组年龄&quot;, age_cat~&quot;年龄组分布&quot;, insurance_type~&quot;医保类型&quot;, lab_item_name.x~&quot;肌钙蛋白cTn检查 [人数(比例)]&quot;, ), missing_text = &quot;Missing&quot;) %&gt;% as_gt() %&gt;% tab_spanner( label=&quot;DRUG GROUP&quot;, columns = starts_with(&quot;stat_&quot;) ) %&gt;% gtsave(&quot;t1.html&quot;,path=&quot;E:/projects/RX021PD0908/coronary/yunxian.report&quot;) ## 细节- 添加表格的次级合并分组： as_gt() %&gt;% ## 还可以进行分组生成一级标题： tab_spanner( label = &quot;用药分组&quot;, columns = starts_with(&quot;stat_&quot;) ) tbl_strata_ex1 &lt;- trial %&gt;% select(age, grade, stage, trt) %&gt;% mutate(grade = paste(&quot;Grade&quot;, grade)) %&gt;% tbl_strata( strata = grade, .tbl_fun = ~ .x %&gt;% tbl_summary(by = trt, missing = &quot;no&quot;) %&gt;% add_n(), .header = &quot;**{strata}**, N = {n}&quot; ) # Example 2 ---------------------------------- tbl_strata_ex2 &lt;- trial %&gt;% select(grade, response) %&gt;% mutate(grade = paste(&quot;Grade&quot;, grade)) %&gt;% tbl_strata2( strata = grade, .tbl_fun = ~.x %&gt;% tbl_summary( ## 这里的response label = list(response = .y), missing = &quot;no&quot;, statistic = response ~ &quot;{p}%&quot;) %&gt;% add_ci(pattern = &quot;{stat} ({ci})&quot;) %&gt;% modify_header(stat_0 = &quot;**Rate (95% CI)**&quot;) %&gt;% ## 这里上面一步的%&gt;% 也可以使用逗号来替换，但会增加一个百分号注释； modify_footnote(stat_0 = NA), ## 这个函数的作用是提取及合并数据： .combine_with = &quot;tbl_stack&quot;, ## 这一步是删除空白行： .combine_args = list(group_header = NULL), .quiet = TRUE) %&gt;% modify_caption(&quot;**Response Rate by Grade**&quot;) 7.1.5.5.2.2 5.5.2.2 组间描述性统计（P值） trial %&gt;% tbl_summary( by = trt, ## type = all_continuous() ~ &quot;continuous2&quot; ：将连续变量统计描述结果展示为多行，type设置为&quot;continuous2&quot;； type = all_continuous() ~ &quot;continuous2&quot;, statistic =all_continuous() ~ c( &quot;{mean} ({sd})&quot;, &quot;{median} ({p25}, {p75})&quot;, &quot;{min}, {max}&quot;), missing=&quot;always&quot;, ## 强制展示缺失值是否存在 missing_text=&#39;missing&#39;)%&gt;% ## 修改缺失值的显示标签 add_p(pvalue_fun = ~style_pvalue(.x, digits = 2), ## 定义检验形式：，组间比较，连续变量默认检验方法为秩和检验，分类变量为卡方检验， list(all_continuous() ~ &quot;t.test&quot;, all_categorical() ~ &quot;fisher.test&quot;)) %&gt;% add_n() %&gt;% ## 添加每个样本的数量描述性统计 add_overall %&gt;% modify_header(label = &quot;**Variable**&quot;) %&gt;% ## 更新列名； ## 添加表格标题 modify_caption(&quot;Patient Characteristics&quot;) %&gt;% ## 添加尾注：还有一种方法是直接使用modify_footnote # as_gt() %&gt;% #使用gt存在的问题是无法使用flex_table() # gt::tab_source_note(gt::md(&quot;*This data is simulated*&quot;)) ## 表格导出保存; as_flex_table() %&gt;% flextable::save_as_docx(.,path=&#39;C:/Users/DELL/tab_2.docx&#39;) 7.1.5.5.3 回归分析 7.1.5.5.3.1 5.5.3.1 线性回归/逻辑回归 ## Linear regression table_35 &lt;- glm(inpatient_time ~ fbg_diff + fbg_reg + age + sex + region + cad + hbp + stroke + ckd + group_1 + treatment_plan, family = &quot;gaussian&quot;, data = secondary5) %&gt;% tbl_regression( label = list( fbg_diff ~ &quot;空腹血糖变化差值&quot;, # pbg_diff ~ &quot;餐后血糖变化差值&quot;, fbg_reg ~ &quot;末次空腹血糖&lt;6.1mmol/l&quot;, age ~ &quot;年龄&quot;, sex ~ &quot;性别&quot;, # insurance_type ~ &quot;医疗保险&quot;, region ~ &quot;地区&quot;, cad ~ &quot;冠心病&quot;, # disease_num ~ &quot;合并疾病数目&quot;, group_1 ~ &quot;基础vs.预混&quot;, treatment_plan ~ &quot;糖尿病用药方案&quot;)) %&gt;% bold_p(0.05) %&gt;% modify_header(label = &quot;**变量**&quot;) table_35 ## 逻辑回归： mod1 &lt;- glm(response ~ trt + age + grade, trial, family = binomial) t1 &lt;- tbl_regression(mod1, exponentiate = TRUE) ## tab_regression参数解析： tbl_regression( x, label = NULL, ## 添加标签： exponentiate = FALSE, include = everything(), show_single_row = NULL, ## 单行或多行展示，例如性别是否要分组展示等情况； conf.level = NULL, intercept = FALSE, estimate_fun = NULL, pvalue_fun = NULL, tidy_fun = NULL, add_estimate_to_reference_rows = FALSE, show_yesno = NULL, exclude = NULL, ... ) ## 使用add_glance_table来输出函数结果中更多的参数： ## add_glance_table()来自一个补充的R包： 包括模型的R方、AIC信息、样本量等，仅能适用于lm函数，不适合glm mod_va&lt;-lm(retinol_μmolL~sex+age_cut+Region+SBP+DBP,data=my2) %&gt;% tbl_regression() mod_va%&gt;% add_glance_table( label = list(sigma ~ &quot;\\U03C3&quot;), include = c(r.squared, AIC, sigma,nobs)) 7.1.5.5.3.2 5.5.3.2 生存分析 library(survival) mod1 &lt;- glm(response ~ trt + age + grade, trial, family = binomial) t1 &lt;- tbl_regression(mod1, exponentiate = TRUE) # build survival model table t2 &lt;- coxph(Surv(ttdeath, death) ~ trt + grade + age, trial) %&gt;% tbl_regression(exponentiate = TRUE) # merge tables tbl_merge_ex1 &lt;- tbl_merge( tbls = list(t1, t2), tab_spanner = c(&quot;**Tumor Response**&quot;, &quot;**Time to Death**&quot;) ) 7.1.5.5.4 自定义函数输出 7.1.5.5.4.1 5.5.4.1 自定义相关系数 作者：一个柒 链接：https://zhuanlan.zhihu.com/p/506580074 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 # 示例中，vars为所有自变量，`SUA`为因变量，数据集为ds # vars &lt;- c(&quot;age&quot;,&quot;BMI&quot;) fun_spearman_r &lt;- function(data, variable, ...) { r = cor.test(data[[variable]],data$`SUA`,method = &quot;spearman&quot;)$estimate dplyr::tibble(r = r) } # 此处tibble(r=r)很必要 fun_spearman_p &lt;- function(data, variable, ...) { cor.test(data[[variable]],data$`SUA`,method = &quot;spearman&quot;)$p.value } # p不需要(p=p)，因为r为主体自定义函数，p为add_stat ds[vars]&lt;- sapply(ds[vars],as.numeric) tbl_custom_summary_ex &lt;- ds %&gt;% select(all_of(vars),SUA) %&gt;% ## tbl_custom_summary:输入自定义函数并且能解析tidyr的结果: tbl_custom_summary(include = all_of(vars), stat_fns = everything() ~ fun_spearman_r, statistic = everything() ~ &quot;{r}&quot;, digits = everything() ~ 3, # 保留3位小数 type = list(everything() ~ &quot;continuous&quot;), missing = &quot;no&quot;, label = list(`age` ~ &quot;年龄&quot;, `BMI` ~ &quot;身体质量指数(BMI)&quot;)) %&gt;% add_stat(fns = everything() ~ fun_spearman_p) %&gt;% ## 不太理解，这里也要使用自定义的函数来输入数据： modify_fmt_fun(add_stat_1 ~ function(x) style_pvalue(x, digits = 3)) %&gt;% # p值如果先保留小数再style_pvalue会出现本来应`&lt;0.001`但变成`0.001`的情况 modify_header(label = &#39;**变量**&#39;,stat_0 ~ &quot;**r**&quot;, add_stat_1 ~ &quot;**P值**&quot;) %&gt;% bold_labels() %&gt;% modify_footnote(all_stat_cols() ~ NA) # 不要脚注 7.1.5.5.4.2 5.4.4.2 效应值计算 ### 效应值计算函数方法1： library(gtsummary) library(tidyverse) # function that returns either Cohen&#39;s D or the 1988 interpretation of its size CohenD &lt;- function(data, variable, by, interpret_d = FALSE, ...) { # Cohen&#39;s d, Hedges&#39;s g (correction=TRUE or FALSE) and Glass’s delta ES &lt;- effectsize::cohens_d(data[[variable]], factor(data[[by]]), ci=.90, pooled_sd=TRUE, paired=FALSE, correction=TRUE) if (interpret_d == TRUE) return(stringr::str_glue(&quot;{effectsize::interpret_d(ES$Cohens_d, rules = &#39;cohen1988&#39;)}&quot;)) # Formatting statistic with CI est &lt;- style_sigfig(ES$Cohens_d) ci_lower &lt;- style_sigfig(ES$CI_low) ci_upper &lt;- style_sigfig(ES$CI_high) # Returning estimate with CI together stringr::str_glue(&quot;{est} ({ci_lower}, {ci_upper})&quot;) } CohenD_interpret_d &lt;- purrr::partial(CohenD, interpret_d = TRUE) trial %&gt;% select(trt, age, marker) %&gt;% tbl_summary(by = trt, missing = &quot;no&quot;, statistic = all_continuous() ~ &quot;{mean} ± {sd}&quot;) %&gt;% add_p(test = everything() ~ t.test) %&gt;% add_stat(fns = everything() ~ CohenD, header = &quot;**ES (90% CI)**&quot;) %&gt;% add_stat(fns = everything() ~ CohenD_interpret_d, header = &quot;**Interpretation**&quot;) ## 效应值计算函数方法2： ## 使用使用该add_stat()函数的方法：增加额外的函数或者置信区间：效应大小[90％CI] ## 关于效应大小的计算： 效应量可以解决P值无法刻画相关程度大小和差异大小的问题，也可以避免“P值操控”现象。 效应量衡量实验真实效果大小或者变量关联强度的指标, 它不受样本容量大小的影响 。 依据效应量的大小, 能够判断具有显著差异的研究结果是否具有实际意义或重要性。 P值代表的是统计学上的意义，而效应量是能反映实际上的意义，有时候即使有显著的统计学意义，但是效应量却可以很小。 library(tidyverse) library(gtsummary) my_EStest &lt;- function(data, variable, by, ...) { # Cohen&#39;s D d &lt;- effsize::cohen.d(data[[variable]] ~ as.factor(data[[by]]), conf.level=.90, pooled=TRUE, paired=FALSE, hedges.correction=TRUE) # Formatting statistic with CI est &lt;- style_sigfig(d$estimate) ci &lt;- style_sigfig(d$conf.int) %&gt;% paste(collapse = &quot;, &quot;) # returning estimate with CI together str_glue(&quot;{est} ({ci})&quot;) } add_ES &lt;- trial %&gt;% select(trt, age) %&gt;% tbl_summary(by = trt, missing = &quot;no&quot;, statistic = list(all_continuous() ~ &quot;{mean} ({sd})&quot;), digits = list(all_continuous() ~ c(1,1))) %&gt;% add_p(test = everything() ~ t.test) %&gt;% add_stat( fns = everything() ~ my_EStest, fmt_fun = NULL, header = &quot;**ES (90% CI)**&quot;) %&gt;% ## 添加尾注： modify_footnote(add_stat_1 ~ &quot;Cohen&#39;s D (90% CI)&quot;) 7.1.5.5.5 辅助/ 参数化 # 合并表格后：remove spanning headers modify_spanning_header(everything() ~ NA) # 控制试验组和安慰剂组的位置顺序： trial %&gt;% select(trt, age, marker) %&gt;% # mutate(trt = forcats::fct_rev(trt)) %&gt;% # reverse the ordering of trt variable tbl_summary(by = trt) ## 两种合并表格的方法： ## 横向合并表格： table_11 &lt;- tbl_merge(tbls = list(a, b, c), tab_spanner = c(&quot;**非内分泌科就诊**&quot;, &quot;**内分泌科就诊**&quot;, &quot;**总共**&quot;) ) ## 纵向合并表格： tbl_stack(list(tbl1, tbl2), group_header = c(&quot;versicolor&quot;, &quot;virginica&quot;)) ## gtsummary自动化参数命名： 使用attr(.,&quot;label&quot;)给参数变量命名，然后gtsummary会优先从列的label中读取描述。 ## 控制列宽： library(gt) library(gtExtras) library(dplyr) cols_fn &lt;- function(data, y) { data %&gt;% select(1:4) %&gt;% gt() %&gt;% gt_theme_espn() %&gt;% cols_width(as.formula(glue::glue(&quot;4~px({y})&quot;)))} cols_fn(head(mtcars), y = 900) ## gtsmmary的主题设计： https://cran.r-project.org/web/packages/gtsummary/vignettes/themes.html 7.1.5.6 finalfit(简易高效) library(finalfit) dependent &lt;- &quot;ulcer.factor&quot; explanatory &lt;- c(&quot;age&quot;, &quot;sex.factor&quot;, &quot;year&quot;, &quot;t_stage.factor&quot;) melanoma %&gt;% ## 自动对分组变量和连续变量求p值，类似于tableone; summary_factorlist(dependent, explanatory, p = TRUE, add_dependent_label = TRUE) 7.1.6 统计结果表格可视化 7.1.6.1 sjplot() ## 参见：https://strengejacke.github.io/sjPlot/index.html sjPlot - Data Visualization for Statistics in Social Science # install.packages(&quot;sjPlot&quot;) #### ## 支持类型出表： # 经典回归、贝叶斯回归、混合效应回归； ## 支持类型出图： ## 支持回归系数、边际效应和交互式出图； ## 支持表格和图形的论文化出图： ## 支持灰度图： 使用colors = &quot;gs&quot;，展示数据灰度； 使用colors = &quot;bw&quot;，展示数据线条类型； ## 支持论文发表为主题的自定义表格输出图 # 可以在一定程度上优化现有的ggplot2体系； # 比调整ggplot2的代码更便捷； # load package library(sjPlot) library(sjmisc) library(sjlabelled) # sample data data(&quot;efc&quot;) efc &lt;- as_factor(efc, c161sex, c172code) m1 &lt;- lm(barthtot ~ c160age + c12hour + c161sex + c172code, data = efc) m2 &lt;- lm(neg_c_7 ~ c160age + c12hour + c161sex + e17age, data = efc) ## 自动聚类分组出统计表：太牛皮了吧。。。 tab_model(m1) tab_model(m1, m2) ## 多组统计表 ## 支持各种统计过程汇总参数再导出： ## 添加se/sts tab_model(m1, show.se = TRUE, show.std = TRUE, show.stat = TRUE) ## 关闭p值等操作； tab_model(m3, m4, show.ci = FALSE, show.p = FALSE, auto.label = FALSE) 7.1.6.2 mtable # we will define a fake model which includes all the IVs mtables &lt;- mtable(&quot;Model fake&quot; = lm(sr ~ dpi + ddpi + log(dpi) + log(ddpi), LifeCycleSavings), &quot;Model 1&quot; = lm(sr ~ dpi + pop15 + pop75, LifeCycleSavings), &quot;Model 2&quot; = lm(sr ~ ddpi + pop15 + pop75, LifeCycleSavings), &quot;Model 3&quot; = lm(sr ~ log(dpi) + pop15 + pop75, LifeCycleSavings), &quot;Model 4&quot; = lm(sr ~ log(ddpi) + pop15 + pop75, LifeCycleSavings)) # we will display mtables without the fake model mtables[2:5] # output Calls: Model 1: lm(formula = sr ~ dpi + pop15 + pop75, data = LifeCycleSavings) Model 2: lm(formula = sr ~ ddpi + pop15 + pop75, data = LifeCycleSavings) Model 3: lm(formula = sr ~ log(dpi) + pop15 + pop75, data = LifeCycleSavings) Model 4: lm(formula = sr ~ log(ddpi) + pop15 + pop75, data = LifeCycleSavings) ===================================================================== Model 1 Model 2 Model 3 Model 4 --------------------------------------------------------------------- (Intercept) 31.457*** 28.125*** 36.304** 26.118*** (7.482) (7.184) (10.511) (7.416) dpi -0.001 (0.001) ddpi 0.428* (0.188) log(dpi) -0.779 (1.018) log(ddpi) 1.584* (0.722) pop15 -0.492** -0.452** -0.506** -0.408** (0.149) (0.141) (0.154) (0.144) pop75 -1.568 -1.835 -1.649 -1.663 (1.121) (0.998) (1.110) (1.009) --------------------------------------------------------------------- R-squared 0.274 0.337 0.271 0.332 adj. R-squared 0.227 0.293 0.223 0.288 sigma 3.939 3.767 3.948 3.780 F 5.797 7.778 5.700 7.608 p 0.002 0.000 0.002 0.000 Log-likelihood -137.410 -135.171 -137.525 -135.355 Deviance 713.767 652.606 717.054 657.424 AIC 284.821 280.341 285.050 280.709 BIC 294.381 289.902 294.610 290.269 N 50 50 50 50 ===================================================================== 7.1.6.3 stargazer # stargazer can be an option library(stargazer) stargazer(lm(sr ~ dpi + pop15 + pop75, LifeCycleSavings), lm(sr ~ ddpi + pop15 + pop75, LifeCycleSavings), lm(sr ~ log(dpi) + pop15 + pop75, LifeCycleSavings), lm(sr ~ log(ddpi) + pop15 + pop75, LifeCycleSavings), type = &quot;text&quot;, column.labels = c(&quot;Model 1&quot;, &quot;Model 2&quot;, &quot;Model 3&quot;, &quot;Model 4&quot;), model.numbers = FALSE) # output ===================================================================== Dependent variable: --------------------------------------- sr Model 1 Model 2 Model 3 Model 4 --------------------------------------------------------------------- dpi -0.001 (0.001) ddpi 0.428** (0.188) log(dpi) -0.779 (1.018) log(ddpi) 1.584** (0.722) pop15 -0.492*** -0.452*** -0.506*** -0.408*** (0.149) (0.141) (0.154) (0.144) pop75 -1.568 -1.835* -1.649 -1.663 (1.121) (0.998) (1.110) (1.009) Constant 31.457*** 28.125*** 36.304*** 26.118*** (7.482) (7.184) (10.511) (7.416) --------------------------------------------------------------------- Observations 50 50 50 50 R2 0.274 0.337 0.271 0.332 Adjusted R2 0.227 0.293 0.223 0.288 Residual Std. Error (df = 46) 3.939 3.767 3.948 3.780 F Statistic (df = 3; 46) 5.797*** 7.778*** 5.700*** 7.608*** ===================================================================== Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 7.1.7 补充表格包 7.1.7.1 CROSS-TABLE() # 不太好用，暂存： library(crosstable) # 参见网址：https://cran.r-project.org/web/packages/crosstable/vignettes/crosstable.html # 需要指定分类变量后再批量转为表格导出： crosstable(mtcars2, c(mpg, cyl), by=am) %&gt;% as_flextable(keep_id=TRUE) 7.1.7.2 tableone library(tableone) install.packages(&quot;tableone&quot;) tab2 &lt;- CreateTableOne(data=iris) print(tab2) ## 返回(mean (SD)) 7.2 图形可视化 7.2.1 画图参考网站 https://r-graph-gallery.com/ https://chart-studio.plotly.com/feed/?q=plottype:violin#/ ## 配色： https://c.runoob.com/front-end/55/ ## 动态地图可视化： https://rstudio.github.io/leaflet/markers.html ## 其他类型的可视化方案： https://vultr.ywbb.com/4186.html 7.2.2 免代码绘制 7.2.2.1 esquisse包 免代码ggplot 生成器，鼠标点点点就能在R-studio里面画图。 类似BI工具，支持代码导出： esquisse::esquisser() ## 有局限性； 7.2.2.2 ggThemeAssist包 调节ggplot对象的神器, 免代码调节ggplot的细节参数，比如字体，背景颜色等 library(ggplot2) library(ggThemeAssist) 使用mtcars生成一个点图示例 注意这里必须指定定义为gg输出后，再输入； ```gg &lt;- ggplot(mtcars, aes(x = hp, y = mpg, colour = as.factor(cyl))) + geom_point() ``` 开始调整主题 **ggThemeAssistGadget(gg)** ##这个更好用一些 7.2.2.3 ggplotgui包 免代码绘图，导入数据，根据已有类型图绘图！ 提供R包包括： 仅实现了广泛使用的图表：箱线图、密度图、点 + 误差图、点图、直方图、散点图和小提# 琴图。只能更改选定数量的美学特征。并且支持对应代码的导出； install.packages(&quot;devtools&quot;) devtools::install_github(&quot;gertstulp/ggplotgui&quot;) library(&quot;ggplotgui&quot;) ggplot_shiny() ggplot_shiny(mpg) 7.2.3 统计辅助绘图 7.2.3.1 smplot/trelliscopejs ### 可视化优化R 包 -- smplot #### install.packages(&#39;devtools&#39;) devtools::install_github(&#39;smin95/smplot&#39;) ## 参见；https://smin95.github.io/dataviz/manual-and-examples-of-smplot.html library(smplot) ## 简单来说，这个程序优化了以往写ggplot2的不便之处， ## 加强了统计可视化的便利性，以及增加了画图的易用性： ## 改变主题： p1 + sm_corr_theme() ## 去除边框和图例： p2 &lt;- p1 + sm_corr_theme(borders = FALSE, legends = FALSE) p2 ## 改变颜色： p2 + scale_color_manual(values = sm_palette(7)) ## 相关图：-添加R和P值； p1 + sm_corr_theme() + sm_statCorr(color = sm_color(&#39;green&#39;)) ## 箱线图+散点图： p1 + sm_corr_theme() + sm_statCorr(color = sm_color(&#39;green&#39;)) 7.2.3.2 可视化优化facet_trelliscope 优化分图显示太多造成的困扰，使用滑窗展示的方法： library(trelliscopejs) library(ggplot2) library(gapminder) ## 参见；https://cran.r-project.org/web/packages/trelliscopejs/vignettes/trelliscopejs.html ## 支持行列数量控制、标签控制、筛选控制和排序控制； ## 作为参数参与ggplot2的相关下游可视化： qplot(year, lifeExp, data = gapminder) + xlim(1948, 2011) + ylim(10, 95) + theme_bw() + facet_trelliscope(~ country + continent, nrow = 2, ncol = 7, width = 300) ## 作为管道，直接接受上游ggplot2的图层，再优化： library(trelliscopejs) country_plot &lt;- function(data, model) { plot_ly(data = data, x = ~year, y = ~lifeExp, type = &quot;scatter&quot;, mode = &quot;markers&quot;, name = &quot;data&quot;) %&gt;% add_trace(data = data, x = ~year, y = ~predict(model), mode = &quot;lines&quot;, name = &quot;lm&quot;) %&gt;% layout( xaxis = list(range = c(1948, 2011)), yaxis = list(range = c(10, 95)), showlegend = FALSE) } ## 需要注意这里使用purrr包对应的map_系列的绘图函数； by_country &lt;- by_country %&gt;% mutate(data_plot = map2_plot(data, model, country_plot)) ## 然后输入后进行管道运算优化： by_country %&gt;% arrange(-resid_mad) %&gt;% trelliscope(name = &quot;by_country_lm&quot;, nrow = 2, ncol = 4) 7.2.3.3 ggpubr ## 描述：一次绘制一个或一组变量。 内容： 基因表达数据 箱线图 小提琴情节 条形图和点图 密度图 直方图 经验累积密度函数 分位数 - 分位数图 将 P 值和显着性水平添加到 ggplots ## 描述：计算并自动将 p 值和显着性水平添加到 ggplots。 内容： 比较均值的方法 R函数添加p值 比较均值（） stat_compare_means() 比较两个独立的组 比较两个配对样本 比较两个以上的组 多个分组变量 具有相关性和边际直方图的完美散点图 ## 描述：使用相关系数和边际直方图/密度创建漂亮的散点图。 内容： 基本地块 按组着色 添加浓度椭圆 添加点标签 气泡图 按连续变量着色 添加边际图 添加二维密度估计 应用于基因表达数据 绘制均值/中位数和误差线 描述：用误差线轻松绘制均值或中位数。 内容： 误差图 线图 条形图 添加标签 应用于基因表达数据 条形图和现代替代方案 描述：轻松创建基本和有序的条形图，以及条形图的一些现代替代品，包括棒棒糖图和克利夫兰的点图。 内容： 基本条形图 多个分组变量 有序条形图 偏差图 条形图的替代方案 棒棒糖图表 克利夫兰的散点图 将文本标签添加到直方图和密度图 ## 描述：创建直方图/密度图并突出显示图中的一些关键元素。 ggplot2 - 在同一页面上混合多个图形的简单方法 描述：在同一页面上以及在多个页面上组合多个 ggplot 的分步指南。 内容： 创建一些图 在一页上排列 注释排列的图形 对齐绘图面板 更改绘图的列/行跨度 为组合 ggplots 使用公共图例 带有边际密度图的散点图 混合表格、文本和 ggplot2 图表 在 ggplot 中插入图形元素 在 ggplot 中放置一个表格 在 ggplot 中放置一个箱形图 将背景图像添加到 ggplot2 图形 排列多个页面 使用 ggarrange() 的嵌套布局 导出地块 ggplot2 - 更改图形参数的简便方法 # 描述：描述函数 ggpar() [in ggpubr]，可用于简单轻松地自定义任何基于 ggplot2 的图形。 内容: 更改标题和轴标签 更改图例位置和外观 更改调色板 组颜色 渐变色 更改轴限制和比例 自定义轴文本和刻度 旋转绘图 更改主题 移除 ggplot 组件 创建和自定义多面板 ggplots：Facet 简单指南 # 描述：通过一个或多个变量拆分您的数据，并将数据的子集可视化在一起。 内容： 按一个分组变量分面 由两个分组变量分面 修改面板标签外观 # 参见网页： https://rpkgs.datanovia.com/ggpubr/ ## ggpubr全参数详解： http://www.sthda.com/english/articles/24-ggpubr-publication-ready-plots/ ## 密度图： ggdensity(wdata, x = &quot;weight&quot;, add = &quot;mean&quot;, rug = TRUE, color = &quot;sex&quot;, fill = &quot;sex&quot;, palette = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;)) ## 柱状图： gghistogram(wdata, x = &quot;weight&quot;, add = &quot;mean&quot;, rug = TRUE, color = &quot;sex&quot;, fill = &quot;sex&quot;, palette = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;)) ## 箱线图病添加统计p值： ggboxplot(df, x = &quot;dose&quot;, y = &quot;len&quot;, color = &quot;dose&quot;, palette =c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), add = &quot;jitter&quot;, shape = &quot;dose&quot;) my_comparisons &lt;- list( c(&quot;0.5&quot;, &quot;1&quot;), c(&quot;1&quot;, &quot;2&quot;), c(&quot;0.5&quot;, &quot;2&quot;) ) p + stat_compare_means(comparisons = my_comparisons)+ # Add pairwise comparisons p-value stat_compare_means(label.y = 50) ## 小提琴图 ggviolin(df, x = &quot;dose&quot;, y = &quot;len&quot;, fill = &quot;dose&quot;, palette = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), add = &quot;boxplot&quot;, add.params = list(fill = &quot;white&quot;))+ stat_compare_means(comparisons = my_comparisons, label = &quot;p.signif&quot;)+ # Add significance levels stat_compare_means(label.y = 50) ## 条线图： ggbarplot(dfm, x = &quot;name&quot;, y = &quot;mpg&quot;, fill = &quot;cyl&quot;, # change fill color by cyl color = &quot;white&quot;, # Set bar border colors to white palette = &quot;jco&quot;, # jco journal color palett. see ?ggpar sort.val = &quot;desc&quot;, # Sort the value in dscending order sort.by.groups = FALSE, # Don&#39;t sort inside each group x.text.angle = 90 # Rotate vertically x axis texts ) ## 点线图： ggdotchart(dfm, x = &quot;name&quot;, y = &quot;mpg&quot;, color = &quot;cyl&quot;, # Color by groups palette = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), # Custom color palette sorting = &quot;ascending&quot;, # Sort value in descending order add = &quot;segments&quot;, # Add segments from y = 0 to dots ggtheme = theme_pubr() # ggplot2 theme ) 7.2.4 base-plot()/ggplot2() 7.2.4.1 符号和线条 ## base-R: pch指定图形（在type=”p”/”o”/”b”时）,可加cex参数，用来控制图中的符号大小，默认为cex=1, lty指定线形,lwd改变线条粗细; type指定线性的类型： “p”点图 、 “l”线图 、 “b”点线图，线不穿过点 、“c”虚线图 、“o”点线图，线穿过点 “h”直方图、 “s”阶梯图、 “S”步骤图 、“n”无图 ## 绘制： x&lt;-c(1:10) png(&quot;~/plotSamples.png&quot;,width=9,height=9,unit=&quot;in&quot;,res=108) #在工作目录下创建plotSamples.png图 par(mfcol=c(3,3)) plot(x,type=&quot;p&quot;,main=&quot;p&quot;) dev.off() 7.2.4.2 颜色参数： 7.2.4.2.1 颜色参数配置： ## base-R col：默认绘图颜色。某些函数(如lines、pie)可以接受一个含有颜色值的向量，并自动循环使用。 例如：col=c(&quot;red&quot;, &quot;blue&quot;)需要绘制三条线，那么三条颜色分别为red、blue、red col.axis：坐标轴刻度文字的颜色，不是坐标轴的颜色 col.lab：坐标轴标签(名称)的颜色 col.main：标题的颜色 col.sub：副标题的颜色 fg：图形的前景色 bg：图形的背景色 col指定图形颜色 colors()方法可以查看R中所有可用的颜色名，一共有657种颜色名，根据颜色名可直接设置图形的显示颜色。下面是部分颜色，完整的图见链接：R语言颜色表 除了名称外，同样可以用下标，十六进制颜色值，RGB值和HSV值来指定颜色。例子：col=1、col=&quot;white&quot;、col=&quot;#FFFFFF&quot;、col=rgb(1,1,1)和col=hsv(0,0,1)。 另外，R中还有许多生成颜色变量的函数。有rainbow()、heat.colors()、terrain.colors()、topo.colors()、cm.colors()方法，gray()方法生成多阶灰度色。 7.2.4.2.2 颜色梯度： # 设置 # 方法1： col = grey(1:100/100) # 方法2： ttt &lt;- colorRampPalette(colors = c(&#39;black&#39;, &#39;white&#39;))( 100 ) plot(env_bgExt_na,col=ttt ) ## 00:全透明 ## FF:不透明 ### 蓝色：#0000FF7F ### 黄色：#FFFF008c ### 绿色：#00FF0080 ## 透明色：#FFFFFF0 浅黄色：khaki 深黄色：yellow 深蓝色：&quot;lightskyblue&quot; 浅蓝色：&quot;slateblue1&quot; 浅绿色：seagreen1 亮绿色：chartreuse 浅红色：pink 深红色：red ## 参见网站：https://c.runoob.com/front-end/55 透明度 16进制值 00% FF（不透明） 5% F2 10% E5 15% D8 20% CC 25% BF 30% B2 35% A5 40% 99 45% 8c 50% 7F 55% 72 60% 66 65% 59 70% 4c 75% 3F 80% 33 85% 21 90% 19 95% 0c 100% 00（全透明 ncores = 30 ## 参见网站： https://www.cnblogs.com/summary-2017/p/7504126.html ## 写16进制代码的方式是将颜色加入到前面，最后再加透明度即可！！ 7.2.4.3 4.3 文本配置 ## base-R: ## 配置全局的文字： windowsFonts(myFont=windowsFont(&quot;华文中宋&quot;)) ## 文本属性(用来指定字号、字体、字样) cex.axis：坐标轴刻度文字的缩放倍数 cex.lab：坐标轴标签(名称)的缩放倍数 cex.main：标题的缩放倍数 cex.sub：副标题的缩放倍数 font：整数。用于指定字体样式。1常规、2粗体、3斜体、4粗斜体 ## 7.2.4.4 图形尺寸与边界 ## base -R pin：以英寸表示图形的宽和高 mai：以数值向量表示边界大小，顺序为&quot;下、左、上、右&quot;，单位为英寸 mar：以数值向量表示边界大小，顺序为&quot;下、左、上、右&quot;，单位为英分，默认值c(5, 4, 4, 2)+0.1 7.2.4.5 坐标轴： ## base -R: plot参数 axes=FALSE 将禁用全部坐标轴，框架和刻度全部没有了 xaxt=&quot;n&quot; 禁用x轴的刻度线 yaxt=&quot;n&quot; 禁用y轴的刻度线 也可以通过axis函数自定义axis(……) side：一个整数。表示在图形的哪边绘制坐标轴（1=下，2=左，3=上，4=右） at：一个数值向量，表示需要绘制刻度线的位置 labels：一个字符型向量(也可以是数值型)，表示刻度线旁边的文字标签(刻度值)，如果整个不写，则直接使用at的值 col：线条和刻度的颜色 lty：线条类型 las：标签的字体是否平行(=0)或者垂直(=2)坐标轴 tck：刻度线的长度(默认值-0.01，负值表示刻度在图形外，正值表示刻度在图形内侧) ## GGPLOT2: # 笛卡尔坐标：从来看coord_cartesian的参数相对比较简单，x和y的数据限定范围 coord_cartesian(xlim = NULL, ylim = NULL) # 横向转换坐标：把x轴和y轴互换，没有特殊参数 coord_flip(...) # 坐标形式转换：包括对数转换，平方根转换等，这里x和y 的值可以是log10,log2或squal等，另外两个参数也是限定坐标范围 coord_trans(x = &quot;identity&quot;, y = &quot;identity&quot;, limx = NULL, limy = NULL) # 等坐标转换：使用这个函数后，x轴和y轴坐标会被转换成相等形式，此时图形会产生较大的缩放，radio可以进一步调整缩放比例（x和y的比值） coord_equal(ratio=1, ...) # 极坐标转换：可以做出蜘蛛图或饼图的效果，参数方面theta 可以选择x或y，表示外延的坐标，start是坐标开始的角度，默认其实位置是12点钟， coord_polar(theta = &quot;x&quot;, start = 0, direction = 1) direction 表示数据的方向，1是顺时针，-1为逆时针。 7.2.4.6 参考线/辅助线： ## base -R: abline(h=yvalues, v=xvalues) 例如：plot(1:10) abline(h = c(1, 5)) #则在y=1和5处各有一条水平线 abline(v = c(1, 5)) #则在x=1和5处各有一条垂直线 contour(elev_sa, nlevels=7, levels=c(0, 500, 1000, 1500, 2000, 3000, 4000, 5000), add=T, labels=““, lwd=0.2) ## GGPLOT2: geom_vline(xintercept = 0, color = &#39;gray&#39;, size = 0.4) + geom_hline(yintercept = 0, color = &#39;gray&#39;, size = 0.4) 7.2.4.7 标题配置： # base-R title(main = &quot; &quot;, sub = &quot; &quot;, xlab = &quot; &quot;, ylab = &quot; &quot;) 也可以直接把title里面的参数直接放在plot()里面 7.2.4.8 图例设置： ## base-R: # 通过legend函数我们可以添加图例。 x &lt;- seq(-pi, pi, len = 65) plot(x, sin(x), type = &quot;l&quot;, ylim = c(-1.2, 1.8), col = 3, lty = 2) points(x, cos(x), pch = 3, col = 4) lines(x, tan(x), type = &quot;b&quot;, lty = 1, pch = 4, col = 6) title(&quot;legend(..., lty = c(2, -1, 1), pch = c(NA, 3, 4), merge = TRUE)&quot;, cex.main = 1.1) legend(-1, 1.9, c(&quot;sin&quot;, &quot;cos&quot;, &quot;tan&quot;), col = c(3, 4, 6), text.col = &quot;green4&quot;, lty = c(2, -1, 1), pch = c(NA, 3, 4), merge = TRUE, bg = &quot;gray90&quot;) legend(&quot;bottom&quot;, legend = levels(iris$Species), col = c(&quot;#999999&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;), pch = c(16, 17, 18), inset = -0.25, xpd = TRUE, horiz = TRUE) 7.2.4.9 图形边距设置： ## base-R par函数可以用来设置图形参数 par函数设置的参数默认值 ## 调整图片间的间距： par(mfcol=c(2,1),mar=c(1,4,1,1),oma=c(2,2,2,2)) ## 参见： https://www.cnblogs.com/mmtinfo/p/12106309.html ## 解决plot.new() : figure margins too large op &lt;- par(mar = rep(0, 4)) plot.new() par(op) 7.2.4.10 添加额外图层： library(ggplot2) library(cowplot) p &lt;- ggplot(mpg, aes(displ, cty)) + geom_point() + theme_minimal_grid() cowplot::ggdraw(p) + draw_label(&quot;Draft&quot;, colour = &quot;#80404080&quot;, size = 120, angle = 45) 7.2.5 绘图 7.2.5.1 绘制组图 ## base-R: ## 借助par扩大画幅： par(mar=c(2,2,2,2), mfrow=c(1,3), oma=c(0,0,2,0)) plot(nndist(ppp.random), main=&#39;independant&#39;, pch=16, cex=0.8) plot(nndist(ppp.regular), main=&#39;regular&#39;, pch=16, cex=0.8) plot(nndist(ppp.locations), main=&#39;clustered&#39;, pch=16, cex=0.8) mtext(&quot;Nearest Neighbor Distances&quot;, outer = TRUE, cex = 1.5) ## ggplot2系列： library(ggtree) multiplot(p1, revts(p1), ncol = 2) ## 多图-相似图例： library(rasterVis) levelplot(all2,layout=c(4, 4),col.regions=cols, main=&quot;HTHI_FOREEST&quot;) ## 另外的组图高级R包： ## cowplot和patchwork 7.2.5.2 导出图形 ## base-R: png(&quot;./fut_hthi_pro/BIOSNOW2.png&quot;,width =3200, height = 2000,res =300) plot(raster(all[1]),col = cols) dev.off() ## 配置图片参数： png(&#39;plot1.png&#39;,height = 20,width = 20,units = &#39;cm&#39;,res = 800) print(p1_cor) dev.off() ## tiff(file = &quot;./f3_论文出图/try4.tiff&quot;, res = 600, width = 2000, height =2500, compression = &quot;lzw&quot;) ## GGPLOT2风格： ## 批量出图： ## 使用ggpubr::ggexport()导出成pdf文件 library(ggpubr) ##一张图一页； ggexport(plotlist = list(scree.plot, ind.plot, var.plot), filename = &quot;PCA.pdf&quot;) ## 一张图多页； ggexport(plotlist = list(scree.plot, ind.plot, var.plot), nrow = 2, ncol = 2, filename = &quot;PCA.pdf&quot;) ##将图表导出到png文件。如果指定绘图列表，则将自动创建多个png文件以保存每个绘图 ggexport(plotlist = list(scree.plot, ind.plot, var.plot), filename = &quot;PCA.png&quot;) ## 批量导出图片到pdf格式： ff &lt;- list(bio11,BIO16) library(cowplot) dir.create(&quot;./f3_论文出图&quot;) pdf(&quot;./f3_论文出图/boxplot4.pdf&quot;) # tiff(file = &quot;./f3_论文出图/boxplot.tiff&quot;, res = 300, width = 2000, height =6000, compression = &quot;lzw&quot;) ggdraw(plot_grid(plot_grid(plotlist = ff,ncol=1,nrow=2, align=&#39;v&#39;), # plot_grid(NULL, legend,ncol=2), rel_widths=c(1, 0.2))) dev.off() ### GGSAVE(): ## 保存图片：使用ggsvae： ggsave(filename =&quot;ggplot2_point.png&quot;,plot =p1, width=8.9,height= 6,units=&quot;cm&quot;,dpi=600) ggsave(filename = &quot;ggolot2-point.pdf&quot;,plot= p1,device=&quot;pdf&quot;, width=8.9,height= 6,units=&quot;cm&quot;,dpi=600) ## 保存为jpg格式： ggsave(&quot;p1.jpg&quot;,width=6,height =5) jpeg(&quot;p1.jpg&quot;,width =1200,height = 2000,units =&quot;px&quot;) p1 dev.off() 7.2.6 统计图形代码 7.2.6.1 线性模型 7.2.6.1.1 一般线性模型可视化base-R： ## base-R: plot(data$height, data$weight) abline(model, col = &#39;red&#39;) legend(&quot;topleft&quot;, legend = c(&#39;data points&#39;, &#39;regression line&#39;), cex = 0.7, col = c(&#39;black&#39;, &#39;red&#39;), lwd = c(NA, 1), pch = c(1, NA)) ## GGPLOT2 - 标记R2和p： #读取鱼类物种丰度和水体环境数据 dat &lt;- read.delim(&#39;fish_data.txt&#39;, sep = &#39;\\t&#39;, row.names = 1) #绘制二维散点图观测各环境变量与鱼类物种丰度的关系 library(ggplot2) dat_plot &lt;- reshape2::melt(dat, id = &#39;fish&#39;) p &lt;- ggplot(dat_plot, aes(value, fish)) + geom_point() + facet_wrap(~variable, ncol = 3, scale = &#39;free&#39;) + geom_smooth(method = &#39;lm&#39;) p #拟合各环境变量与鱼类物种丰度的一元回归，并提取各自 R2 和 p 值 env &lt;- c(&#39;acre&#39;, &#39;do2&#39;, &#39;depth&#39;, &#39;no3&#39;, &#39;so4&#39;, &#39;temp&#39;) R2_adj &lt;- c() p_value &lt;- c() for (i in env) { fit_stat &lt;- summary(lm(dat[[&#39;fish&#39;]]~dat[[i]])) #一元线性回归 R2_adj &lt;- c(R2_adj, fit_stat$adj.r.squared) #提取校正后 R2 p_value &lt;- c(p_value, fit_stat$coefficients[2,4]) #提取显著性 p 值 } env_stat &lt;- data.frame(row.names = env, R2_adj, p_value) env_stat #数据框中存储了各环境变量与鱼类物种丰度的一元回归的 R2 和 p 值 #在散点图中添加各环境变量与鱼类物种丰度的一元回归的 R2 和 p 值作为标识 #注：该 R2 和 p 值仅为单个变量一元回归的 R2 和 p 值，和下文即将提到的多元回归的 R2 和 p 值存在区别 env_stat$fish &lt;- max(dat$fish) * 0.8 for (i in env) env_stat[i,&#39;value&#39;] &lt;- max(dat[[i]]) * 0.8 #这句和上一句，定义文字在图中的展示坐标，这里仅粗略设置下 env_stat$variable &lt;- rownames(env_stat) env_stat$label &lt;- paste(&#39;R2.adj =&#39;, round(env_stat$R2_adj, 3), &#39;\\np =&#39;, round(env_stat$p_value, 3)) #文字标签 env_stat &lt;- env_stat[c(&#39;fish&#39;, &#39;variable&#39;, &#39;value&#39;, &#39;label&#39;)] dat_plot$label &lt;- NA dat_plot &lt;- rbind(dat_plot, env_stat) #和先前的数据框合并，便于作图 p + geom_text(data = dat_plot, aes(label = label), size = 3) 7.2.6.1.2 一般线性模型可视化ggplot2-R： label_df = data.frame( x = c(-0.5,-0.5,-0.5), y = c(0.3,0.25,0.2), label = c(paste0(&quot;COR = &quot;, round(cor(train_simu,train$shortwave),2)), paste0(&quot;p = &quot;, formatC(cor.test(train_simu,train$shortwave)$p.value)), paste0(&quot;RMSE = &quot;, round(rmse(train_simu,train$shortwave),3))) ) p1_cor = ggplot()+ geom_point(data = pl_point_df, aes(x = SIMU, y= TRAIN), size = 5, color = &#39;blue&#39;, shape = 1)+ geom_abline(intercept = 0,slope = 1,size = 1)+ geom_text(data = label_df,aes(x = x,y = y,label = label),size = 6,color = &#39;black&#39;, hjust = 0)+ theme_bw()+ theme( axis.text = element_text(face = &#39;bold&#39;,color = &#39;black&#39;,size = 12), axis.title = element_text(face = &#39;bold&#39;,color = &#39;black&#39;,size = 14, hjust = .5) )+ xlab(&#39;SIMULATION RESULT&#39;)+ ylab(&#39;REAL SHORTWAVE&#39;) ### 添加P值和R2: https://lukemiller.org/index.php/2012/10/adding-p-values-and-r-squared-values-to-a-plot-using-expression/ 7.2.6.1.3 多变量相关性图： library(GGally) ### 第一种形式： ggpairs(dat[which(names(dat) != &#39;fish&#39;)]) ### 第二种形式： ggcorr(mtcars[-c(5,7,9)], nbreaks = NULL, label = TRUE, low = &#39;red3&#39;, high = &#39;green3&#39;, label_round = 2, name = &#39;Correlation Scale&#39;, label_alpha = TRUE, hjust = 0.75) + ggtitle(label = &#39;Correlation Plot&#39;) + theme(plot.titl = element_text(hjust = 0.6)) ## 第三种形式 library(lattice) splom(df) 7.2.6.1.4 三维散点图： ## 可视化数据框： library(scatterplot3d) scatter.3d &lt;- with(df, scatterplot3d(Girth, Height, Volume, pch = 16, highlight.3d = TRUE, angle = 60)) scatter.3d$plane3d(model2) 7.2.6.2 分组统计图： 7.2.6.2.1 豆荚图 ## 学习制作豆荚图来优化数据的可视化流程： install.packages(&quot;beanplot&quot;) library(&quot;beanplot&quot;) # 创建三组数据，分别为双峰，均匀和正态分布 bimodal &lt;- c(rnorm(250, -2, 0.6), rnorm(250, 2, 0.6)) uniform &lt;- runif(500, -4, 4) normal &lt;- rnorm(500, 0, 1.5) # 制作豆荚图 beanplot(bimodal, uniform, normal, # 注明用于作图的三组数据 main = &quot;Beanplot&quot;, # 题目 col = c(&quot;#CAB2D6&quot;, &quot;#33A02C&quot;, &quot;#B2DF8A&quot;), # 修改颜色：分布，豆荚内横线，豆荚外横线 border = &quot;#CAB2D6&quot;) # 豆荚图的边缘颜色 ## 另外一种例子： data(&quot;singer&quot;, package = &quot;lattice&quot;) mydata &lt;- singer # 查看数据集 summary(mydata) beanplot(height ~ voice.part, # 可以使用熟悉的模式: y ~ x data = mydata, # 数据 ll = 0.05, # 改变豆荚内横线的长度 main = &quot;Beanplot&quot;, # 题目 ylab = &quot;Body height&quot;, # y轴标签 col = &quot;gray90&quot;) # 改变豆荚内的分布颜色 7.2.6.2.2 森林图 7.2.6.2.2.1 6.2.2.1 ggplot2 ### 森林图： forest_35 &lt;- glm(inpatient_time ~ fbg_diff + fbg_reg + age + sex + region + cad + hbp + stroke + ckd + group_1 + treatment_plan, family = &quot;gaussian&quot;, data = secondary5) %&gt;% tidy(conf.int = TRUE) %&gt;% mutate(across(where(is.numeric), round, digits = 2), #set order of levels to appear along y-axis term = fct_relevel(term, ...), term = recode(term, &quot;fbg_diff&quot; = &quot;空腹血糖变化差值&quot;, &quot;fbg_reg&quot; = &quot;末次空腹血糖&lt;6.1mmol/l&quot;, &quot;age&quot; = &quot;年龄&quot;, &quot;sex女&quot; = &quot;性别：女&quot;, # &quot;insurance_type医保类型缺失&quot; = &quot;医保类型缺失&quot;, # &quot;insurance_type有基本医疗保险&quot; = &quot;有基本医疗保险&quot;, &quot;region东部地区&quot; = &quot;东部地区&quot;, &quot;region西部地区&quot; = &quot;西部地区&quot;, &quot;cad&quot; = &quot;冠心病&quot;, &quot;hbp&quot; = &quot;高血压&quot;, &quot;stroke&quot; = &quot;卒中&quot;, &quot;ckd&quot; = &quot;慢性肾功能不全&quot;, # &quot;disease_num1&quot; = &quot;合并1种疾病&quot;, # &quot;disease_num2&quot; = &quot;合并2种疾病&quot;, # &quot;disease_num3&quot; = &quot;合并3种疾病&quot;, # &quot;disease_num4&quot; = &quot;合并4种疾病&quot;, &quot;group_1预混胰岛素组&quot; = &quot;预混胰岛素&quot;, &quot;treatment_plan二联治疗&quot; = &quot;二联治疗&quot;, &quot;treatment_plan三联治疗&quot; = &quot;三联治疗&quot;, &quot;treatment_plan胰岛素多次治疗&quot; = &quot;胰岛素多次治疗&quot;)) %&gt;% filter(term != &quot;(Intercept)&quot;) %&gt;% ## plot with variable on the y axis and estimate (OR) on the x axis ggplot(aes(x = estimate, y = term)) + ## show the estimate as a point geom_point() + ## add in an error bar for the confidence intervals geom_errorbar(aes(xmin = conf.low, xmax = conf.high)) + ## show where OR = 1 is for reference as a dashed line geom_vline(xintercept = 0, linetype = &quot;dashed&quot;) + ylab(&quot;变量&quot;) + xlab(&quot;Beta&quot;) + ggtitle(&quot;表35森林图&quot;) forest_35 7.2.6.2.2.2 6.2.1.2 森林表 library(survival) library(survminer) library(eoffice) #导出ppt用 # install.packages(&quot;eoffice&quot;) # 第一种方法： ----- # 以colon数据为示例数据 # 时间变量是time，结局变量是status，自变量选择 sex，rx和 adhere model &lt;- coxph( Surv(time, status) ~ sex + rx + adhere, data = colon ) # 森林图 ggforest(model, #coxph得到的Cox回归结果 data = colon, #数据集 main = &#39;Hazard ratio of colon&#39;, #标题 cpositions = c(0.05, 0.15, 0.35), #前三列距离 fontsize = 1, #字体大小 refLabel = &#39;reference&#39;, #相对变量的数值标签，也可改为1 noDigits = 3 #保留HR值以及95%CI的小数位数) # 第二种方法：----- # 安装R包 install.packages(&quot;devtools&quot;) devtools::install_github(&quot;ddsjoberg/bstfun&quot;) # 载入 library(&quot;gtsummary&quot;) # 表格制作 library(&quot;dplyr&quot;) # 数据处理 library(&quot;bstfun&quot;) # 画出森林图 # 建立线性回归模型 linear_model &lt;- lm(mpg ~ am + drat + wt + hp, data = mtcars) # 制作回归结果的表格 linear_model %&gt;% tbl_regression() linear_model %&gt;% tbl_regression() %&gt;% add_inline_forest_plot() ## 第三种方法：---- ## 参见：https://zhuanlan.zhihu.com/p/493258386 ## 参数配置：https://www.jianshu.com/p/7331c8f40d87 install.packages(&quot;forestplot&quot;) library(forestplot) result &lt;-read.table(&quot;clipboard&quot;, header = F, sep = &#39;\\t&#39;) View(result[,c(1:3,7:8)]) ## 核心构建思路是将图表中数据进行重构，搭建缺失数形式，然后其他数据自然出表， ## 而森林图组分使用mean/lower/upper来构建对应的参数形式： forestplot(result[,c(1:3,7:8)], # {1:3，7:8指Excel中的列号} mean=result[,4], lower=result[,5], upper=result[,6], zero=1, # {表示我们以坐标x轴=1为中心} boxsize=0.3, graph.pos = 4, hrzl_lines=list(&#39;1&#39;=gpar(lty=1,lwd=2), &#39;2&#39;=gpar(lty=1,lwd=2), &#39;24&#39;=gpar(lwd=2,lty=1)), # {绘制表格线，33表示所有行数+1} graphwidth=unit(.25,&#39;npc&#39;), xticks=c(0,1,2,3,4), # {此处定义横坐标，so就设了1,2,3,4} # is.summary=c(T, # T,F,F, # T,F,F, # T,F,F,F, # T,F,F, # T,F,F, # T,F,F,F,F, # T,F,F, # T,F,F, # T,F,F,F), #{T,F表示从第一行开始是否需要横杠，T表示不需要跑出} txt_gp=fpTxtGp(label = gpar(cex=1), ticks = gpar(cex=1.1), xlab = gpar(cex=1), title = gpar(cex=2)), xlab=&#39;Hazard ratio&#39;, lwd.zero=2, lwd.ci=2, lwd.xaxis=1, lty.ci=1, ci.vertices=T, ci.vertices.height=0.2, clip=c(0,4), ineheight=unit(8,&#39;mm&#39;), line.margin=unit(8,&#39;mm&#39;), colgap=unit(6,&#39;mm&#39;), col=fpColors(zero = &#39;black&#39;, box = &#39;black&#39;, lines = &#39;black&#39;), fn.ci_norm=&#39;fpDrawCircleCI&#39; ) fig 7.2.6.2.3 蜜蜂图： ## 方案1： library(beeswarm) data(breast) beeswarm(time_survival ~ ER, data = breast, pch = 16, pwcol = 1 + as.numeric(event_survival), xlab = &quot;&quot;, ylab = &quot;Follow-up time (months)&quot;, labels = c(&quot;ER neg&quot;, &quot;ER pos&quot;)) legend(&quot;topright&quot;, legend = c(&quot;Yes&quot;, &quot;No&quot;), title = &quot;Censored&quot;, pch = 16, col = 1:2) ## 方案2 ggplot(breast,aes(x=ER,y=time_survival))+ geom_beeswarm(aes(color=factor(event_survival)),cex=1.5)+#cex用于设置点的密集程度 theme_bw()+ theme( legend.position = c(&quot;top&quot;), panel.grid = element_blank() )+ scale_color_manual(values=c(&quot;Black&quot;,&quot;Red&quot;),name=&quot;Censored&quot;,labels=c(&quot;Yes&quot;,&quot;No&quot;))+ scale_x_discrete(labels=c(&quot;ER neg&quot;,&quot;ER pos&quot;))+ xlab(&quot;&quot;)+ ylab(&quot;Follow-up time (months)&quot;) 7.2.6.3 栅格密度图： ## 单个栅格： library(rastervis) levelplot(tmin1.c) ## 多个栅格： raster::pairs(clim_subset,maxpixels=1000 ## 使用par(mfrow =c(2,4)) ## 在给定范围内可视化： ## 绘图中extent的范围： plot(predictors[[1]], col=&quot;gray80&quot;, ext=c(-8, -3, 42, 44)) ## 地图可视化： #plot the map ggplot() + geom_polygon(data = G1, aes(x = long, y = lat, group = group), colour = &quot;grey10&quot;, fill = &quot;#fff7bc&quot;) + geom_point(data = dwd.data, aes(x = LON, y = LAT, colour = data.subset), alpha = .5, size = 2) + theme_bw() + xlab(&quot;Longitude&quot;) + ylab(&quot;Latitude&quot;) + coord_map() ## 高程叠加可视化：3D： ## 不能使用library(rayshader)这个包，需要更换： library(rasterVis) ## 添加投影坐标系下的栅格；前置高程，后置叠加栅格； plot3D(demms,drape= biox , adjust = FALSE) 7.2.6.4 交互式绘图 7.2.6.4.1 plotly::ggplotly() ## 使用 R、plotly 和闪亮的基于 Web 的交互式数据可视化 https://plotly-r.com/index.html ### 交互式绘图的方法，是将静态图传递给ggplotly() deaths_plot &lt;- ggplot(data = weekly_deaths)+ # begin with weekly deaths data geom_line(mapping = aes(x = epiweek, y = pct_death)) # make line deaths_plot # print deaths_plot %&gt;% plotly::ggplotly() ## 常用交互式绘图的三件套： metrics_plot %&gt;% plotly::ggplotly() %&gt;% ## 对应R包指令； plotly::partial_bundle() %&gt;% ## 抽取部分数据集展示； ### 调整绘图展示参数； plotly::config(displaylogo = FALSE, modeBarButtonsToRemove = plotly_buttons_remove) 7.2.6.4.2 ggvis-shiny 7.2.6.4.2.1 6.4.2.1 ggvis-shiny-动态图 ggvis VS ggplot2主要区别：http://ggvis.rstudio.com/ggplot2.html ggplot→ggvis geom→layer function stat→compute function aes→props +→%&gt;% ggvis目前不支持分面； 使用ggvis而不添加任何层类似于qplot。 library(ggvis) library(dplyr) mtcars %&gt;% ggvis(~wt, ~mpg) %&gt;% layer_points(fill = ~factor(cyl), size := 25, shape := &quot;diamond&quot;, stroke := &quot;red&quot;) %&gt;% group_by(cyl) %&gt;% layer_model_predictions(model = &quot;lm&quot;, se = TRUE) ## 提供shiny-Bar来绘制R图的修改参数： mtcars %&gt;% ggvis(~wt, ~mpg, size := input_slider(10, 100), opacity := input_slider(0, 1) ) %&gt;% layer_points() ## 鼠标查看： mtcars %&gt;% ggvis(~wt, ~mpg) %&gt;% layer_points() %&gt;% add_tooltip(function(df) df$wt) ## 其他调整图层的shiny参数： p %&gt;% layer_points(fill := &quot;red&quot;, stroke := &quot;black&quot;) p %&gt;% layer_points(size := 300, opacity := 0.4) p %&gt;% layer_points(shape := &quot;cross&quot;) ## 参见资源； https://ggvis.rstudio.com/interactivity.html 7.2.6.4.2.2 6.6.4.2.2 ggvis-构造shiny # ui.R library(shiny) # Define UI for miles per gallon application shinyUI(sidebarLayout( sidebarPanel( sliderInput(&quot;size&quot;, &quot;Area&quot;, 10, 1000, 500) ), mainPanel( uiOutput(&quot;ggvis_ui&quot;), ggvisOutput(&quot;ggvis&quot;) ) )) # server.R library(shiny) library(ggvis) library(dplyr) mpgData &lt;- mtcars mpgData$am &lt;- factor(mpgData$am, labels = c(&quot;Automatic&quot;, &quot;Manual&quot;)) # Define server logic required to plot various variables against mpg shinyServer(function(input, output) { input_size &lt;- reactive(input$size) mtcars %&gt;% ggvis(~disp, ~mpg, size := input_size) %&gt;% layer_points() %&gt;% bind_shiny(&quot;ggvis&quot;, &quot;ggvis_ui&quot;) }) 7.2.6.4.3 highcharter-高级交互式 # install.packages(&quot;highcharter&quot;) ## 参见网页：https://jkunst.com/highcharter/ 提供海量图层资源； ## 这个R包与ggplot2不同的地方在于使用管道夫进行添加图层； library(highcharter) library(tidyverse) #导入数据集 data&lt;-data.frame(index = c(1:7), value = c(1,2,5,3,7,6,8)) data #散点图1-普通散点图 data_point&lt;-data hchart(data_point,&quot;scatter&quot;,hcaes(x = &quot;index&quot;,y = &quot;value&quot;)) #散点图3-设置坐标轴标题 hchart(data_point,&quot;scatter&quot;,hcaes(x = &quot;index&quot;,y = &quot;value&quot;))%&gt;% hc_xAxis(title = list(text = &quot;x轴&quot;))%&gt;% hc_yAxis(title = list(text = &quot;y轴&quot;)) 7.2.7 医学分析图 7.2.7.1 jinseob2kim ## 数据资料： (https://github.com/jinseob2kim/jskm/commits?author=jinseob2kim) ## R包开发目的： ## 优化生存曲线展示： Kaplan-Meier Plot with ‘ggplot2’: ‘survfit’ and ‘svykm’ objects from ‘survival’ and ‘survey’ packages. 7.2.7.2 gg-km曲线 # Kaplan-Meier Survival Curve KM &lt;- ggsurvplot( fit, # 添加内容 pval = TRUE, conf.int = TRUE, risk.table = TRUE, risk.table.col = &quot;strata&quot;, linetype = &quot;strata&quot;, fun = &#39;event&#39;, # 主题设置 palette = c(&#39;#4f81bd&#39;,&#39;#c0504d&#39;), ggtheme = theme_bw(), # 横纵坐标设置 break.x.by = 1, xlim = c(0,6), ylim = c(0,0.3), # labs设置 legend.labs = c(&quot;control group&quot;,&quot;exposed group&quot;), xlab = &quot;Months&quot;, ylab = &quot;Cumulative incidence&quot;, title = &quot;Kaplan-Meier survival curve of *** (6 Months)&quot; ) KM$plot &lt;- KM$plot+ theme( legend.background = element_rect(fill = &quot;white&quot;, colour = &quot;black&quot;, size =0), legend.position = c(0.999,0.999), legend.justification = c(1,1)) KM # Log Rank p.val &lt;- 1-pchisq(fit$chisq,length(fit$n)-1) p.val 7.2.8 补充图形库 7.2.8.1 ggplot2的相关扩展 ## 参见：提供了ggplot2出图的大量扩展： https://exts.ggplot2.tidyverse.org/ggstance.html ## ggiraph --使得图形交互性： geom_bar_interactive geom_boxplot_interactive geom_histogram_interactive geom_line_interactive geom_map_interactive geom_path_interactive geom_point_interactive geom_polygon_interactive geom_rect_interactive geom_segment_interactive geom_text_interactive geom_tile_interactive ## ggstance: 使得ggplot2的图形倒置； ## ggalt 添加额外的坐标轴和设置； ## ggtech 、ggthemes ggplot2 技术主题、比例和几何。 ## gganimate -坑需要提供第一个对应的序列，比如时间序列； gganimate 包装动画包以创建动画 ggplot2 绘图。 ## ggradar -雷达图 有时候这个用于展示数据结构差异，还是挺有用的；或者汇总结果比较； ## ggridges包 绘制山脊图 ## ggsignif包 添加显著性统计P值 ## ggstatsplot -添加P值和R平方； 自动化绘图： "],["Common-statistical-R-code.html", "第 8 章 常用统计R代码 8.1 R-系统功能 8.2 R报错机制学习 8.3 常用快捷命令 8.4 正则表达式", " 第 8 章 常用统计R代码 8.1 R-系统功能 8.1.1 常用R系统代码 # 代码环境的配置信息： sessionInfo() ## 查看函数的参数： args(sample) ## 查看文件路径： Sys.getenv(&quot;PATH&quot;) ## 默认加载R包： 从R的安装目录bai的etc目录下找到duRprofile.site文件(例如R-2.15.0\\etc\\Rprofile.site)，用记事本打开，在这个文dao件最后添加一行： options(defaultPackages=c(getOption(&quot;defaultPackages&quot;), &quot;foreign&quot;, &quot;survival&quot;)) ## 查看系统时间： as.character(format(Sys.time(), &quot;%s&quot;)) ## 当前时间： Sys.Date() date() ## 全局参数配置： ## 关闭科学技术法： options(scipen=999) ## 设置全局小数点打印位数： options(scipen = 1, digits = 2) ## 全局中文字符修正测试; options(encoding = &#39;UTF-8&#39;) ## 修改全局语言-减少中文乱码的可能性： Sys.setlocale(&quot;LC_ALL&quot;,&quot;Chinese&quot;) ## 全局代码编写参数： # -*- coding: utf-8 -*- # 适用于一般脚本的抬头； ## 设置java运行的内存： options(java.parameters = &quot;-Xmx10g&quot; ) ## 局部参数配置： 强制保留3位小数： format(round(p.value*100)/100,nsmall=3) format(round(t1$p.value,digits =3),nsmall=3) 8.1.2 辅助R包： ##################### 高效安装R包： library(pacman) p_load(beepr,esquisse) ## 它会自动安装未安装的R包，并加载到环境中； ##################### 工作结束后提醒： library(beepr) beepr::beep(sound =8) ####################### 构建文件树 # 构建文件树路径： ## 方法1：中文不识别： fs::dir_tree(path = &quot;./&quot;, recurse = TRUE) ## 方法2：中文识别： install.packages(&quot;drat&quot;) drat::addRepo(&quot;stlarepo&quot;) install.packages(&quot;dir2json&quot;) library(dir2json) cat(dir2tree(&quot;./&quot;)) ## 中文不识别： dir2json::shinyDirTree(&quot;.&quot;) ## 方法3： 使用dos命令中的tree 工具，即tree /f &gt; 1.txt 复制到对应的剪切板中：tree | clip ###################### 代码注释器： ## 快速生成代码注释器： # install.packages(&quot;bannerCommenter&quot;) library(bannerCommenter) ## 注释代码： banner(&quot;hkaisydfiukasdjh&quot;) boxup(&quot;alkwsu&quot;, &quot;akijdsk&quot;) ################### 查看函数的查看内存和运行时间 library(data.table) library(dplyr) set.seed(123) tbl.test &lt;- data.table(x = rnorm(1e6)) bench::mark( fcase = tbl.test[, .(fcase(x &lt; 1, x + 1, rep_len(TRUE, length(x)), x))],case_when = tbl.test[, .(case_when(x &lt; 1 ~ x + 1, TRUE ~ x))]) #&gt; expression min median `itr/sec` mem_alloc `gc/sec` #&gt; &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt; &lt;dbl&gt; &lt;bch:byt&gt; &lt;dbl&gt; #&gt; 1 fcase 16.5ms 24.3ms 33.3 36.1MB 43.1 #&gt; 2 case_when 146.4ms 147.9ms 6.60 129.9MB 28.1 ###################### 整理脚本集中的代码： ## 方法1： install.packages(&quot;styler&quot;) ## 运行下面代码：1、代码保存成文件形式；2、代码无执行错误； styler::：style_active_file() ## 方法2： install_github(&quot;ROpenSci/Rclean&quot;) library(Rclean) script &lt;- system.file(&quot;example&quot;, &quot;long_script.R&quot;, package = &quot;Rclean&quot;) clean(script, &quot;fit_sqrt_A&quot;) 8.1.3 常用系统脚本 8.1.3.1 R置换系统R包迁移 #启动当前版本，输入以下命令 oldip &lt;- installed.packages()[,1] save(oldip, file=&quot;installedPackages.Rdata&quot;) #卸载旧版本 #下载安装新版本，启动新版本输入以下命令 load(&quot;installedPackages.Rdata&quot;) newip &lt;- installed.packages()[,1] for(i in setdiff(oldip, newip)) install.packages(i) 8.1.3.2 R与linux交互 if (!require(ape, quietly = TRUE)) install.packages(&#39;ape&#39;) library(ape) args &lt;- commandArgs(TRUE) if (length(args) &gt;3 &amp;&amp; length(args) &lt;2) { cat(&quot;usage: Rscript fasta2nexus.R dirname postfix filterout\\n&quot;) cat(&quot;filterout: joined with comma\\n&quot;) }else{ setwd(args[1]) if(length(args)==2) arg[3] &lt;- &quot;&quot; filter &lt;- unlist(strsplit(arg[3],&quot;,&quot;)) for (i in grep(paste0(args[2], &quot;$&quot;), value = TRUE, list.files())) { temp &lt;- read.FASTA(i) names(temp) &lt;- gsub(&quot;@.*&quot;, &quot;&quot;, names(temp)) temp &lt;- temp[setdiff(names(temp),filter)] write.nexus.data(temp, paste0(i, &quot;.nex&quot;)) } } 8.1.3.3 键盘和鼠标监听 ## 案例 1： 1. 利用R语言画图，根据键盘输入显示对应的文字。 2. 每按一下键盘，输出一个字母在屏幕上面。 3. 按ctrl+c时，停止键盘事件 # 字母工具 letter&lt;-function(){ # 画图函数 draw&lt;-function(label=&#39;&#39;,x=0,y=0){ plot(x,y,type=&#39;n&#39;) text(x,y,label=label,cex=5) } # 键盘事件 keydown&lt;-function(K){ if (K == &quot;ctrl-C&quot;) return(invisible(1)) print(K) draw(K) } # 画图 draw() # 注册键盘事件，启动监听 getGraphicsEvent(prompt=&quot;Letter Tool&quot;,onKeybd = keydown) } #启动程序 letter() 8.1.3.4 文件管理 8.1.3.4.1 3.4.1 常用文件管理： ## 修改文件夹的内的文件名： library(tidyverse) newname&lt;- list.files(&quot;E:/sjdata/F2_ENVS/aseutif/&quot;,pattern = &quot;tif&quot;) %&gt;% strsplit(.,&quot;.tif&quot;) nw &lt;- c() for(i in 1:42){ nw &lt;- c(nw,newname[[i]][1])} nw news &lt;- paste0(&quot;E:/sjdata/F2_ENVS/aseutif/&quot;,nw,&quot;.tif&quot;) file.rename(from = list.files(path=&quot;E:/sjdata/F2_ENVS/aseutif/&quot;,pattern = &quot;tif&quot;,full.names = TRUE), to = news) ## 查看文件大小： ### 查看数据集的大小： ## fs包： fs::file_size(file) https://cran.r-project.org/web/packages/fs/fs.pdf ## dir_info() 查看指定路径下文件的元数据信息； ## 使用dir重新替代list.files() list.files() 函数等价于 dir() 函数被替代了； dir ( &quot;../. .&quot; , pattern = &quot;^[a-lr]&quot; , full.names = TRUE , ignore.case = TRUE ) ## 构建文件树路径； fs::dir_tree(path = here(&quot;data&quot;), recurse = TRUE) ##### 关于here的用法补充 ##### ## 直接读取上层文件夹下的数据路径层： dir(here(&quot;data&quot;)) ## 注意here这里的data是针对的上层文件夹； fs::dir_tree(path = here(&quot;data&quot;), recurse = TRUE) ## here中连接的子文件夹中不需要再写对应的参数位置； # 如下所示，就是表示data文件夹下的aa文件夹；但感觉这个思路也很蠢！！！ fs::dir_tree(path = here(&quot;data&quot;,&quot;aa&quot;), recurse = TRUE) ## 获取当前数据名称下内存的占用： l2 &lt;- list(mtcars, mtcars, mtcars, mtcars) lobstr::obj_size(l2) 8.1.3.4.2 3.4.2 全集-处理R文件 ## 创建code文件夹： if(!file.exists(&quot;code&quot;)) dir.create(&quot;code&quot;) # 创建并写入数据到文件A.txt和B.txt cat(&quot;file A\\n&quot;, file = &quot;A.txt&quot;) #新建tmp文件夹，并拷贝A.txt和B.txt到tmp文件夹下 dir.create(&quot;tmp&quot;) file.copy(c(&quot;A.txt&quot;, &quot;B.txt&quot;), &quot;tmp&quot;) #删除tmp文件夹,并删除里面的内容,当recursive = TRUE; unlink(&quot;tmp&quot;, recursive = TRUE) #修改文件名C.txt为D.txt file.rename(&quot;C.txt&quot;, &quot;D.txt&quot;) #读取bin目录下所有文件及目录(包含全路径) df &lt;- dir(file.path(R.home(), &quot;bin&quot;), full.names = TRUE) #返回当前目录下所有文件及目录的信息 file.info(list.files()) #返回当前目录下所有文件的最近一次修改时间 file.mtime(list.files()) file.info(list.files())$mtime #返回当前目录下所有文件的大小，目录size返回为0 file.size(list.files(())) file.info(list.files())$size ## 复制文件 file.copy用于 file.copy(from, to, overwrite = recursive, recursive = FALSE,           copy.mode = TRUE, copy.date = FALSE) from是原始文件（目录）名，to是新文件（目录）名，二者可以是vector，但是长度需相同； overwrite 若为TRUE，则文件被覆盖； recursive 复制目录时recursive需为TRUE； copy.mode若为TRUE，permission bits一并复制过来； copy.date若为TRUE，文件日期一并复制过来。 ## 删除 函数unlink可以用来删除文件或目录，函数file.remove可以用来删除文件。 8.1.3.5 工作环境保存 ## RDS格式的数据共享： ## 单一的多维数据矩阵： export(linelist, here(&quot;data&quot;, &quot;clean&quot;, &quot;my_linelist.rds&quot;)) ## Rdata格式的数据共享: ## 可以同时包含多个数据框、模型结果、列表； rio::export(my_list, my_dataframe, my_vector, &quot;my_objects.Rdata&quot;) rio::import_list(&quot;my_list.Rdata&quot;) ## 读取数据： load(&quot;mydata.RData&quot;) load(&quot;mydata.rda&quot;) ## 将数据结果导出到rdata: save(object1, object2, file=“mywork.rda&quot;) # Slected objects 8.1.3.6 配置git-RSTUDIO 8.1.3.6.1 配置git-RSTUDIO image-20220909160449309 https://www.bilibili.com/video/BV1QQ4y1v7p1?spm_id_from=333.337.search-card.all.click 8.1.3.6.2 github配置R报错相关 ## 解决报错1： 在使用github客户端提交代码时，报错 failed to receive handshake ssl/tls connection failed ## 解决方法： # 任意位置git-bush： $ git config --global http.sslBackend &quot;openssl&quot; $ git config --global http.sslCAInfo [path to .pem file] ## 解决报错2： 报错解决]Error in nchar(object, type = &quot;chars&quot;) : invalid multibyte string, element 1 ## 解决方法： 在R的窗口上运行Sys.setlocale(category = &quot;LC_ALL&quot;, locale = &quot;us&quot;)即可解决 ## 解决报错3： # 关于一个非常复杂的报错： Error: Failed to install &#39;unknown package&#39; from GitHub: HTTP error 401. Bad ## 解决办法： ## 先利用: # R中打开： usethis::browse_github_pat() # 网页端打开github：R:GITHUB_PAT=nicheer124499 # R打开： usethis::edit_r_environ() 这会打开一个环境页，将刚才设置的R.GITHUB_PAT=gouzahndang123 粘贴复制上 然后保存，重启R就可以了。注意剪切上去的形式不是和上面的一致：需要： GITHUB_PAT=gouzahndang123(等价的秘钥) 然后打开还会持续报错； ## 在git中使用： $ git config --global http.sslBackend &quot;openssl&quot; git config --global http.sslCAInfo D:/bclang/R-4.0.2/library/openssl/cacert.pem 之后，使用 &gt; Sys.getenv(&quot;GITHUB_PAT&quot;) &gt; Sys.unsetenv(&quot;GITHUB_PAT&quot;) &gt; Sys.getenv(&quot;GITHUB_PAT&quot;) 8.1.3.7 默认加载R包： ## 默认加载R包 从R的安装目录的etc目录下找到Rprofile.site文件(例如R-2.15.0\\etc\\Rprofile.site)，用记事本打开，在这个文件最后添加一行： options(defaultPackages=c(getOption(&quot;defaultPackages&quot;), &quot;foreign&quot;, &quot;survival&quot;)) 把&quot;foreign&quot;, &quot;survival&quot;改成你需要自动加载的包就行了 8.1.3.8 改变R 的临时缓冲目录 ## 因为在构建模型过程中会涉及大量的rep，而部分R包的优化效果并不好，会产生局量 的临时缓存文件，但C盘往往是不够的；因此需要改变系统的临时缓冲文件到其他盘； ## 网络上检索到的各种办法都不能实际解决临时缓冲文件的问题： ## 解决办法是在对应版本的R文件下： ## 如：创建一个名为：Renviron.site的文件： （例如C：\\ Program Files \\ R \\ R-3.3.2 \\ etc） ## 创建后的文件，用notepad++打开： ## E:/rtemp 为临时文件缓冲路径：实测有效！！ TMPDIR=E:/rtemp TMP=E:/rtemp TEMP=E:/rtemp 8.1.4 S3和S4事件 ## S3 类型，list函数的变体，一个对象可以有多个类；使用attr来定义对象的类； x &lt;- 1 attr(x, &quot;class&quot;) &lt;- &quot;foo&quot; // class(x) &lt;- c(&quot;foo&quot;, &quot;bar&quot;) x ## S3构造的形式： myClass &lt;- structure(list(), class = &quot;myClass&quot;) myClass &lt;- list() class(myClass) &lt;- &quot;myClass&quot; # 继续添加类型； class(dnaseq) = append(class(dnaseq),&quot;DNAseq&quot;) class(dnaseq) [1] &quot;list&quot; &quot;DNAseq&quot; ## 读取类型，使用$; class(x) ## 读取顺序： 如果 class 属性是一个向量 c(”foo”, ”bar”), 则优先寻找 mean.foo, 然后 mean.bar, 最后 mean.default. ## 继承关系； 由于 class 属性可以是向量, 所以 S3 中的继承关系自然地表 现为 class 属性的前一个分量是后一个的子类.NextMethod() 函 数可以使得一系列的方法被依次应用于对象上. bar &lt;- function(x) UseMethod(&quot;bar&quot;, x) bar.son &lt;- function(x) c(&quot;I am son.&quot;, NextMethod()) bar.father &lt;- function(x) c(&quot;I am father.&quot;) foo &lt;- structure(1, class = c(&quot;son&quot;, &quot;father&quot;)) bar(foo) ## 注意这里调用的方法，是根据bar.son中的结果返回下一个类； ## ## S4 类型： S4 调度很复杂，因为 S4 有两个重要的特性： 多重继承，即一个类可以有多个父类， 多重分派，即泛型可以使用多个参数来选择一个方法。 ###---------------------S4类------------------- # setClass(Class, representation, prototype, contains=character(), # validity, access, where, version, sealed, package, # S3methods = FALSE, slots) #Class：定义的类名 #slots：定义类的属性 #prototype：定义属性的默认值（这个有什么用，我还没有发掘） #contains = character()：定义父类和继承关系 #where：定义存储空间 #sealed：若是TRUE，则同类名不能被定义 #package：定义所属包 setClass(&quot;Person&quot;, slots = list(name=&quot;character&quot;,age=&quot;numeric&quot;)) #实例化一个人,也就是创建一个对象,这里使用new来实例化对象： Person3 &lt;- new(&quot;Person&quot;,name=&quot;potter&quot;,age=49) Person3 #创建一个子类 setClass(&quot;Student&quot;,slots = list(ID=&quot;character&quot;,school= &quot;character&quot;), contains = &quot;Person&quot;) #contain表明类的继承关系，继承自Person #实例化类 Person3 &lt;- new(&quot;Person&quot;,name=&quot;potter&quot;,age=49) Student1 &lt;- new(&quot;Student&quot;,name=&quot;liming&quot;,age=16,ID=&quot;2020051419&quot;,school=&quot;Taiwan University&quot;) Student1 ## 使用@ 来检测类的属性： Student1@name Student1@school work&lt;-function(x) cat(x, &quot;is working&quot;) work(&#39;Conan&#39;) # 定义Person对象 setClass(&quot;Person&quot;,slots=list(name=&quot;character&quot;,age=&quot;numeric&quot;)) # 定义泛型函数work，即接口 setGeneric(&quot;work&quot;,function(object) standardGeneric(&quot;work&quot;)) # 定义work的实现，并指定参数类型为Person对象 setMethod(&quot;work&quot;, signature(object = &quot;Person&quot;), function(object) cat(object@name , &quot;is working&quot;) ) # 创建一个Person对象a a&lt;-new(&quot;Person&quot;,name=&quot;Conan&quot;,age=16) # 把对象a传入work函数 work(a) # 检查work的类型 class(work) # 直接查看work函数 work # 查看work函数的现实定义 showMethods(work) # 查看Person对象的work函数现实 getMethod(&quot;work&quot;, &quot;Person&quot;) selectMethod(&quot;work&quot;, &quot;Person&quot;) # 检查Person对象有没有work函数 existsMethod(&quot;work&quot;, &quot;Person&quot;) hasMethod(&quot;work&quot;, &quot;Person&quot;) 8.2 R报错机制学习 8.2.1 1 常见异常处理函数 ## 报错及反馈机制学习 # 异常处理是指在程序（脚本）运行期间发生的异常，是一种预先知道的状态。 # 错误是异常，异常并不总是错误。 ## 1 常见异常处理函数； -stop() -stopifnot() -warning() -supressWarnings() -error() -on.exit() -warn() -Try -TryCatch -withCallingHandlers # 异常处理的辅助函数： ## message()： # 在函数内部输出(并且message内部可以执行预算） # 类似于print()，更适合函数编程打印； ## class: # 解析Error和Warning message; 8.2.2 2 STOP ################# 停止 ###### # 2.1 stop(): # 停止表达式，并中断函数体： test_stop &lt;- function(min, max) { if (max &lt; min) stop(&#39;Upsii:max &lt; min&#39;) print(&#39;We are good: max &gt; min&#39;) } test_stop(22,10) #&gt; Error in test(22, 10) : max &lt; min test_stop(10,22) #&gt; [1] &quot;We are good: max &gt; min&quot; ## 2.2 stopifnot() ： ## 条件判断： ## 分为两种情况： ## 第一种是在stopifnot内部设置条件判断，true执行计算，不返回；否则，报错Error； ## 第二种是在stopifnot前，也即在输入stopifnot内部前设置条件判断， # 并基于条件判断给出反馈。 input_numbers &lt;- c(23,22,25) stopifnot(is.numeric(input_numbers)) #all TRUE input_statements &lt;- c(10==10, 22 &gt; 10) stopifnot(input_statements) #all TRUE input_pi &lt;- 3.15 #this is not a PI number! stopifnot(all.equal(pi, input_pi)) #Error returned 8.2.3 3 WARNING ############## 警告 ######## # warning() and supressWarnings() functions ## 3.1 warning() test_warning &lt;- function(min, max){ if(max &lt; min) { warning(&quot;Warning, warning. Max &lt; Min&quot;) } if(max &gt; min) { message(&quot;Well, max &gt; min&quot;)}} test_warning(20,10) -&gt; t1 #&gt; Warning message: #&gt; In test_warning(20, 10) : Warning, warning. Max &lt; Min # 3.2 supressWarnings():禁止警告；也即警告消息不反馈； suppressWarnings(test_warning(10,20)) # 这个函数功能也可以在option中设置： options(warning = F) # ----还需要确定 8.2.4 4 捕捉错误 ################# 捕捉错误： ########### ## 4.1 try()：捕捉错误，并忽略，收集错误信息 input_list &lt;- list(1, 2, 3, &#39;R Rules&#39;, 10, -500, 1.23, 0, 1024) for(inp in input_list) { print(paste(&quot;Power to the 2 of&quot;, inp, &quot;is: &quot;, inp**2)) } # Try() with silent ON for(inp in input_list) { try(print(paste(&quot;Power to the 2 of&quot;, inp, &quot;is: &quot;, inp**2)), silent = TRUE) } # Try() without silent：会挑选出条件错误判断语句，并返回错误！ for(inp in input_list) { try(print(paste(&quot;Power to the 2 of&quot;, inp, &quot;is: &quot;, inp**2))) } # try()函数的另外一个重要作用是解析错误，并返回错误属性； sos =function(x,y){x*y} x.inv &lt;- try(sos(1,&#39;2&#39;), silent=TRUE) print(&#39;try-error&#39; %in% class(x.inv)) ## TRUE ## 4.2 TryCatch()： 捕捉错误，停止，并基于错误进行下一步条件判断； TC_fun = function(x) { tryCatch(x**2, warning = function(w) {print(paste(&quot;negative argument&quot;, x)); -inp**x}, error = function(e) {print(paste(&quot;non-numeric argument&quot;, x)); NaN}) } ## 关于warning，在catch体系中不打印，直接执行函数体； ## 关于error：识别到错误，打印错误，并基于错误执行二次函数运算； for(input in input_list) { print(paste(&quot;TryCatch function of&quot;, input, &quot;=&quot;, TC_fun(input))) } ## 4.3 流控制： ## 4.3.1 在内部执行参数运算： power_calculation &lt;- function(x,y){ tryCatch( expr = { message(x**y) message(&quot;Successfully calculated x to power of y.&quot;) }, error = function(e){ message(&#39;Caught an error!&#39;) print(e) }, warning = function(w){ message(&#39;Caught an warning!&#39;) print(w) }, finally = { message(&#39;... Program execution finished ...&#39;)})} power_calculation(0,3) ## 4.3.2 在外部执行参数运算： beera &lt;- function(expr){ tryCatch(expr, error = function(e){ message(&quot;An error occurred:\\n&quot;, e) }, warning = function(w){ message(&quot;A warning occured:\\n&quot;, w) }, finally = { message(&quot;Finally done!&quot;) })} beera({2 + &#39;2&#39;}) 8.2.5 5 代码错误调试 ## 调试：错误跟踪：---- traceback() check_n_value &lt;- function(n) { if(n &gt; 0) { browser() ## Error occurs around here stop(&quot;n should be &lt;= 0&quot;) } } check_n_value(1) 8.2.6 6 代码时间消耗跟踪 ## 时间消耗跟踪：----- library(microbenchmark) microbenchmark(a &lt;- rnorm(1000), b &lt;- mean(rnorm(1000))) ## 函数运算时间统计： start &lt;-proc.time() proc.time()-start time_start&lt;-Sys.time() print(1) exc_time&lt;-difftime(Sys.time(),time_start,units = &#39;secs&#39;) print(paste0(&#39;code执行时间：&#39;,round(exc_time,2),&#39;secs&#39;)) 8.2.7 7 补充案例 ######################## stop： stop(&quot;na entries in table&quot;) # 另外的用途在于可以在函数中使用: 例如else之后的结果可以直接使用 else stop(&quot;这是个错误！&quot;) # 使用try来避开错误循环的影响结果：直接进行下一个循环； ###answer1:##### count &lt;- 0 inverses &lt;- vector(mode = &quot;list&quot;, 100) repeat { if (count == 100) break count &lt;- count + 1 x &lt;- matrix(sample(0:2, 4, replace = T), 2, 2) ## 这一步收集错误，并使得函数能够继续运行，并且不报错！ x.inv &lt;- try(solve(x), silent=TRUE) if (&#39;try-error&#39; %in% class(x.inv)) { next ## 这里是 函数不返回信息，也可以选择输出一些指定参数说明； } else{ inverses[[count]] &lt;- x.inv } } inverses ## 使用tryCatch()来设计收集函数错误；并且返回错误位置，下次方便继续运行； for (n in 1:length(productlink)){ tryCatch({ download.file(productlink[n],paste0(getwd(),&quot;/html/&quot;,productid[n,],&quot;.html&quot;),cacheOK = TRUE) },error=function(e){cat(&quot;ERROR :&quot;,conditionMessage(e),&quot;\\n&quot;)}) Sys.sleep(0.5) #增加了Sys.sleep(seconds)函数，让每一步循环都暂停一段时间。这个虽然会降低程序速度，但对于有访问限制的网站，不失为一个好的办法。 } ## 在这里数据监测有效性，在利用stop返回错误； if (!sum(check)==length(productlink)) { productlink&lt;-NULL productid&lt;-NULL stop(&quot;invalid productid please double check if any space or else in, and resave the file or the script will not run&quot;) } ## 直接显式收集错误，然后将错误按照打印输出；类似于stop的形式； data&lt;-function(n){ ####隐掉获得productname/price/category的代码 if(!length(productname)==1) {productname=&quot;Product not download or not existing&quot;} if (!length(price)==1) { price=NA category&lt;-&quot;Product not download or not existing&quot; } data.frame(productname,price,category) 8.2.8 8 相关资料 https://www.geeksforgeeks.org/handling-errors-in-r-programming/ https://bookdown.org/rdpeng/RProgDA/error-handling-and-generation.html http://adv-r.had.co.nz/Exceptions-Debugging.html 8.3 常用快捷命令 8.3.1 高效管道符号 8.3.1.1 使用%$%来传递属性值 ## 在传统的base R统计中： %$% 的作用是把左侧数据的属性名传给右侧，让右侧的调用函数直接通过名字，就可以获取左侧的数据。 iris %&gt;% table() -- 无法直接运用； 但可以使用%$%来进行优化base_R的管道运行； ## 执行高效运算： set.seed(123) data.frame(x=1:10,y=rnorm(10),z=letters[1:10]) %$% .[x&gt;5,] 8.3.1.2 使用%T&gt;%向左遍历输出： library(magrittr) set.seed(123) rnorm(10000) %&gt;% abs %&gt;% `*` (50) %&gt;% ## 指定参数运算时，可以直接使用``进行注释运算； matrix(ncol=100) %&gt;% rowMeans %&gt;% round %&gt;% ## 传统在分析时，需要先将结果导出后在执行hist，这里还是用%T&gt;%就可以实现边出图边向右输出。执行逻辑是在执行%T&gt;%之前进行前后向遍历； `%%`(7) %T&gt;% hist %&gt;% sum 8.3.1.3 %&lt;&gt;% 复合操作符 %&lt;&gt;%复合赋值操作符， 功能与 %&gt;% 基本是一样的，多了一项额外的操作，就是把结果写回到最左侧的对象（覆盖原来的值）。 library(magrittr) set.seed(123) x&lt;-rnorm(10) x %&lt;&gt;% abs %&gt;% sort ## 等价于,相当于直接赋值更新； x %&gt;% abs %&gt;% sort -&gt;x 8.3.1.4 %||% 验证函数 ## %||%是rlang包的一个验证函数参数： ## 原函数如下： function (x, y) ## x为非na时，输出y，否则输出x； { if (is_null(x)) y else x} ## 对执行一些默认函数输出时很有用，比如ggplot2中的画图参数常有默认值，就需要执行用户参数判断来输出是否验证结果形式。 8.3.1.5 %&amp;% 矩阵乘积 %&amp;% 是Matrix包一个取矩阵乘积的函数； 8.3.1.6 &lt;&lt;— 参数赋值 ## 使用全局赋值函数在tidy体内部来实现参数的外部函数命名： ## 在这里可以看到传统写法是将cox.zph() -&gt; zph_result; # 但在这里使用的是{zph_result &lt;&lt;- .} ，其中{}是dplyr流的固定写法； # 并且基于这种全局参数赋值方法以后，并不影响后续函数的结果的持续输出和转换： # 也即在cox.zph()输出结果的结果还可以直接传递到plot()体系中； melanoma %&gt;% coxphmulti(dependent_os, explanatory) %&gt;% cox.zph() %&gt;% {zph_result &lt;&lt;- .} %&gt;% plot(var=5) 8.3.2 常用辅助函数： 8.3.2.1 #&gt; 添加自动注释 ## 下面这种#&gt;写法，会在使用回车键后下一行的开头，也同步再增加一个#&gt;来写注释； #&gt; [1] &quot;We are good: max &gt; min&quot; 8.3.2.2 使用``来执行条件运算 ## 举例1： rnorm(10000) %&gt;% abs %&gt;% `*` (50) ## 举例2： rename_at(vars(names(.)) ,~ name1_ms) %&gt;% slice(-1) %&gt;% `row.names&lt;-`(&quot;1&quot;) 8.3.2.3 Rstudio View() ## 最有用的快捷键： 安装ctrl，并点击数据集对应的名称即可； 8.3.2.4 查看R包内的所有函数： ls(&quot;package::ggplot2&quot;) 8.4 正则表达式 8.4.1 常用正则表达式命令-1 ## 正则表达式： x &lt;- c(&quot;New theme&quot;, &quot;Old times&quot;, &quot;In the present theme&quot;) str_view(x, &quot;the&quot;) ## 常用正则： # 1、匹配任意字符：使用点号： s &lt;- c(&quot;abc&quot;, &quot;cabs&quot;, &quot;lab&quot;) str_view_all(s, &quot;ab.&quot;) # 点号表示提取任意字符； str_view_all(c(&quot;a.txt&quot;, &quot;a0txt&quot;), &quot;a[.]txt&quot;) # [.]不做特殊解释； ## 2、使用[] 来定义条件筛选： ## 模式“[ns]a.[.]xls” 表示匹配的第一个字符是n或s， 第二个字符是a，第三个字符任意，第四个字符是句点， 然后是xls。 ## 3、原位匹配：所谓原位匹配是设定标准进行数据寻找： grep(&quot;int x\\\\[5\\\\]&quot;, c(&quot;int x;&quot;, &quot;int x[5]&quot;), perl=TRUE) ## 案例2 x &lt;- c(&quot;2008-08-08&quot;, &quot;2017-01-18&quot;) m &lt;- regexpr(&quot;\\\\d\\\\d(\\\\d\\\\d)-\\\\1-\\\\1&quot;, x) regmatches(x, m) ## 4、匹配数字 [[:digit:]]+ grep(&quot;n\\\\d[.]xls&quot;, c(&quot;n1.xls&quot;, &quot;na.xls&quot;), perl=TRUE) ## 5、加好重复匹配： s &lt;- c(&quot;sa1&quot;, &quot;dsa123&quot;) mres &lt;- regexpr(&quot;sa[[:digit:]]+&quot;, s, perl=TRUE) regmatches(s, mres) ## 6、计数重复： grep(&quot;[[:digit:]]{3}&quot;, c(&quot;1&quot;, &quot;12&quot;, &quot;123&quot;, &quot;1234&quot;)) ## 7、正整数： &quot;\\\\A[0-9]+\\\\Z&quot; 8.4.2 常用正则表达式命令-2 1、 查询目录下包含data,并以dat结尾的文件： 1.1 使用函数 . * :其中 .表示匹配除 \\n 之外的其他单个字符；而*号则表示单次或多次重复*之前的内容 这样就可以使用 data.*.dat :正则的效果就可以提现出来实现筛选和提取 1.2 使用 函数 ? \\w : 问号 ? 一般表示匹配单个字符，另外?则表示匹配前面的子表达式零次或一次。 而\\w则匹配包括下划线在内的任何单个字符，相当于[a-zA-Z0-9_] 2、 ^ 表示该正则开始的位置，$ 表示该正则结束的位置；通常abv$,表示匹配abv结尾的数据格式； 3、() 表示一个子表达式开始和结束的位置 4、 + 匹配前面的子表达式一次或多次 * 匹配前面零次到多次 5、&quot;\\d&quot;: 匹配数字 6、 &quot;\\s&quot;：匹配空格,如有字符间有多个空格直接把&quot;\\s&quot; 写成 &quot;\\s+&quot; 7、常用的几种反义： &quot;\\W&quot; 匹配任意不是字母，数字，下划线 的字符 &quot;\\S&quot; 匹配任意不是空白符的字符 &quot;\\D&quot; 匹配任意非数字的字符 &quot;\\B&quot; 匹配不是单词开头或结束的位置 &quot;[^abc]&quot; 匹配除了abc以外的任意字符 8、[abc] 字符组 匹配包含括号内元素的字符 ，如可以写成[a-z] 8.4.3 量词 &quot;*&quot;(贪婪) 重复零次或更多 例如&quot;aaaaaaaa&quot; 匹配字符串中所有的a 正则： &quot;a*&quot; 会出到所有的字符&quot;a&quot; &quot;+&quot;(懒惰) 重复一次或更多次 例如&quot;aaaaaaaa&quot; 匹配字符串中所有的a 正则： &quot;a+&quot; 会取到字符中所有的a字符， &quot;a+&quot;与&quot;a*&quot;不同在于&quot;+&quot;至少是一次而&quot;*&quot; 可以是0次，稍后会与&quot;?&quot;字符结合来体现这种区别 &quot;?&quot;(占有) 重复零次或一次 例如&quot;aaaaaaaa&quot; 匹配字符串中的a 正则 ： &quot;a?&quot; 只会匹配一次，也就是结果只是单个字符a &quot;{n}&quot; 重复n次 例如从&quot;aaaaaaaa&quot; 匹配字符串的a 并重复3次 正则： &quot;a{3}&quot; 结果就是取到3个a字符 &quot;aaa&quot;; &quot;{n,m}&quot; 重复n到m次 例如正则 &quot;a{3,4}&quot; 将a重复匹配3次或者4次 所以供匹配的字符可以是三个&quot;aaa&quot;也可以是四个&quot;aaaa&quot; 正则都可以匹配到 &quot;{n,}&quot; 重复n次或更多次 与{n,m}不同之处就在于匹配的次数将没有上限，但至少要重复n次 如 正则&quot;a{3,}&quot; a至少要重复3次 8.4.4 捕获分组 ## 常用捕获分组 &quot;(exp)&quot; 匹配exp,并捕获文本到自动命名的组里 &quot;(?&lt;name&gt;exp)&quot; 匹配exp,并捕获文本到名称为name的组里 &quot;(?:exp)&quot; 匹配exp,不捕获匹配的文本，也不给此分组分配组号 ## 以下为零宽断言 &quot;(?=exp)&quot; 匹配exp前面的位置 如 &quot;How are you doing&quot; 正则&quot;(?&lt;txt&gt;.+(?=ing))&quot; 这里取ing前所有的字符，并定义了一个捕获分组名字为 &quot;txt&quot; 而&quot;txt&quot;这个组里的值为&quot;How are you do&quot;; &quot;(?&lt;=exp)&quot; 匹配exp后面的位置 如 &quot;How are you doing&quot; 正则&quot;(?&lt;txt&gt;(?&lt;=How).+)&quot; 这里取&quot;How&quot;之后所有的字符，并定义了一个捕获分组名字为 &quot;txt&quot; 而&quot;txt&quot;这个组里的值为&quot; are you doing&quot;; &quot;(?!exp)&quot; 匹配后面跟的不是exp的位置 如 &quot;123abc&quot; 正则 &quot;\\d{3}(?!\\d)&quot;匹配3位数字后非数字的结果 &quot;(?&lt;!exp)&quot; 匹配前面不是exp的位置 如 &quot;abc123 &quot; 正则 &quot;(?&lt;![0-9])123&quot; 匹配&quot;123&quot;前面是非数字的结果也可写成&quot;(?!&lt;\\d)123&quot; 8.4.5 python使用编译 "],["Statistical-flow-of-clinical-medicine.html", "第 9 章 临床医学统计流 9.1 COMMON.R 9.2 Rawdata cleaning pipeline 9.3 target_groups 9.4 dataset_for_analysis 9.5 final_analysis", " 第 9 章 临床医学统计流 9.1 COMMON.R 9.1.1 加载R包和配置全局环境 ## 1 加载R包和配置全局环境：---- install.packages(&quot;pacman&quot;) library(pacman) pacman::p_load(purrr,tidyverse,scales,dplyr,flextable,gt,gtsummary,rstatix,forcats,janitor,table1,tableone,arrow, data.table,datacleanr,stringr,tidyr,lubridate,openxlsx) # Global options options(scipen = 1000) options(scipen = 1, digits = 2) options(encoding = &#39;UTF-8&#39;) # getOption(&quot;digits&quot;) Sys.setlocale(&quot;LC_ALL&quot;,&quot;Chinese&quot;) 9.1.2 使用officer来启动函数参数： 9.1.3 Table of content center_par &lt;- fp_par(text.align = &quot;center&quot;,padding = 10) bold_face &lt;- shortcuts$fp_bold(font.size = 20) toc &lt;- fpar( ftext(&quot;目 录&quot;, prop = bold_face ), fp_p = center_par) 9.1.4 Cover template -封面 ## 基于officer包生成项目封面： bold_face1 &lt;- fp_text(font.size = 25, bold = TRUE, font.family = &quot;Times New Roma&quot;) bold_face2 &lt;- fp_text(font.size = 20, font.family = &quot;宋体&quot;) cover &lt;- fpar( run_linebreak(), run_linebreak(), run_linebreak(), run_linebreak(), run_linebreak(), # logo external_img(here(&quot;figures&quot;, &quot;palan.png&quot;), height = 1.045, width = 4.975), run_linebreak(), run_linebreak(), run_linebreak(), run_linebreak(), run_linebreak(), run_linebreak(), run_linebreak(), run_linebreak(), run_linebreak(), run_linebreak(), run_linebreak(), #TITLE ftext(&quot;...&quot;, prop = bold_face1), run_linebreak(), run_linebreak(), run_linebreak(), run_linebreak(), run_linebreak(), run_linebreak(), run_linebreak(), run_linebreak(), run_linebreak(), run_linebreak(), run_linebreak(), # AUTHOR AND DATE ftext(&quot;athorA&quot;, prop = bold_face2), run_linebreak(), ftext(&quot;athorB&quot;, prop = bold_face2), run_linebreak(), ftext(&quot;athorC&quot;, prop = bold_face2), run_linebreak(), run_linebreak(), ftext(paste(&quot;日期：&quot;,Sys.Date(),sep = &quot;&quot;), prop = bold_face2), fp_p = fp_par(text.align = &quot;center&quot;)) 9.1.5 快捷辅助函数： vi = function(data){ View(data)} # vi(iris) ## 2.1.2 计算唯一患者人数： pe = function(data){ return(n_distinct(data$patient_id)) } ## 2.1.3 转换格式：num/cha/fac/dat ## 需要注意as_date默认的时间设置时分秒为早晨八点； tr = function(data,lab,funss){ if(funss == &quot;num&quot;){ return( data %&gt;% mutate(across(.cols = lab, .fns = as.numeric))) }else if(funss == &quot;dat&quot;){ return( data %&gt;% mutate(across(.cols = lab, .fns = as_date))) }else if(funss == &quot;cha&quot;){ return( data %&gt;% mutate(across(.cols = lab, .fns = as.character))) }else if(funss == &quot;fac&quot;){ return( data %&gt;% mutate(across(.cols = lab, .fns = as.factor))) }else{ return(&quot;input function fasle&quot;) } } # iris %&gt;% tr(.,&quot;Sepal.Length&quot;,&quot;cha&quot;) ## 2.1.4 查询文件名： na = function(data){return(names(data))} ## 2.1.5转化频次 fr = function(data){ data = as.numeric(data) return( paste0(round(data*100,2),&quot;%&quot;,sep=&quot;&quot;) ) } # fr(5) ## 2.1.6 unique(): un = function(data){return(unique(data))} ## 2.1.7 计算平均值和标准差： ## 需要注意是全局已经设置小数为2位； ## data；输入数据，可适配tidyvrse： ## select1: 输入数据，并提取指定行计算平均值和标准差： mean_sd = function(data,select1){ data %&gt;% select({{select1}}) %&gt;% summarise(mean = mean({{select1}}), sd = sd({{select1}})) %&gt;% mutate(across(where(is.numeric),~ round(.,2))) } # iris %&gt;% ms(.,Sepal.Length) %&gt;% as.data.frame() mean_sd_union = function(data,select1){ t1 = data %&gt;% select({{select1}}) %&gt;% summarise(mean = mean({{select1}}), sd = sd({{select1}})) %&gt;% mutate(across(where(is.numeric),~ round(.,2))) return(paste0(t1$mean,&quot;(&quot;,t1$sd,&quot;)&quot;)) } ## 2.1.8 人数比例： ## 分母：denomination：den pop_percent = function(data,den){ out =list() out$pat = pe(data) out$per = fr(pe(data)/pe(den)) return(out) } pop_per_union = function(data,den){ union_t = paste0(pe(data),&quot;(&quot;,fr(pe(data)/pe(den)),&quot;)&quot;) return(union_t) } ## prec_bp %&gt;% pp(.,pe(prec)) ## 2.1.9 round(2): round2 = function(data,n=2){ data %&gt;% mutate(across(where(is.numeric),~ round(.,n))) } ## 2.1.10 统计visit_id的数量： vis = function(data){ data %&gt;% select(visit_id) %&gt;% n_distinct() } 9.1.6 快捷统计函数： ## 2.1.7 计算平均值和标准差： ## 需要注意是全局已经设置小数为2位； ## data；输入数据，可适配tidyvrse： ## select1: 输入数据，并提取指定行计算平均值和标准差： mean_sd = function(data,select1){ data %&gt;% select({{select1}}) %&gt;% summarise(mean = mean({{select1}}), sd = sd({{select1}})) %&gt;% mutate(across(where(is.numeric),~ round(.,2))) } # iris %&gt;% ms(.,Sepal.Length) %&gt;% as.data.frame() ## 将平均值和标准差以括号进行合并： mean_sd_union = function(data,select1){ t1 = data %&gt;% select({{select1}}) %&gt;% summarise(mean = mean({{select1}}), sd = sd({{select1}})) %&gt;% mutate(across(where(is.numeric),~ round(.,2))) return(paste0(t1$mean,&quot;(&quot;,t1$sd,&quot;)&quot;))} ## 2.1.8 人数比例： ## 分母：denomination：den pop_percent = function(data,den){ out =list() out$pat = pe(data) out$per = fr(pe(data)/pe(den)) return(out) } pop_per_union = function(data,den){ union_t = paste0(pe(data),&quot;(&quot;,fr(pe(data)/pe(den)),&quot;)&quot;) return(union_t) } ## prec_bp %&gt;% pp(.,pe(prec)) 9.1.7 描述性分析统计函数 9.1.7.1 年龄计算 ## birthDate: 出生日期： ## refDate： 默认为当前时间，项目需要index_date: ## 输出患者年龄： calc_age &lt;- function(birthDate, refDate = Sys.Date(), unit = &quot;year&quot;) { require(lubridate) if (grepl(x = unit, pattern = &quot;year&quot;)) { as.period(interval(birthDate, refDate), unit = &#39;year&#39;)$year } else if (grepl(x = unit, pattern = &quot;month&quot;)) { as.period(interval(birthDate, refDate), unit = &#39;month&#39;)$month } else if (grepl(x = unit, pattern = &quot;week&quot;)) { floor(as.period(interval(birthDate, refDate), unit = &#39;day&#39;)$day / 7) } else if (grepl(x = unit, pattern = &quot;day&quot;)) { as.period(interval(birthDate, refDate), unit = &#39;day&#39;)$day } else { print(&quot;Argument &#39;unit&#39; must be one of &#39;year&#39;, &#39;month&#39;, &#39;week&#39;, or &#39;day&#39;&quot;) NA }} 9.2 Rawdata cleaning pipeline ## 常用线性清理范式（1） linelist &lt;- linelist_raw %&gt;% # standardize column name syntax janitor::clean_names() %&gt;% # define class mutate(across(contains(&quot;date&quot;), as.Date), generation = as.numeric(generation), age = as.numeric(age)) %&gt;% # mutation mutate(days_onset_hosp = as.numeric(date_hospitalisation - date_onset)) %&gt;% mutate(hospital = recode(hospital, &quot;Mitylira Hopital&quot; = &quot;Military Hospital&quot;, &quot;other&quot; = &quot;Other&quot;)) %&gt;% mutate(hospital = replace_na(hospital, &quot;Missing&quot;)) %&gt;% mutate(age_years = case_when( age_unit == &quot;years&quot; ~ age, age_unit == &quot;months&quot; ~ age/12, is.na(age_unit) ~ age, TRUE ~ NA_real_)) %&gt;% mutate( age_cat = epikit::age_categories(age_years, breakers = c(0, 5, 10, 15, 20, 30, 50, 70))) %&gt;% 9.3 target_groups 9.3.1 加载集成数据集 9.3.1.1 Vroom library(here) source(here(&quot;R&quot;, &quot;common.R&quot;), encoding = &quot;utf-8&quot;) patient_filepath &lt;- fs::dir_ls(here(&quot;data&quot;, &quot;preprocess&quot;, &quot;patient_clean&quot;), glob = &quot;*/*.csv&quot;) diagnosis_filepath &lt;- fs::dir_ls(here(&quot;data&quot;, &quot;preprocess&quot;, &quot;diagnosis_clean&quot;), glob = &quot;*/*.csv&quot;) patient &lt;- vroom(patient_filepath) %&gt;% select(-&quot;...1&quot;) %&gt;% distinct() diagnosis &lt;- vroom(diagnosis_filepath) %&gt;% select(-&quot;...1&quot;) %&gt;% distinct() 9.3.1.2 Arrow library(here) source(here(&quot;R&quot;, &quot;prepare and analyse data&quot;, &quot;prepare.R&quot;), encoding = &quot;UTF-8&quot;) patient &lt;- open_dataset( here(&quot;data&quot;, &quot;preprocess&quot;, &quot;lan&quot;, &quot;patient_clean&quot;)) %&gt;% collect() visit &lt;- open_dataset( here(&quot;data&quot;, &quot;preprocess&quot;, &quot;lan&quot;, &quot;visit_clean&quot;)) %&gt;% collect() 9.3.2 保存数据集 Make sure only patient_id, visit_id and provider_id are kept in the end. overall_target &lt;- diagnosis %&gt;% filter(patient_type == &quot;住院患者&quot;) %&gt;% filter(!sub_group == &quot;other&quot;) target_group_1 &lt;- overall_target %&gt;% filter(sub_group == &quot;非内分泌科就诊&quot;) %&gt;% left_join(select(visit, patient_id, visit_id, admission_datetime)) %&gt;% group_by(patient_id) %&gt;% arrange(admission_datetime) %&gt;% slice_head() %&gt;% ungroup() %&gt;% select(patient_id, visit_id, provider_id) %&gt;% distinct() 9.3.3 患者平衡匹配前后信息比较 This part is for comparing the differences in baseline characteristics of the dataset after using PSM. Make sure only baseline characteristics are kept in the end. table_psm_before &lt;- CreateTableOne(vars = c(&#39;年龄&#39;, &#39;年龄分组&#39;, &#39;性别&#39;, &#39;医院&#39;, &#39;医保&#39;, &#39;医院等级&#39;), data = data_psm_before, strata = &#39;strata&#39;, factorVars = c(&#39;年龄分组&#39;, &#39;医保&#39;, &#39;性别&#39;, &#39;医院等级&#39;), smd = TRUE ) table_psm_before &lt;- print( table_psm_before, printToggle = FALSE, noSpaces = TRUE, smd = TRUE # add smd ) table_psm_after &lt;- CreateTableOne(vars = c(&#39;年龄&#39;, &#39;年龄分组&#39;, &#39;性别&#39;, &#39;医院&#39;, &#39;医保&#39;, &#39;医院等级&#39;), data = data_psm_after, strata = &#39;strata&#39;, factorVars = c(&#39;年龄分组&#39;, &#39;医保&#39;, &#39;性别&#39;, &#39;医院等级&#39;), smd = TRUE ) table_psm_after &lt;- print( table_psm_before, printToggle = FALSE, noSpaces = TRUE, smd = TRUE # add smd ) 9.4 dataset_for_analysis Here is an example: library(here) source(here(&quot;R&quot;, &quot;prepare and analyse data&quot;, &quot;lan&quot;, &quot;target_groups.R&quot;), encoding = &quot;UTF-8&quot;) data_endpoint3 &lt;- group_endpoint3 %&gt;% left_join(patient) %&gt;% left_join( select(visit, patient_id, visit_id, insurance_type) ) %&gt;% left_join( select(lab, patient_id, visit_id, if_ctn, ctn, if_ckmb, ckmb, if_ck, ck) ) %&gt;% left_join( select(diagnosis, patient_id, visit_id, &quot;diag1&quot;, &quot;diag1&quot;, &quot;diag1&quot;, &quot;diag1&quot;, &quot;diag1&quot;, &quot;diag1&quot;, &quot;diag1&quot;) ) %&gt;% left_join( select(prescribing, patient_id, visit_id, &quot;diag1&quot;, &quot;diag1&quot;, &quot;diag1&quot;, &quot;diag1&quot;, &quot;diag1（ACEI）&quot;, &quot;diag1（ARB）&quot;, start_kangban, end_kangban, kangban) ) %&gt;% distinct() %&gt;% mutate_at(vars(if_ctn, if_ckmb, if_ck), ~ replace_na(., 0)) secondary5 &lt;- rbind(subgroup, overall) %&gt;% apply_labels(fbg_diff = &quot;空腹血糖变化差值&quot;, fbg_reg = &quot;末次空腹血糖&lt;6.1m/mol&quot;, age = &quot;年龄&quot;, sex = &quot;性别&quot;, insurance_type = &quot;医疗保险&quot;, region = &quot;就诊地区&quot;, cad = &quot;冠心病&quot;, hbp = &quot;高血压&quot;, stroke = &quot;卒中&quot;, ckd = &quot;慢性肾功能不全&quot;, disease_num = &quot;合并疾病数目&quot;, group_1 = &quot;基础vs.预混&quot;, treatment_plan = &quot;糖尿病用药方案&quot;, inpatient_time = &quot;住院时长&quot;, total_fee = &quot;总花费&quot;, trt_fee = &quot;治疗费&quot;, service_fee = &quot;服务费&quot;, sub_group = &quot;亚组&quot; ) write_dataset( data_endpoint3, here(&quot;data&quot;, &quot;analysis&quot;, &quot;lan&quot;, &quot;endpoint3&quot;), format = &quot;parquet&quot;) 9.5 final_analysis 此部分主要用于深层进阶分析 ## present 使用rmarkdown批量输出 --- output: officedown::rdocx_document --- knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE) library(here) source(here(&quot;R&quot;, &quot;prepare and analyse data&quot;, &quot;lan&quot;, &quot;endpoint1.R&quot;), encoding = &quot;utf-8&quot;) cover \\newpage使用其进行换页 toc block_toc() endpoint one table_1 %&gt;% as_flex_table() %&gt;% set_table_properties(layout = &quot;autofit&quot;) endpoint two table_3 %&gt;% as_flex_table() %&gt;% set_table_properties(layout = &quot;autofit&quot;) endpoint three table_4 %&gt;% as_flex_table() %&gt;% set_table_properties(layout = &quot;autofit&quot;) "],["Statistical-books.html", "第 10 章 统计书籍 10.1 R-统计书籍 10.2 使用gitbook写书 10.3 使用bookdwon写书", " 第 10 章 统计书籍 10.1 R-统计书籍 10.1.1 tidyverse相关 ## 使用tidyverse流来构建数据清洗、整理、建模及可视化流程： https://moderndive.com/index.html ## 使用tidyverse进行数据统计的最关键书籍 -R 数据科学 https://r4ds.had.co.nz/ ## The tidyverse style guide 编程风格指南； https://style.tidyverse.org/index.html ## Modern R with the tidyverse ## 包含Rstudio介绍、函数、数据统计、图表、统计建模、R包开发和标准化报告等； https://b-rodrigues.github.io/modern_R/ ## 《R语言编程—基于tidyverse》 -张敬信-统计学教师 https://zhuanlan.zhihu.com/p/198185888 10.1.2 ggplo2相关-网站 ## poltly:交互式R视图 # 提供海量R图形和对应的R代码 https://chart-studio.plotly.com/feed/#/ ## The R Graph Gallery 这是使用R 编程语言 制作的图表集合，画廊将重点放在 tidyverse 和ggplot2上 https://www.r-graph-gallery.com/ ## R代码图表编辑指南： # 提供大量的图表及原始代码；还包括BASE R、ggplot2和颜色的各种图表设置 https://r-charts.com/ ## GGPLOT2最经常绘制的50种图形： http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html 10.1.3 绘图指南 ## Data Visualization with R # 单变量图、多元变量图、地图、时间相关图、统计出图、交互式图表和其他特殊图表： https://rkabacoff.github.io/datavis/ ## Fundamentals of Data Visualization # 包含数据可视化的类型、数据可视化的原则和误区，以及其他补充 https://clauswilke.com/dataviz/ ## 数据可视化实用的介绍 Data Visualization A practical introduction ## 包含ggplot2的设计美感、各种绘图参数设计以及绘图优化等 https://socviz.co/index.html#preface ## ggplot2的绘图指南： # 这是Springer 出版的“ggplot2：用于数据分析的优雅图形”的第 3 版正在进行中的在线版本。 https://ggplot2-book.org/index.html 10.1.4 绘图相关的博客 ## 目前最全的R语言-图片的组合与拼接 https://www.jianshu.com/p/fcfc3c7cb4e0 ## R---plot()参数详解 https://www.jianshu.com/p/419d84e83548 10.1.5 医学指南相关 ############ 纯统计相关 #################### ## 医学统计学：偏理论公式推导-无代码； https://wangcc.me/LSHTMlearningnote/index.html ## 医学统计学-读书笔记： # 某个医学研究生的读书笔记：偏向于理论公式推导 https://bookdown.org/wxhyihuan/Notebook-of-medical-statistics-1605856202966/ ## Prism 9 统计指南 - 统计学原理 http://www.graphpad-prism.cn/guides/prism/9/statistics/stat_---_principles_of_statistics_-.htm ############ R代码实现统计相关 #################### ## 临床医学统计-偏R代码实现； https://bookdown.org/jbrophy115/bookdown-clinepi/ ## (Mostly Clinical) Epidemiology with R # 数据分析、关联、统计推断、临床设计、因果推断、混杂因素、偏差分析、分类、 https://bookdown.org/jbrophy115/bookdown-clinepi/ ## 使用 R 进行可重复的医学研究 -- 很重要的一本书 # 数据清洗、临床统计、样本量计算、随机化、可视化 https://bookdown.org/pdr_higgins/rmrwr/ ## 科研者之家提供的部分医学相关的R代码实现统计知识： https://www.home-for-researchers.com/static/index.html#/ ## 流行病学家 R 手册 R for applied epidemiology and public health https://epirhandbook.com/en/index.html ## 提供简易的R医学课程指导，包括最常用的cox回归等知识； https://argoshare.is.ed.ac.uk/healthyr_book/ ## R实现分组调查设计和分析： Survey Design and Analysis https://bookdown.org/mpfoley1973/survey/#the-survey-package ## 提供是生存分析，多种回归的优化解析方案： https://argoshare.is.ed.ac.uk/healthyr_book/summary-2.html 10.1.6 医学统计R包 ## 2021年八月：新的 CRAN 医学软件包的“前 40 名”精选 --这个还需要更仔细研究下 https://rviews.rstudio.com/2021/08/12/r-medicine-2021/ 10.1.7 医学统计相关博客 ## 简书中提供的大量关于医学统计相关的信息： https://www.jianshu.com/search?q=%E4%B8%B4%E5%BA%8A%E6%95%B0%E6%8D%AE%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90&amp;page=1&amp;type=note ## 真实世界研究相关的资料： # 超越数据——了解 RWE 如何应用于肿瘤学研究和药物开发。 https://rwe.flatiron.com/ ## 医学统计学相关的课程全集：--医垰会 https://www.mediecogroup.com/zhuanlan/lessons/772/?utm_source=wechat1211 10.1.8 统计指南-R代码实现 ## 高级统计建模（课程代码）：--R代码实现： https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ https://www.stat.cmu.edu/~cshalizi/mreg/15/ https://adv-r.hadley.nz/index.html http://adv-r.had.co.nz/Functional-programming.html https://stat545.com/functions-part1.html https://bookdown.org/rdpeng/RProgDA/advanced-r-programming.html https://discdown.org/rprogramming/functions.html ## 因果数据科学指南： https://tlverse.org/tlverse-handbook/index.html ## 聚合大量统计学知识：-附R代码实现； https://rcompanion.org/handbook/K_01.html ## 主要参考书籍：数据科学的 R 编程 R Programming for Data Science ## 建议日常打开放在网页中使用： # 包含：数据存储、数据分组、时间、dplyr、循环控制函数、字符正则、代码优化、 # 数值模拟、并行计算； https://bookdown.org/rdpeng/rprogdatascience/ 10.1.9 数据建模相关 ## 使用 brms、ggplot2 和 tidyverse重新思考统计数据 https://bookdown.org/content/c23e1ece-b5d6-4cab-8d8e-978c2e5b7a53/ ## 快速入门 | mlr3 书 https://mlr3book.mlr-org.com/ 10.1.10 整合流程 ## quick R 数据输入 数据管理 统计数据 高级统计 图表 https://www.statmethods.net/stats/withby.html ## 数据输入 数据管理 统计数据 高级统计 图表 大数据框架 https://techvidvan.com/tutorials/r-hadoop-integration/ ## R语言忍者秘笈-数据结构、文本、自动化报告、编写R包； https://bookdown.org/yihui/r-ninja/ ## 全学科的R统计指南：简称R大书，包含各个学科知识体系： https://www.bigbookofr.com/ ## R语言教程-李东风教授中文版-用于统计课程教学 # 包括：变量、函数、标准化报告和数据统计、绘图、统计建模、文本处理等 https://www.math.pku.edu.cn/teachers/lidf/docs/Rbook/html/_Rbook/index.html ## 高级R ADVANCE R ## 提供大量R 的高阶优化知识，主要包括函数优化和面向对象编程 https://adv-r.hadley.nz/index.html ## 在线书籍pdf：Hands-On Programming with R ## 提供数据输入 数据管理 统计数据 高级统计 图表 https://d1b10bmlvqabco.cloudfront.net/attach/ighbo26t3ua52t/igp9099yy4v10/igz7vp4w5su9/OReilly_HandsOn_Programming_with_R_2014.pdf ## 整洁R代码：特点是包括训练与预测、评估与调优 https://datasciencebook.ca/ ## Cookbook for R 中文版 https://www.bookstack.cn/read/Cookbook-for-R-Chinese/040fb5e44c703675.md 10.1.11 博客社区 ## R的博客社区-共享书籍： https://rviews.rstudio.com/2021/11/04/bookdown-org/ ## R的博客社区-可提供会议链接和最新的共享代码资源： https://rviews.rstudio.com/ ## rmarkdwon编写的统计指南，包含大量R的最新指南信息 https://rpubs.com/ ## 提供海量基于markdown格式写的R相关书籍； https://bookdown.org/ ## R-博客，提供大量与R相关的博客知识； https://www.r-bloggers.com/ ## 中文：统计之都 https://cosx.org/ 10.1.12 Rmarkdown ## R Markdown Reference Guide https://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf?_ga=2.258283010.941366294.1644810156-143414957.1644300675 ## R Markdown Cookbook # Yihui Xie, Christophe Dervieux, Emily Riederer # 非常详细且完善； https://bookdown.org/yihui/rmarkdown-cookbook/ ## R Markdown: The Definitive Guide # Yihui Xie, J. J. Allaire, Garrett Grolemund # 更完善的构建文档，包含各个类型文档编写的形式； https://bookdown.org/yihui/rmarkdown/ 10.1.13 R-Rstudio ## r的各个版本： https://cran.r-project.org/bin/windows/base/old/ ## RStudio Cheatsheets - RStudio-小抄笔记 https://www.rstudio.com/resources/cheatsheets/ ## R中的软件开发： https://bookdown.org/rdpeng/RProgDA/profiling-and-benchmarking.html 10.1.14 shiny ## master shiny https://mastering-shiny.org/scaling-functions.html ## shiny工程机应用： https://engineering-shiny.org/ 10.2 使用gitbook写书 ## 如何使用gitbook来写书； https://blog.csdn.net/weixin_39637597/article/details/104456912?utm_medium=distribute.pc_relevant.none-task-blog-baidujs_title-2&amp;spm=1001.2101.3001.4242 ## gitbook使用指南： 安装： 首先安装nodejs; 然后使用rpm安装gitbook； npm install -g gitbook-cli gitbook -V ## 会自动查看gitbook并更新软件； ## 初始化命令： gitbook init 生成两个初始文件；readme.md对书籍的简单介绍；summary为书籍目录结构； gitbook server:预览书目； ## 构建目录： gitbook子目录写作： * [zilu](zilu.md) - [zilu](zilu.md) - (hrttps) ## 安装插件： 在该文件夹下生成book.json文件； 并在npm下搜索相关插件的命令（https://www.npmjs.com/），把对应的代码复制到上面即可； 复制完代码后执行， gitbook install 安装对应的插件； 最后，在进行gitbook serve即可； # 对应的添加评论插件； https://www.npmjs.com/package/gitbook-plugin-comments-footer ## 其他可能有用的； gitbook-plugin-donate(打赏按钮): https://www.npmjs.com/package/gitbook-plugin-donate gitbook-plugin-github-buttons(GitHub按钮): https://www.npmjs.com/package/gitbook-plugin-github-buttons gitbook-plugin-edit-link(GitHub编辑按钮): https://www.npmjs.com/package/gitbook-plugin-edit-link ## 命令合集： gitbook init //初始化目录文件 gitbook help //列出gitbook所有的命令 gitbook --help //输出gitbook-cli的帮助信息 gitbook build //生成静态网页 gitbook serve //生成静态网页并运行服务器 gitbook build --gitbook=2.0.1 //生成时指定gitbook的版本, 本地没有会先下载 gitbook ls //列出本地所有的gitbook版本 gitbook ls-remote //列出远程可用的gitbook版本 gitbook fetch 标签/版本号 //安装对应的gitbook版本 gitbook update //更新到gitbook的最新版本 gitbook uninstall 2.0.1 //卸载对应的gitbook版本 gitbook build --log=debug //指定log的级别 gitbook builid --debug //输出错误信息 ## 相关插件： # 常用插件推荐： https://blog.csdn.net/qq_37149933/article/details/64170653?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_utm_term~default-1.searchformbaiduhighlight&amp;spm=1001.2101.3001.4242 disqus 添加 disqus 评论插件。 * [介绍](README.md) - [next](zil.md) ## 插件的参考写作方法： { &quot;title&quot;: &quot;编写gitbook电子书教程&quot;, &quot;description&quot;: &quot;gitbook电子书教程&quot;, &quot;author&quot;: &quot;sphard&quot;, &quot;language&quot;: &quot;zh-hans&quot;, &quot;root&quot;: &quot;.&quot;, &quot;plugins&quot;: [ &quot;donate&quot;, &quot;github-buttons@2.1.0&quot;, &quot;edit-link&quot; ], &quot;pluginsConfig&quot;: { &quot;donate&quot;: { &quot;wechat&quot;: &quot;https://sphard.com/images/wechatpay.jpg&quot;, &quot;alipay&quot;: &quot;https://sphard.com/images/alipay.jpg&quot;, &quot;title&quot;: &quot;&quot;, &quot;button&quot;: &quot;打赏&quot;, &quot;alipayText&quot;: &quot;支付宝打赏&quot;, &quot;wechatText&quot;: &quot;微信打赏&quot; }, &quot;github-buttons&quot;: { &quot;repo&quot;: &quot;darrenliuwei/gitbook&quot;, &quot;types&quot;: [ &quot;star&quot; ], &quot;size&quot;: &quot;small&quot; }, &quot;edit-link&quot;: { &quot;base&quot;: &quot;https://github.com/darrenliuwei/gitbook/edit/master&quot;, &quot;label&quot;: &quot;Edit This Page&quot; } } } ## gitbook写作知识进阶 https://www.jianshu.com/p/a3f2316aee77 ## 相关报错解决： # Gitbook错误cb.apply isnotafunction的解决办法 根据报错路径找到`cb.apply`相关的函数全部注释掉即可； 10.3 使用bookdwon写书 10.3.1 常用功能标记 10.3.1.1 快速预览 ## 快速预览章节： preview_chapter() ## 实时预览更新html: serve_book() 10.3.1.2 标题命名 ## 常规标题： #使用`#`和`##`等形式来实现自动标题命名，会自动添加序号； ## 不给`#`号标注的行进行标题命名，使用{-} ## 标题 {-} ## 另外一种非编号的写法： {.unnumbered} --注意非编码但仍然可以作为大标题使用； 10.3.1.3 常用Markdown标记 ## 块输出：使用`&gt;`做标记； &gt; --- Mark Twain ## 纯代码块输出，使用`双``点号形式标注 # 与md格式的文件形式是一致的； 10.3.2 R代码的构建 ## [1] 2 ## [1] -1.76920533 -0.78936774 0.04017023 -1.36672421 -0.40089046 -0.33401977 ## [7] 0.85150096 -1.39108964 -1.56090217 -0.02001170 图 10.1: A figure caption. 10.3.3 动态r代码 install.packages(&quot;webshot&quot;) webshot::install_phantomjs() DT::datatable(iris) 10.3.4 引入shiny应用 knitr::include_url() # 或者： knitr::include_app(&quot;https://yihui.shinyapps.io/miniUI/&quot;, height = &quot;600px&quot;) 10.3.5 指定输出和解析的文件 rmd_files: [&quot;index.Rmd&quot;, &quot;chapters/chapt1.RMD&quot;, &quot;chapters/chapt2.RMD&quot;] 10.3.6 文件最后修改时间 file.info(&quot;文件路径/文件名.Rmd&quot;)$mtime 10.3.7 添加指定序号 在00-author.RMD之后的01-文件的首行添加 \\mainmatter; 并在每个分级标题第一个一级标题中的右侧添加： # 确立分析目标 {#Establish-analysis-objectives} 10.3.8 辅助知识：—- 1、bookdwon支持解析md格式的文件的html化； 2、仿照别人书的结构是最快速的方法； "],["Statistics-and-Research-Design.html", "第 11 章 统计与研究设计 11.1 常用参数释义 11.2 临床研究中常用的统计方法和常见问题 11.3 差异分析 11.4 多元线性回归 11.5 相关检验 11.6 医疗相关检验", " 第 11 章 统计与研究设计 11.1 常用参数释义 11.1.1 因果回归参数 11.1.1.1 自变量/解释变量 决定因变量/被解释变量的变量。 11.1.1.2 因变量/被解释变量 被自变量/解释变量影响的变量。 内生变量：在模型内部被决定的变量。 外生变量：独立于模型其他解释变量的解释变量，模型中其他解释变量的变化不影响该变量的变化，而我们要研究的外生变量的变化反过来会造成内生变量的变化。 变量与方程式的关系：方程左边的变量一般是被解释变量/因变量，也是内生变量。外生变量一般只作为参数和自变量使用，不作为被解释变量/因变量。但也有一些反例，如无差异曲线和预算线，等产量线和等成本线，式子左端被设定为常数，可以视作参数/外生变量。 关系：单方程模型中，解释变量/自变量一般均为外生变量，包括多变量单方程模型，里面所有解释变量都是自变量，也都是外生变量，此外，模型的参数也都可视作外生变量，只是在模型中视作常数。换句话说，在单方程模型中，内生变量就指的是方程左端的因变量/被解释变量，而外生变量包括方程左端的自变量/解释变量，以及方程的参数。 关于参数：在理论模型中，参数是外生变量，由参数决定的均衡点的坐标元素是内生变量。参数和理论模型中的均衡分析相关。我们在研究均衡(均衡一般与联立方程模型相关)的静态分析时，一般假设参数值均不变，研究此时的均衡状态；在比较静态分析中，我们可以放开某个参数，研究该参数变动时原有均衡状态的变动，并分析比较新旧均衡状态。动态分析涉及到滞后内生变量，解释起来太麻烦，略去不提。 虚拟变量/哑变量：一些只有属性但没法量化的因素对被解释变量/因变量也有影响，例如季节、教育程度等等，为了研究这些因素对被解释变量/因变量的影响，可以构造只取“0”或“1”的人工变量，这就是哑变量或者虚拟变量。举个例子，研究教育程度对平均工资的影响时，可以设定0代表小学以下，D1代表小学教育程度，D2代表中学，D3代表大学，这三个变量都只能取值0或1，于是可设立模型Y＝β0+β1D1+β2D2+β3D3，如果你是大学毕业，那就是D3＝1，D1和D2＝0，这样的话就可以研究这种只有属性但没法量化的变量对因变量/被解释变量的影响。 控制变量，是个要求，不是变量。控制变量在物理学的概念是指那些除了实验因素(自变量)以外的所有影响实验结果的变量，这些变量不是本实验所要研究的变量，所以又称无关变量、无关因子、非实验因素或非实验因子。只有将自变量以外一切能引起因变量变化的变量控制好，才能弄清实验中的因果关系。控制变量衍生到生活中的作用是控制一定影响因素从而得到真实的结果(抄的百度百科)。在经济学里，控制变量可以理解为“其他条件不变，α变量变动对β变量/均衡状态(X1, X2)的影响”。 协变量：在一个更大的系统里，协变量也影响因变量，但在特定的模型中，协变量属于我们不想研究的、希望它能保持不变的变量，这种变量一般被归为“其他条件不变”，这就要求你研究的模型所有变量位于同一个协变量水平上。 11.1.2 生存分析参数 11.1.2.1 OR/HR/RR image-20220401111952315 RR考虑了终点事件的发生与否，而HR不仅考虑了终点事件的有无，还考虑了到达终点所用的时间及截尾数据。HR就是包含了时间效应的RR。 当总体发生率比较低时，OR与RR基本相等。当总体率差值较大时，OR与RR差异明显。 补充一点在cox回归中RR值和HR值的计算逻辑是一样的； 11.1.2.2 逻辑回归中的OR值 OR值是《流行病学》中的重要概念，称作“优势比”（odds ratio），也称“比值比”，反映的是某种暴露与结局的关联强度。一般而言，OR值的意义可以总结如下（假设结局发生记为1，不发生记为0）：OR = 1，暴露与结局的无相关性；OR &gt; 1，暴露可以促进结局的发生；OR &lt; 1，暴露可以抑制结局事件的发生Logistic很重要的意义就在于会直接输出OR值，这一点甚至比看直接的回归系数（β）还有意义。OR值与回归系数β的数量关系为：OR = eβ。 ## 所谓的“优势”可以理解为“暴露比值”！那怎么理解暴露比值呢？ 在本例中，对于患有糖尿病的对象，暴露比值为：吸烟的比例除以不吸烟的比例，即为：24/16 = 1.50；同样，在不患有糖尿病的人群中，也可以计算一个吸烟的比例除以不吸烟的比例，即为：18/22 = 0.82。把这两个比例相除，就得到了吸烟与糖尿病相关关系的OR值，即OR = 1.50/0.82= 1.83&gt;1。由此，我们可以初步推断，吸烟会加重患糖尿病的风险。 11.1.3 方差分析参数解释 11.1.3.1 常见研究设计： （1）完全随机设计：将受试对象随季节分配到各组中进行实验或者观察。 （2） 配对设计：数据不独立； 11.1.3.2 分组设计匹配： 11.1.3.2.1 3.2.1 自体匹配 1 将同一受试对象处理（实验或者治疗）前后的结果进行比较。 2 对同一样品采取两种不同的方法测量同一指标 3 同一受试对象的不同部位进行测量。 11.1.3.2.2 3.2.2 异体匹配： 2.4 两个受试对象匹配后，按照随机原则接受两种处理。 4 随机区组设计（randomized block design）: 数据不独立 将条件或者性质相似的受试对象配成一个区组，然后区组内每个受试对象再随机分到各个比较组，分别接受不同处理。 5 重复测量设计（repated measurement design）：数据不独立 指同一观测对象被基于某种处理后，在不同时间点上对某指标进行多次测量（M&gt;2）所得的资料。 11.1.3.2.3 3.2.3 配对检验 配对样本t检验是单样本t检验的特例。配对t检验有多种情况：配对的两个受试对象分别接受两种不同的处理；同一受试对象接受两种不同的处理；同一受试对象处理前后的结果进行比较（即自身配对）；同一对象的两个部位给予不同的处理 配对检验的特点： （1）在配对设计得到的数据中，每队数据之间都有一定的相关，如果采用成组t检验就无法利用这种关系，从而浪费大量统计信息。 （2）统计学解决方法是求出每对的差值，通过检验该差值总体均数是否为零，就可以得知量化总处理有无差异 11.1.3.3 方差分析中常用参数释义 11.1.3.3.1 3.3.1 期望和标准差 期望：概率的平均值 标准差：衡量数据的波动大小。 11.1.3.3.2 3.3.2 自由度 简单说， [公式] 个样本，如果在某种条件下，样本均值是先定的 (fixed)，那么只剩 [公式] 个样本的值是可以变化的。 ##参见知乎的【如何理解统计学中「自由度」这个概念？】的答案： # 自由度的解释参见： ## 维基百科关于自由度描述为当以样本的统计量来估计总体的参数是，样本中独立或能自由变化的数据的个数统称为为该数据量的自由度。 ## 需要注意是在其他的回答中，自由度定义是更为复杂的，定义为矩阵投影的秩。而且矩阵的自由度在定义时需要注意是统计分布中使用自由度这个概念，而不是在先验分布中使用自由度。 ## 这里作者举了一个简单的例子，但又10个数，如果知道其中9个数和平均值，那么剩下的1个数必然也是知道的。因此自由度为9，n-1.这也就意味着唯一确定的数据需要九个向量维度来决定。 11.1.4 回归分析中常用参数释义 11.1.4.1 残差 ## 理解残差： # 参见网页： https://blog.csdn.net/Noob_daniel/article/details/76087829?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromBaidu-1.control&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromBaidu-1.control # z在实际建模中，数据之间的残差是指实际值与观测值之间的差。残差蕴含相关模型的重要信息，可以将残差看做误差的观测值。 # 使用：可以查看残差的分布规律，进而通过分布规律来评估残差和原始数据之间的拟合关系。 par(marow=c(1,4)) plot(lm) 残差图的分布趋势可以帮助判明所拟合的线性模型是否满足有关假设。如残差是否近似正态分布、是否方差齐次，变量间是否有其它非线性关系及是否还有重要自变量未进入模型等。.当判明有某种假设条件欠缺时， 进一步的问题就是加以校正或补救。需分析具体情况，探索合适的校正方案，如非线性处理，引入新自变量，或考察误差是否有自相关性。 ## 线性回归模型的残差检验假定条件 线性假定：X与Y之间的关系是线性的 独立性假定：对于一个特定的x，它所对应的残差∈ \\in∈与其它x所对应的残差∈ \\in∈相互独立 正态假定:残差服从期望为0的一个正态分布 同方差假定:对于所有的x，残差∈ \\in∈的方差都是相同的 11.1.5 常用分布描述 11.1.5.1 时间分布族： 指数分布解决的问题是“要等到一个随机事件发生，需要经历多久时间” 伽玛分布解决的问题是“要等到n个随机事件都发生，需要经历多久时间” 泊松分布解决的是“在特定时间里发生n个事件的概率”。 韦伯分布是泊松分布的现实版本，泊松分布过于严苛。 11.1.5.1.1 5.1.1 指数分布 指数分布 (Exponential distribution) 用来表示独立随机事件发生的时 间间隔，比如旅客进机场的时间间隔、中文维基百科新条目出现的时间间隔 等等。许多电子产品的寿命分布一般服从指数分布。有的系统的寿命分布也可用指数分布来近似。λ是平均单位时间发生次数，如果λ越大，两次间隔时间必然越短。 指数分布是伽玛分布和 weibull 分布的特殊情况，产品的失效是偶然失效时，其寿 命服从指数分布。 描述：x由小到大，y由大到小，单向递减； 泊松过程的时间增量（即时间间隔）符合指数分布，此处的指数分布是事件的间隔时间的概率。下面这些一般符合指数分布： 婴儿出生的时间间隔 来电的时间间隔 11.1.5.1.2 5.1.2 伽玛分布 伽玛分布(Gamma)是著名的皮尔逊概率分布函数筷中的重要一员，称为皮尔逊型分布。它的曲线有一个峰，但左右不对称。假设随机变量X为等到第α件事发生所需之等候时间。 “指数分布”和“χ2分布”都是伽马分布的特例。 Gamma分布中的参数α称为形状参数（shape parameter），β称为逆尺度参数。 11.1.5.1.3 5.1.3 泊松分布 1、事件是独立事件； 2、在任意相同的时间范围内，事件发生的概率相同； 3、可以知道某个时间范围内，发生某件事情x次的概率是多大； 4、泊松分布的特点是期望与标准差的值是一致的。 日常生活中，大量事件是有固定频率的： 事件在单位时间内发生0次的概率、发生1次的概率 ... 发生∞次的概率。 假设：事件之间的平均时间是已知的，事件的确切时间是随机的。 某医院平均每小时出生3个婴儿 某公司平均每10分钟接到1个电话 11.1.5.1.4 5.1.4 韦伯分布 因为正态分布或者泊松分布过于理想化，韦伯分布相对来说更接近现实一些(从概率密度函数来看，韦伯分布一般具有长尾分布，即右偏分布的特点)。 weibull(韦伯) 分布，又称韦氏分布或威布尔分布，是可靠性分析和寿 命检验的理论基础。Weibull 分布能被应用于很多形式，分布由形状、尺度 （范围）和位置三个参数决定。其中形状参数是最重要的参数，决定分布密 度曲线的基本形状，尺度参数起放大或缩小曲线的作用，但不影响分布的形 状。Weibull 分布通常用在故障分析领域 ( field of failure analysis) 中；尤 其是它可以模拟 (mimic) 故障率 (failture rate) 持续 ( over time) 变化的分 布。故障率为：一直为常量 (constant over time)，那么 α = 1，暗示在随机事件中发生一直减少 (decreases over time)，那么 α &lt; 1，暗示 “早期失效 (infant mortality)” 一直增加 (increases over time)，那么 α &gt; 1，暗示 “耗 尽 (wear out)” 随着时间的推进，失败的可能性变大。 11.1.5.1.5 5.1.5 Gompertz分布 Gompertz分布是一种重要的连续时间的寿命分布，通过调整参数值，Gompertz分布的概率密度函数可能是单调递减或者单峰分布的。 ## 该分布是参数cox回归的一种重要分布形式： 11.1.5.2 连续概率分布 11.1.5.2.1 5.2.1 F分布 F-分布（F-distribution）是一种连续概率分布，被广泛应用于似然比率检验，特别是 ANOVA 中。F 分布定义为：设 X、Y 为两个独立的随机变量，X 服从自由度为 k1 的卡方分布，Y 服从自由度为 k2 的卡方分布，这2个独立的卡方分布被各自的自由度除以后的比率这一统计量的分布。 11.1.5.2.2 5.2.2 T分布、高斯分布(Gaussian) 应用在估计呈正态分布的总体的平均数。它是对两个样本均值差异进行显著性测试的 学生 t 检定的基础。 11.1.5.3 离散概率分布 11.1.5.3.0.1 5.3.1 二项分布/伯努利分布 案例发生是离散的。结果只有二项选择；可多次发生，称之为累计二项分布； 1、做某件事次数是固定的，用n表示 2、每一次事件都有两个可能的结果； 3、每次成功的概率都是相等的，成功的概率用p表示 4、分析者所感兴趣的是成功x次的概率是多少 11.1.5.3.0.2 5.3.2 几何分布 ## 几何是二项分布的延进模式：分析者所感兴趣的进行x次尝试后，取得第一次成功的概率； 1、做某件事次数是固定的，用n表示 2、每一次事件都有两个可能的结果； 3、每次成功的概率都是相等的，成功的概率用p表示 11.1.5.3.1 5.3.3 贝塔（Beta）分布 贝塔分布 (Beta Distribution) 是指一组定义在 (0,1) 区间的连续概率分 布，Beta 分布有 和 两个参数 α,β&gt;0，其中 α 为成功次数加 1，β 为失败 次数加 1。Beta 分布的一个重要应该是作为伯努利分布和二项式分布的共 轭先验分布出现，在机器学习和数理统计学中有重要应用。贝塔分布中的参 数可以理解为伪计数，伯努利分布的似然函数可以表示为，表示一次事件发 生的概率，它为贝塔有相同的形式，因此可以用贝塔分布作为其先验分布。 ## 简单来说： 其实描述的就是我们在做抛硬币实验的过程中，我们当前如果已经观测到 a+1次正面， b+1次反面，那么此时硬币正面朝上的真实概率的可能性分布。正如二项分布可以看做是重复进行伯努利实验所得到的分布，Beta分布可以看做是重复进行二项分布所得到的分布。 image-20220902161607901 11.1.6 其他统计参数释义 11.1.6.1 信息熵 ## 参见论文：&lt;熵 理 论 及 其 应 用&gt; 陈建珍 ,赖志娟 ## 熵最早期源于热力学的相关研究，早期的研究中普朗克、玻尔兹曼等注明物理学家都对熵系统做了深入的研究；熵就是系统处于某一宏观状态下可能性的的度量，也就是说，系统的熵越大，则系统处于该状态下概率就越大。 ## 熵增加原理：孤立系统的熵永不减少，或者任一孤立系统或绝热系统的自发发生过程总是朝着熵增加的方向进行； ## 系统的平衡态上述熵具有最大值的状态；也就是出现概率最大的状态； ## 获取信息等于消除熵，也就是将高熵降低为低熵。有一个问题是裸地是高熵还是低熵状态，评价高熵还是低熵的标准是混乱程度，显然在裸地—森林的等级演替过程中，系统的熵是始终增加的。并且在系统达到相对稳定时，达到最大熵状态(出现概率最大的状态，为何这种状态胡出现，以及这种状态是如何偏移的)。这种最大熵状态反映了群落内部的平均性结构化，此次群落内部每个物种的功能特性所扮演的功能生态位在整体群落结构中具有一致性；但也不能否认，单独物种在群落构造中所扮演功能的差异，但是功能的差异以量级来评估本身就是存在问题的，这只是我们定义的单维尺度的评估，而在符合维度尺度上，物种可能处于等价态。此外，还需要考虑到群落系统是如何维持最大熵状态下，系统的熵增过程是伴随着无序化的增加（这种无序化是如何体现的？实际上系统是逐步进行有序化的一个进程，群落演替不断发生，但伴随着群落的演替阶段的进行，群落的生产力也在不断提高，光合作用吸收外界的二氧化碳（产生高能的葡萄糖，排出氧气），同时呼吸作用分解糖释放co2和水分，释放），系统以负熵为食(有序的，如太阳光) ## 关于信息熵的定义实际上和概率值的求解基本一致： ## 所有事情概率均等发生时，log2M ## 一般分布：累积和(pilog2pi-1)，其中pi表示事情每次发生个概率，因为事情发生的概率会 随着抽样结果(剩余因素)的改变而改变。 11.1.6.2 最大熵 ## 实际上最大熵模型是本质上 贝叶斯理论的一种延伸范式，和信息熵理论相结合； ## 具体解释：参见： https://blog.csdn.net/qq_16137569/article/details/82560785 ## 最大熵理的两个理论前置假设： 第一个是系统在平衡状态下，熵最大；（## 还未参那篇论文关于最大熵的应用生态学解释） 第二个是如果有约束条件，最大熵原理也可以表述为在满足约束条件的模型集合中选取熵最大的模型。 ## 在统计分析时接近均匀分布的概率具有较高的熵。当且仅当X的分布是均匀分布事上式右边的等号成立，所以X服从均匀分布时熵最大。 ## 将上面的结论和最大熵模型进行结合分析，实际上在评估生态系统时，使用最大熵模型和中性定理的本质是类似的。在均匀分布的背景假设下，物种的实际分布是存在约束偏倚的；这种约束偏移本身对均匀分布进一步修正，这个结果反映是在修正背景环境约束下，物种在所有受到约束的背景环境中处于平均分布状态，这种平均分布状态本身就是对物种的保守型的估计，这种保守型说明物种在边缘种群和核心种群(## 或者说集合种群)中所处的环境条件异质性状态会在某些环境组合下处于均质化状态。 上面这种均质化状态，又是相对特殊的。均质化反映的是物种在所处群落环境中保留某一特殊状态的下的偏好，这种偏好性是存在波动和偏差以及局域适应性差异的。这种适应性差异本身和群落本身存在构造对应、发育对应关系。如何将这种关系定量的体现出现，将对于解释生态位理论在物种共存中将起到重要解释作用； 但最大熵理论在解释物种共存态时，本身存在的一个问题是它是否有足够的解释度来阐释系统熵的问题——这里的系统熵，需要的不仅仅是局域、景观、区域以及全球生态系统本身的架构，还需要构建“熵流”。熵流的方向和大小本身也是需要定义的；##这里可能需要重新去理解和认识邬建国老师的论文中关于熵耗理论的认识； 此外，需要理解的是最大熵在构建过程中是否考虑到种群的构建过程，这或许也是为什么种群建模的假设过程中，种群是处于平衡态的原因。但是 种群内往往是处于不平衡状态的，特别是对于先锋种群来讲，如何理解先锋种群的建模结果也是非常重要的。 此外，关于建模过程中存在的若干问题，还需要理解的是关于那篇文章的相关理解； 种群的均质化反映是内在的，还是外在的，也是需要确认的；内在所谓的就是物种本身的生理特性所决定物种的均一化生境需求，而外在是在种群构建过程中，外在环境的给与提供了合适的生境供其生长。此外，这种内在和外在之间的联系对于，不同分布区的物种是否存在特异性差异？这种差异性的来源是什么？需要进一步的分解问题、、、分解到基本 单元再去解释。 学而有崖，知而无崖； 11.2 临床研究中常用的统计方法和常见问题 ## 资料来源： 1、《临床研究中常用的统计方法和常见问题》 11.2.1 统计推断与区间估计 ## 统计推断： 由样本信息对相应总体的特征进行推断： 参数估计： 根据样本的统计量估计总体参数的过程：点估计和区间估计； （如果区间包含原假设值 --&gt; 接受原假设；反之则相反） 假设检验： 对所估计的总体首先提出一个假设，然后通过样本数据去推断是否拒绝这一假设；包括参数检验和非参数检验；关键是用P值衡量数据与关于参数的原假设的差异程度； 总体参数：总体的某些数据值特征 样本统计量：根据样本计算得出的某些数据值特征；---例如均值、四分位数等等； 区间估计：基于数据推断对应参数的一个可能区间 如果区间包括原假设值--&gt;接受原假设（无法拒绝原假设） 如果区间不包括对应原假设值---&gt;拒绝原假设（无充足理由接受备择假设） ## 为什么要做统计推断： 因为需要从全局的范围，也即从总体上对问题作出判断。不可能也不允许对研究总体的每一个个体均做观测； ## 参数估计： - 点估计： 点估计：直接以样本统计量作为相应总体参数的估计值； 优点：方法简单； 去诶单：没有充分利用样本信息，不能反映抽样误差的影响； ## 参数估计： -- 区间估计 区间估计： 在给定置信度下（如95%），采用样本统计量估计总体参数的可能范围； 置信区间：根据样本均数计算出有（1-@）把握包含总体均数的一个数值范围，这个数值范围成为总体均数的置信区间，该（1-@）称为置信度； ## 总结来说： 假设检验是对检验样本与未知总体分布做的检验评估，但数据形式多样性，在此基础上可用点估计或者区间估计两种方法来间接评估检验样本与未知总体分布的关系。 11.2.2 统计方法选择： 11.2.2.1 统计方案的目的 统计描述：集中趋势和离散趋势； 差异性分析：比较组间均数、率和中位数等的差异； 相关性分析：分析两个或者多个变量之间的关系 影响性分析：分析某一个结局发生的影响因素（如线性回归、逻辑回归和生存分析等） 11.2.2.2 统计方法选择 包含分析目的、资料类型和研究设计三个组分； 分析目的：统计描述/推断？比较组别间差异? 以及计算变量的关联？ 资料类型：数值/分列/生存资料？ 是否服从正态？是否为等级资料？ 研究设计：数据是否独立？样本量大小？几组（因素和水平）？ 结局（outcome） 暴露因素 单因素 多因素 检验效应 分类变量 分类变量 卡方检验 逻辑回归 OR, 95%CI 连续变量 T检验 逻辑回归 OR, 95%CI ROC分析 AUC,95% CI 连续变量 分类变量 卡方检验 多元线性回归 beta, 95% CI 连续变量 回归分析 协变量、多元回归 截距值、95% CI time to event 分类 Kaplan meier Cox 回归 HR，beta，95% CI 除此以外，效应的估计还可以基于相对危险度（relative risk,RR）进行计算。包括危险度比（risk ratio）:暴露组的危险度（累积发病率）与对照组之比；率比（rate ratio ，RR）暴露组的发病密度与对照组之比。 分析指标 常用模型 血糖变化值 协方差分析（一般线性模型） 呕吐次数 Poission回归模型 改善率 Logisitic回归模型 OS,PFS Cox比例风险模型 image-20220401105717706 image-20220401105733530 image-20220401110131876 image-20220401111126993 image-20220401111235730 11.2.2.3 描述性统计的讨论 定性资料 对于定性变量（包括二分类变量、顺序变量和名义变量），一般采用频数和百分比描 述，其中等级资料可采用中位数和四分位间距进行描述[13]。 定量资料 先进行正态性检验，如果变量近似正态分布一般采用均数和标准差，偏态分布需采用中位数和四分位间距[14]。 临床研究的主要结局： 对主要结局指标通常会采用多种形式进行综合描述：均值、标准差、中位数、最小值 和最大值或范围、变异系数。 包含时间的生存资料： 对于小样本或大样本未分组的生存资料常采用Kaplan-Meier法，而对例数较多的分组资 料采用寿命表法，对生存时间进行中位数及上下四分位间距的统计描述，同时绘制生存曲线直观反映生存情况。 11.2.2.3.1 一元定性资料 单组设计：常用于样本率的参数与总体的已知率之间差异性检验，主要适用于一些无法设计对照组。 该资料的统计主要利用二项分布原则和总体进行比较，大样本（N&gt;30）采用按近似正态分布基于Z分布进行计算，小样本（n&lt;30）时采用采用Clopper-Pearson精确法或Blyth-Still的二项式比例计算。 成组设计：若响应变量是二分类，则构成常见的四格表。 在横断面研究中根据不同的条件选择卡方检验或Fisher精确检验，评价组间构成比的差异。在病例-对照研究和队列研究的四格表可用于计算OR和RR及其可信区间，同时采用Mantel-Haenszel卡方对OR和RR进行统计检验。 在临床试验中，常用于安全性指标（不良事件是否发生）发生率的组间评价，样本例数较少时，直接指定Fisher精确检验。 若响应变量是多值有序变量，可采用Wilcoxon秩和检验（两组）和Kruskal-Wallis检验（两组以上）。 若行和列的属性构成配对结构时，可用McNemar检验或Kappa检验对一致性分布进行定性和定量的检验。 若分组变量为有序多分类，而响应变量为二分类时，除了可用卡方检验外，还可使用Cochran-Armitage趋势检验来检验率和有序多分类变量之间是否存在线性趋势 11.2.2.3.2 一元定量资料 单组设计： 与定性资料应用范围类似，但样本估计的参数是均值和中位数，需要根据正态性检验的结果选择统计方法。当符合正态分布时，采用单样本T检验，均值的可信区间基于正态分布计算。不满足则使用秩和检验中位数的可信区间基于非参方法计算。 成组设计： 若两组定量资料是配对关系，则计算差值后采用单组设计的统计方法。 若组间的资料独立，则需根据各组的正态分布检验和方差齐性检验选择独立样本的t检 验和Wilcoxon秩和检验（两组），或方差分析和Kruskal-Wallis检验（两组以上）。 对于两组以上的检验，如果差异有统计学意义，可能需要根据研究设计选择合适的方法进行事后的两两比较分析。呈现结果时，需要呈现差异值（均值或中位数）及其95%可信区间。t检验和方差分析可根据t分布或正态分布进行可信区间的估计。而非参数检验不基于概率分布，差值中位数的可信区间估计需采用Hodges-Lehmann估计或bootstrap估计。 11.2.2.4 相关分析和回归分析 在临床研究中常用的回归分析是多重线性回归（连续型变量）、Logistic回归 （二值变量、多值有序变量和多值名义变量，二项分布）、Poisson回归（计数型变量，Poisson分布）、负二项回归（计数型变量，负二项分布）和COX模型回归（生存资料），在实际应用中根据不同的结局指标类型选择合适的回归模型。 协方差分析： 在研究干预效果时，基线作为特殊的协变量必须要纳入考虑，协方差分析是回归分析和方差分析的结合，扣除基线值对因变量的影响之后，再研究分组的修正均值的差别，该方法在多重线性回归中实现。 在干预型临床试验中一般会设立对照组并进行试验前后的测量，协方差分析仅能有效检验试验后组间的差异，不能用差异来衡量干预效果， 双重差分方法使用越来越广泛[20]，其思想是实验组前后的变化值减去对照组前后变化值得到真正的干预效应。 11.2.2.5 生存资料分析 生存分析不仅需要分析感兴趣的重点事件是否发生，还要考虑到达终点时经历的时间长短。 其中生存曲线的组间比较常采用Log-rank检验（对远期差异敏感）和Wilcoxon检验（对近期敏感）。 一般可分为参数模型的回归分析和半参数COX回归分析。若确定生存资料服从某种特定的分布（包括Weibull 分布、指数分布、对数正态分布或者Gamma分布等），需使用相应的参数模拟拟合。若无法获得生存资料数据分布时，可采用COX等比例风险模型，其不依赖特定分布的特点，在随访研究中得到非常广泛的应用。 COX模型的使用需要满足风险等比例的前提假设，对分类协变量可检验生存曲线是否交叉，对连续协变量需拟合偏残差与生存时间的关系。 一般的生存资料假定受试者在随访时间内最多经历一次随访事件，然而受试者可能经过 多次相同或类似的结局事件（复发），针对该类生存资料需要采用Anderson-Gill强度模型，该模型假定每次事件类型相同且相互独立。 11.2.2.6 重复测量数据 所谓重复测量数据是指在纵向资料中可能会对结局指标进行多次测量，构成重复测量的数据。由于该数据的 非独立性，不满足一般回归的前提假设，常用的统计方法有重复测量的方差分析、混合效应模型和广义估计模型。 重复测量的方差分析思想是总变异分解成个体内的变异和个体间的变异，需要满足正态性、方差齐性和球形的前提假设。但数据中存在缺失时，分析会将存在的研究对象全部删除，这会降低有效样本量的大小。 混合效应模型和广义估计模型采用纵向数据格式，能有效利用样本的信息，根据不同的协方差矩阵结构保证分析结果更加准确保守，同时能对时变因素进行多重比较。所以针对重复测量数据应主要采用混合效应模型和广义估计模型。 混合效应模型考虑随机效应，是对个体水平值的估计，而广义估计模型主要考虑固定效应，是对总体平均水平值的估计，在分析时根据需求选择 连续变量的一致性评价： ### 连续变量的一致性评价 我们常需要考察连续变量之间的一致性，连续变量之间，只有属性相同，才可以做一致性分析。比如，不同方法测量结果的一致性，多次测量结果的一致性，不同研究者评分的一致性。研究者在解决这类问题时，常用的方法包括配对t检验、相关分析、组内相关系数等。采用这些方法进行一致性分析，是否恰当呢? - t检验： 做出统计结果，并不一定有临床统计意义； - 相关分析： 只有相关性，无因果性 - 回归法：比如最常见线性回归，可建立两测量结果之间的线性方程，y=a bx。如果截距a与0无差异，斜率b与1无差异，则可认为两测量结果一致。如图，橙线为理想的情况y=x，蓝线为建立的线性方程y=0.939x-0.135。Bootstrap获得截距的区间估计为 -0.356~0.039，包含0;斜率的95%区间估计为0.885~1.010，包含1，可认为两测量结果一致。‍需要注意的是需要先判定数据是否可以使用置信区间方法进行分析。 - **组内相关系数‍** 组内相关系数(Intraclass correlation coefficient，即ICC)，主要用于考察定量资料的一致性。组内相关系数介于0和1之间，越接近1，说明重复测量之间的差异越小，即重复测定的一致性越好。组内相关系数是研究对象间变异占总变异的比例，当测量结果范围较小，即重复测量中，各测量结果均较接近，此时ICC的计算就与试试不太相符了。这说明，ICC的应用受到测量结果范围的影响，比较适用于不同研究对象结果差别较大的情况。 - **5.Bland-Altman法‍** Bland-Altman法最早由Bland和Altman在1986年提出。绘制两测量值差值D对应于均值A的散点图，即Bland-Altman图。如果D和A是独立的，当大部分测量值落在差值的95%参考值范围内，且此范围在专业上也可认为是一致的，则认为两测量结果一致。如图，横坐标为两测量结果的均值，总坐标为两测量结果的差值，大部分测量结果均落在差值的95%参考值范围内，可说明两测量结果一致性较好。当然，还需要考察差值的95%参考值范围在专业上是否可以被认为是一致的。 11.2.2.7 正交设计 11.2.2.7.1 试验设计的基本概念 1、因素和试验指标并非成因果关系，而是相关关系。 2、因素水平： 因素在试验中所处的各种状态或所取的不同值，称为该因素的水平，也称为水平或位级。 因素可以包含具体指、状态 3、组合处理：所有试验因素的水平组合所形成的试验点称为处理组合。 以2个二水平因素的试验为例： 表示为 两个变量，每个变量有两个水平。 4、全面试验：对全部组合处理都进行试验。表示为试验各因素水平的乘积； 5、存在全面试验和部分试验的矛盾： 矛盾来自于：全面试验的组合处理多，但希望进行少数试验，即进行正交试验设计；另一方面，实施少数试验希望能够获取全面的试验信息，则称之为科学处理试验结果。 6、因素试验：研究各因素对试验指标的影响，了解各因素的重要程度，并直接获得最优组合处理，或者求得回归方程的试验。 7、因素试验可以为根据试验目的、因素数量和试验时间进行划分。其中实验目的可以划分为验证性试验和探索性试验，因素数量可以划分为单因素试验和多因素试验，试验时间又可以划分为同时试验和序贯试验。 同时试验适合长期实验设计，多个项目同时进行。序贯设计是指下一次试验必须在上一次的实验基础之上。 11.2.2.7.2 正交设计方法 最简单的正交表是 ，含意如下：“L”代表正交表；L 下角的数字“4”表示有 4 横行，简称行，即要做四次试验；括号内的指数“3”表示有3 纵列，简称列，即最多允许安排的属性是3 个；括号内的数“2”表示表的主要部分只有2 种数字，即每个属性有两种水平1与2。正交表的特点是其安排的试验方法具有均衡搭配特性。也就是说，3个属性，每个属性有2个水平的选择实验采用全因子设计需要23=8个备选方案，而采用正交设计只需要4个。当然这是非常简单，差异并不明显，但是如果属性和水平都比较大，二者的差异就会非常明显，显然能够大大减少工作量。因而正交实验设计在包括选择实验在内的很多领域研究中已经得到广泛应用。正交设计的数量可以用以下公式来计算： n=c×s-c+1 其中n为正交设计的数量，即正交表中的行，c为属性的数量，即正交表中的列，s为属性水平的数量。 两种主要设计： 1、等水平正交表：指所有因素均有多个相同水平数量的因素 2、非等水平正交表：指部分因素有多个水平 设计方原则： （1）每一列中，不同的数字出现的次数相等。例如在两水平正交表中，任何一列都有数码“1”与“2”，且任何一列中它们出现的次数是相等的；如在三水平正交表中，任何一列都有“1”、“2”、“3”，且在任一列的出现数均相等。 （2）任意两列中数字的排列方式齐全而且均衡。例如在两水平正交表中，任何两列（同一横行内）有序对子共有4种：（1，1）、（1，2）、（2，1）、（2，2）。每种对数出现次数相等。在三水平情况下，任何两列（同一横行内）有序对共有9种，1.1、1.2、1.3、2.1、2.2、2.3、3.1、3.2、3.3，且每对出现数也均相等。 以上两点充分的体现了正交表的两大优越性，即“均匀分散性，整齐可比”。通俗的说，每个属性的每个水平与另一个属性各水平各碰一次，这就是正交性。 11.2.3 统计分析中的常见问题 11.2.3.1 控制1类错误和2类错误 11.2.3.1.1 1和2类错误的概念定义： 一般来说，一类错误就是：没有效应的时候，统计结果却显示p &lt; 0.05，也叫假阳性；二类错误就是有效应时却显示p &gt; 0.05，也叫假阴性。 控制1类错误，也称之为控制阿尔法（总1类错误率，Familywise Erorr Rate,FWE); 控制2类错误，也称之为控制贝塔；，其中1-贝塔= power，也称为效力； 11.2.3.1.2 控制错误的原因： image-20220401110033947 控制一类错误，最主要的就是控制多重比较带来的假阳性率膨胀。有些多重比较显而易见，比如同一自变量有三个水平，两两比较就产生了3次比较，这种情况下需要校正。另外一个比较隐性的会带来假阳性率膨胀的是在收集数据时，边收集边分析，达到统计显著之后就停止收集。这种边收集边分析实际上是可以让一切实验都变得显著起来，包括Bem (2011)年的那个实验就有这种可能。 上面讲的这个实验是一个试验分析过程中，数据边分析边处理；统计人员就发现在处理一定阶段后，数据结论迅速变得显著。往往临床统计实验会到此阶段结束，这可能会导致第一类错误的发生。而实际上随着实验进展持续补充，可以发现显著性结论就会逐渐消失。 ## 1类错误发生的场景？应该考虑 每一项临床试验的研究目的都可以抽象为一个或多个检验假设。当检验假设为多个时，就要考虑@的控制问题。 多个主要疗效指标组间比较问题； 多个组间比较问题 多个时点组间比较（期中分析） 亚组内组间比较问题？ ## 什么时候不需要考虑阿尔法的控制？ 如果临床试验是单臂或双臂设计、使用单个主要指标、事先只指定了一个与主要指标相关的原假设且在一个时间点上进行统计推断，则无须考虑α的控制问题。 ## 补充说明： 数据不足所导致的1类错误，还可能会导致变量间存在共线性关系。反过来说即为变量间的共线性关系可能会导致1类错误的发生； 11.2.3.1.3 解决多终点问题的统计学考量 ## 控制1类错误的检验方法： 1、闭锁检验法（closed testing） 2、gate-keeper/ 固定顺序法 3、阿尔法分配法 ## 控制二类错误： 增加样本量、提高试验质量 理清思路–最关键 参考资料：https://www.bilibili.com/video/BV1Rq4y1g7g8?from=search&amp;seid=1353280413434012036&amp;spm_id_from=333.337.0.0 项目 没有拒绝H0 拒绝H0 H0为真 正确决策（1-α） I类错误 α H0为假 II类错误（β）取伪错误 正确决策1-β ## 核心概念： 本质上假设检验是抽样分布对总体分析的一次检验评估； 其中原始总体分析成为u0，抽样分布为u1； 1、情况1；正确决策（1-α）： 样本落在非拒绝域，此时总体分布u0 = 抽样分布u1，支持原假设成立。 2、情况2：发生I类错误α ： 造成此种情况的原因在于抽样具有随机性，因此当抽样的分布落在拒绝域时（α的概率），此时就会产生统计错误（I类错误）。 这种情况比较容易发生在多重检验假设中，因此在多重检验假设中数据被随机分组和重抽样的概率远大于整体重抽样的结果。 3、情况3：发生II类错误（β）： 原假设不成立情况为真（备择假设成立），此时如果抽样分布的数量受限（并未对真实分布的全部进行随机抽样，而是造成抽样偏差），那么就有可能u1分布结果中的数据期望落在了uo的非拒绝域，从而造成统计错误（II类错误）。此时，发生二次错误的概率为β。 4、情况4：正确决策（1-β）： 与情况3的情况相反，原假设为假，此时备择假设为真，那么u1分布中的随机抽样情况就有1-β的概率落在u1分布中，在此种情况下会严哥拒绝原假设。其中1-β称之为power，也称功效或检出力或把握度。 ### 不同决策参数之间的关系： 1、 功效（1-β）随着 α的升高而上升； 2、 u1-u0上升，功效1-β也随着上升； 3、样本量（n）增加，功效1-β也随着上升； 11.2.3.2 多重共线性检验 1、**模型参数估计不具备可解释性**，有时甚至会出现回归系数的符号与实际情况完全相反的情况，比如逻辑上应该系数为正的特征系数 算出来为负。 2、**本应该显著的自变量不显著，本不显著的自变量却呈现出显著性**（也就是说，无法从p-值的大小判断出变量是否显著——下面会给一个例子） 一个更直观的例子，在使用gbdt的时候，对feature importance top1的features进行多次repeat，下一次训练的时候，feature importance中这个feature的 3、**多重共线性使参数估计值的方差增大，模型参数不稳定，也就是每次训练得到的权重系数差异都比较大**。 其实多重共线性这样理解会简单很多: 假设原始的线性回归公式为： y=w1*x1+w2*x2+w3*x3 训练完毕的线性回归公式为： y=5x1+7x2+10x3, 此时加入一个新特征x4，假设x4和x3高度相关，x4=2x3,则 y=w1*x1+w2*x2+w3*x3+w4*x4=w1*x1+w2*x2+(w3+2w4)*x3 因为我们之前拟合出来的最优的回归方程为： y=5x1+7x2+10x3 显然w3+2w4可以合并成一个新的权重稀疏 w5，则 y=w1*x1+w2*x2+w5*x3,显然： y=w1*x1+w2*x2+w3*x3和y=w1*x1+w2*x2+w5*x3是等价的。。。。 那么最终最优的模型应该也是 y=5x1+7x2+10x3 但是考虑到引入了x4，所以w4和w3的权重是分开计算出来的，这就导致了 w5=10=w3+2w4，显然这个方程有无穷多的解，比如w3=4，w4=3，或者w4=-1，w3=12等，因此导致了模型系数估计的不稳定并且可能会出现负系数的问题。 ### 补充 数据不足所导致的1类错误，还可能会导致变量间存在共线性关系。反过来说即为变量间的共线性关系可能会导致1类错误的发生； 11.3 差异分析 11.3.1 方差检验与T检验的选择 ### 区别： ## 方差检验： 方差分析是R.A.Fisher发明的，用于两个及两个以上样本均数差别的显著性检验。 方差分析主要用途是均数差别的显著性检验，分离各有关因素并估计其对总变异的作用，分析因素间的交互作用，方差齐性检验。 ## T检验： t检验是戈斯特为了观测酿酒质量而发明的，并于1908年在Biometrika上公布。 ### 联系： 两者都要求比较的资料服从正态分布；而且两样本均数的比较及方差分析均要求比较组有相同的总体方差；配伍组比较的方差分析是配对比较t检验的推广，成组设计多个样本均数比较的方差分析是两样本均数比较t检验的推广； 11.3.2 Z 分布、T检验、F检验和卡方检验 11.3.2.1 假设检验方法的选择 ## 根据总体方差是否已知和样本容量大小（30）： 当样本总体方差已知 或者 方差未知但样本容量较大时，使用Z检验； 当样本总体方差未知，且 样本容量较小时，使用t检验； ## F检验主要适用于检验两个样本分布的方差是否相同： 比较两组数据的方差，以确定他们的精密度是否有显著性差异 在进行双独立样本T检验时，对于两组小样本，需进行F检验以测试其方差齐性 ## 卡方检验： 卡方检验属于非参数检验，主要是比较两个及两个以上样本率（构成比）以及两个分类变量的关联性分析，其根本思想在于比较理论频数和实际频数的吻合程度（偏离期望）或者拟合优度问题。 11.3.2.2 各分布介绍 11.3.2.2.1 Z分布的介绍 Z分布即为正态分布（normal distribution）。 ## 参数分布： 正态分布的两个参数μ和σ决定了正态分布的位置和形态。为了应用方便，常将一般的正态变量X通过u变换[(X-μ)/σ]转化成标准正态变量Z，以使原来各种形态的正态分布都转换为μ=0，σ=1的标准正态分布（standard normaldistribution）,亦称Z分布。 根据中心极限定理，通过抽样模拟试验表明，在正态分布总体中以固定 n 抽取若干个样本时，样本均数的分布仍服从正态分布，即N（μ，σ）。所以，对样本均数的分布进行Z变换，也可变换为标准正态分布N (0,1) ## 适用条件： 正态分布 总体标准差已知或者样本容量足够大(&gt;30) ## 用途 检验一个样本平均数与一个己知的总体平均数的差异是否显著 检验来自两个的两组样本平均数的差异性，从而判断它们各自代表的总体的差异是否显著 z.test(x,y,alternarive =&quot;two.sided&quot;,mu=0,sigma.x =sd(x), sigma.y=sd(y),conf.level=0.95) 11.3.2.2.2 T分布 ## 参数分布： 由于在实际工作中，往往σ(总体方差)是未知的，常用s（样本方差）作为σ的估计值，为了与Z变换区别，称为t变换，统计量T值的分布称为T分布。 ## 适用条件： 独立性、正态性或近似正态、方差齐性（两小样本所对应的两总体方差相等,一般用F检验） 当样本例数较小时，要求样本取自正态总体；（当样本数少于30时，需要检验满足正态分布，若数量较多，根据中心极限定律，样本会趋向正态分布） ## 用途： 样本均数与群体均数的比较看差异是否显著； 两样本均数的比较看差异是否显著。 ## 三种类型条件： 单样本T检验：检测样本均值与总体均值之间的差异 配对样本T检验：检验样本某个状况前后的均值有无差异 双独立样本T检验：检测两组样本均值有无差异（需保证两组小样本的方差齐性） 11.3.2.2.3 F分布： ## 适用条件： 总体均值未知 样本来自于正态总体 ## 用途 比较两组数据的方差，以确定他们的精密度是否有显著性差异 在进行双独立样本T检验时，对于两组小样本，需进行F检验以测试其方差齐性 var.test(x, y, ratio = 1, alternative = c(&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;), conf.level = 0.95, ...) [TOC] 11.3.3 T检验 11.3.3.1 t检验的统计前提 关于T检验的知识原理参见： t检验的适用条件：已知一个总体均数；可得到一个样本均数及该样本标准差；样本来自正态或近似正态总体。 https://zhuanlan.zhihu.com/p/138711532?from_voters_page=true 1、两个样本需来自正太总体； 2、两个样本的方差需相等（方差齐性）。 对于第一个条件，我们可以用一些test来检验其正太性，比如Shapiro-Wilk test，Kolmogorov-Smirnov test，或者用Q-Q plot；而方差齐性可通过F-test， Levene&#39;s test， Bartlett&#39;s test 等来检验。 3、一般来说，我们还希望两样本数量级别趋于一致。 11.3.3.2 单样本t检验 ## 检验mtcars中wt所对应的这批汽车的平均重量是否等于3 View(mtcars) attach(mtcars) mean(wt) ## [1] 3.21725 t.test(wt,mu=3)#双边t检验 ## ## One Sample t-test ## ## data: wt ## t = 1.256, df = 31, p-value = 0.2185 ## alternative hypothesis: true mean is not equal to 3 ## 95 percent confidence interval: ## 2.864478 3.570022 ## sample estimates: ## mean of x ## 3.21725 #假设我们先验得到这批汽车的重量不可能小于3，那么到底是等于3还是大于3呢，可以进行单侧检验 t.test(wt,mu=3,alternative = &quot;greater&quot;)#原假设相等，备择假设大于3 ## ## One Sample t-test ## ## data: wt ## t = 1.256, df = 31, p-value = 0.1092 ## alternative hypothesis: true mean is greater than 3 ## 95 percent confidence interval: ## 2.923979 Inf ## sample estimates: ## mean of x ## 3.21725 注： 1.理论上，t test 依赖于正态性假设 2.但大多数情况下，一些抽样的总体不服从正态分布，由中心极限定理可知，大样本的t统计量也会近似服从正态分布 3.样本量具体多大才符合要求，传统答案是30。但真正不是靠经验推断，而是看总体的Skewness(衡量总体分布的斜度) （ref:How large does n have to be for Z and T intervals） 4.Z test,基于总体方差已知，一般无现实意义。现实中多是由样本参数去估计总体参数。 5、Z检验（Z Test）又叫U检验。由于实际问题中大多数随机变量服从或近似服从正态分布，U作为检验统计量与X的均值是等价的，且计算U的分位数或查相应的分布表比较方便。通过比较由样本观测值得到的U的观测值，可以判断数学期望的显著性，我们把这种利用服从标准正态分布统计量的检验方法称为U检验（U-test）。 11.3.3.3 双样本t检验 # 例如检验mtcars中手动挡的车和自动档的车重量是否有差异） wt_am = wt[am==0] wt_mn = wt[am==1] t.test(wt_am,wt_mn)#Welch Two Sample t-test,基于两种车方差不相等 ## ## Welch Two Sample t-test ## ## data: wt_am and wt_mn ## t = 5.4939, df = 29.234, p-value = 6.272e-06 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.8525632 1.8632262 ## sample estimates: ## mean of x mean of y ## 3.768895 2.411000 t.test(wt_am,wt_mn,var.equal = T)#基于两种车方差相等 ## ## Two Sample t-test ## ## data: wt_am and wt_mn ## t = 5.2576, df = 30, p-value = 1.125e-05 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.8304317 1.8853577 ## sample estimates: ## mean of x mean of y ## 3.768895 2.411000 注：后者依赖于两种车型方差相等假设，power（达到预期建设检验效果的可能性。例如这两种车型的重量存在差异是真实的，那我们就要达到拒绝原假设，接受备择假设的预期目的，以辅助形成我们实验的预期结论）可能会更高 。但一般不推荐，因为同方差检验未必是对的。 sample size determination based on power function(基于功效函数决定样本量) 功效函数：给定真实参数值和样本量，假设被拒绝的概率 由此反推，如果给定真实参数值(现实中可以通过小样本对总体均值和方差进行粗估计)和期望的假设被拒绝的概率，样本量需要多少？ power.t.test(n=32,delta = abs(3.5-3),sd=sd(wt),sig.level = 0.05,power = NULL,type = &quot;one.sample&quot;,alternative = &quot;one.sided&quot;)#3.5为真实总体均数，3为假设检验均数，delta为真实均数与原假设均数的差 ## ## One-sample t test power calculation ## ## n = 32 ## delta = 0.5 ## sd = 0.9784574 ## sig.level = 0.05 ## power = 0.8813668 ## alternative = one.sided power.t.test(n=32,delta = abs(3.2-3),sd=sd(wt),sig.level = 0.05,power = NULL,type = &quot;one.sample&quot;,alternative = &quot;one.sided&quot;)#different(delta)越大，n越大，power越大 ## ## One-sample t test power calculation ## ## n = 32 ## delta = 0.2 ## sd = 0.9784574 ## sig.level = 0.05 ## power = 0.3036989 ## alternative = one.sided #反过来： power.t.test(n=NULL,delta = abs(3.2-3),sd=sd(wt),sig.level = 0.05,power = 0.9,type = &quot;one.sample&quot;,alternative = &quot;one.sided&quot;) ## ## One-sample t test power calculation ## ## n = 206.3315 ## delta = 0.2 ## sd = 0.9784574 ## sig.level = 0.05 ## power = 0.9 ## alternative = one.sided 如果是双样本，标准差建议使用双样本标准差的平方平均 power.t.test(n=NULL,delta = abs(1-0),sd=sqrt(var(wt_am)+var(wt_mn)),sig.level = 0.05,power = 0.9,type = &quot;two.sample&quot;,alternative = &quot;two.sided&quot;)#0为原假设两样本无差异 ## ## Two-sample t test power calculation ## ## n = 21.70693 ## delta = 1 ## sd = 0.9924804 ## sig.level = 0.05 ## power = 0.9 ## alternative = two.sided ## ## NOTE: n is number in *each* group 11.3.3.4 配对t检验 为了提高功效，在实验条件允许的情况下，我们可以采集配对数据，每对数据的其他协变量相近（控制变量法的统计版本），此时不应使用之前的t test，会导致检验结果过于保守（too conservative,p值变大。换句话说，可能存在差异，但是用t test检验不出来） 因为两组样本之间存在强相关性，从而带来更高的precison(精准) and power 例如检验两次模拟考试题的难度，统计8个学生的两次成绩 # （在试验设计时配对设计决定了能否用配对t检验） x=c(113,120,138,120,100,118,138,123) y=c(138,116,125,136,110,132,130,110) cor(x,y) ## [1] 0.2460957 t.test(x,y,paired = T) ## ## Paired t-test ## ## data: x and y ## t = -0.65127, df = 7, p-value = 0.5357 ## alternative hypothesis: true mean difference is not equal to 0 ## 95 percent confidence interval: ## -15.628891 8.878891 ## sample estimates: ## mean difference ## -3.375 t.test(x,y) ## ## Welch Two Sample t-test ## ## data: x and y ## t = -0.56597, df = 13.855, p-value = 0.5805 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -16.177442 9.427442 ## sample estimates: ## mean of x mean of y ## 121.250 124.625 11.3.4 方差分析 11.3.4.1 方差检验的理论基础： 常用采用的分析方法就是方差分析（ANOVA，analysis of variance），这是由英国统计学家R.A.Fisher首创，以F命名，故方差分析又称为F检验。 # 变异是方差分析的基本思想 因为数据越多，变异程度就越大，为了解决这个问题，就需要用变异除以自由度（例数-1），这样比较的就是平均的变异，因此方差分析中就出现了均方（MS）和组内均方的概念。 组间均方/组内均方就是通常所说的F值，实际上代表了这样一个含义：如果组间变异远远大于组内变异，那么组间均方除以组内均方的值肯定很大，反之，这一值就会很小。但是，到底大到什么程度才认为有统计学意义呢，那就得根据F分布来判断。 ## anova计算过程: 第一步，计算组内、组间、随机因素的数据差异程度； 第二步，计算组间数据差异程度占观测变量（因变量）数据总差异的比例，与组内差异、随机因素的差异占比之间的大小关系。 SST=SSA+SSE，SST为观测变量总离差平方和，SSA为组间离差平方和，SSE为组内离差平方和。 ## f统计量： F = MS组间/ MS组内 ## 应用条件： ①各样本是相互独立的随机样本，均来自正态分布总体； ②相互比较的各样本的总体方差相等，即具有等方差齐性； ③每个样本之间都是独立的。 11.3.4.2 数据正态检验、非参检验和事后检验 11.3.4.2.1 事前检验 ## 变量统计分析及可视化： ############### 数据摘要统计： sdmdata %&gt;% group_by(class) %&gt;% ## 不包括p值以外的其他参数均包括在内； get_summary_stats(BIO10, type = &quot;common&quot;) ################ 数据正态检验： #正态分布检验：用shapiro.test() shapiro.test(sdmdata$BIO10) #方差齐性检验：用bartlett.test()或者leveneTest() bartlett.test(BIO10~class,data=sdmdata) #巴雷特检验 # 正态性检验和方差齐性检验，p值都小于0.05，所以不符合正态性，方差不齐，采用非参数检验 ## 或者使用： library (car) #conduct Levene&#39;s Test for equations of variances leveneTest(weight_loss ~ program, data = data) 11.3.4.2.2 参数及非参数检验： ################ 参数检验： result &lt;- aov(value~variable,data=anova1) summary(result) #fit the one-way ANOVA model ## var.equal=TRUE满足方差齐性的需要： result2 &lt;- oneway.test(value~variable,data=anova1,var.equal=TRUE) # 各水平的总体方差不相等(var.equal=FALSE)，则使用Welch的近似方法 result2 &lt;- oneway.test(value~variable,data=anova1,var.equal=FALSE) ############## 非参数检验： # 使用kruskal.test进行非参数检验： kruskal.test(DEPDC1 ~ molecular_group, data=myeloma) # 从Kruskal-Wallis检验的输出中，我们知道组之间存在显着差异，但是我们不知道哪些组对是不同的。 library(rstatix) # games_howell_test，适合方差不齐： ht &lt;- sdmdata %&gt;% games_howell_test(BIO10~class,conf.level = 0.95,detail=T) # 邓恩检验：多适用秩排序数据 pwc &lt;- sdmdata %&gt;% dunn_test(BIO10~class, p.adjust.method = &quot;fdr&quot;) # nemenyi.test：多适用秩排序数据 pwc &lt;- sdmdata %&gt;% nemenyi.test(BIO10~class, p.adjust.method = &quot;tukey&quot;) ############ 事后模型变量残差检验： ## qq图： # 理想情况下，标准化残差将沿着图中的直线对角线下降。 # 然而，在上图中，我们可以看到残差在开始和结束时偏离了这条线。 # 这表明我们的正态性假设可能被违反。 plot(model) residual &lt;- rstudent(result4) shapiro.test(residual) qqnorm(residual, pch=20, cex=2) qqline(residual, col=&quot;gray60&quot;, lwd=2) 11.3.4.2.3 参数及非参数事后检验； 11.3.4.2.3.1 事后检验的参数逻辑 # LSD： 相当于t检验，只不过它需要在方差分析一定要有统计学差异的情况下才用。所以LSD法并没有控制假阳性错误。 # SNK法 SNK法是先按多组均值大小排序，然后按一个有点类似于t检验的公式分别比较（不过误差计算不同）。比如a、b、c三组均值分别是a最小，c最大，b居中。那么比较时，很显然a和c差别最大，所以在最后的界值上做一些调整。 # Bonferroni法 它的思想是调整检验水准，根据比较的次数重新设定检验水准，然后根据P值做出结论。比如常规的检验水准是0.05，只要P小于0.05就认为有统计学差异。但是如果用Bonferroni法调整，则需要0.05除以比较次数，如比较6次，这时调整后的检验水准是0.05/6=0.0083，也就是说，P值小于0.0083才算有差异。过于严苛！ # Tukey法，有时也叫Tukey HSD法（Honestly Significant Difference test） Tukey法也是基于q检验，大概意思是先确定一个最大差异的临界值，然后分别对其中两组比较，看看哪两组差值大于这个界值，就算有差异。Tukey法是大多数统计学家首先推荐的两两比较方法，不过这种方法只适用于组间例数相等的情况。对于组间例数不等的时候，可用修正的Tukey法，也叫作Tukey-Kramer法。 11.3.4.2.3.2 事后检验的执行代码： ############## 参数事后检验： 方差分析得出总体之间有差异，要进一步知道哪两组之间有差异，就要使用均数间的多重比较，常用的比较方法有SNK检验（q检验），LSD检验，Bonferroni检验，Dunnett检验，TurkeyHSD检验， # 使用pairwise.t.test来说事先事后检验： pairwise.t.test(x, g, p.adjust.method = ”bonferroni”, pool.sd = !paired, paired = FALSE, alternative = c(&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;), ...) # multcomp包中glht()函数提供了多重均值更全面的方法， result4 &lt;- aov(value ~ variable, data = anova1) tukey4 &lt;- glht(result4, linfct=mcp(variable=&quot;Tukey&quot;)) summary(tukey4) ############# 非参数事后检验： # 组间检验：使用 posthoc.kruskal.nemenyi.test进行事后秩和检验： posthoc.kruskal.nemenyi.test(BIO10~class,data=sdmdata, dist=&quot;Tukey&quot;) ## 组内检验： # 参见：https://blog.csdn.net/jbb0523/article/details/109990924 # 参见：https://baijiahao.baidu.com/s?id=1731958531513315048&amp;wfr=spider&amp;for=pc # DunnTest:处理组与对比组样本均数之间差别有无统计学意义； # NemenyiTest:是完全随机设计多样本间多重比较秩和检验的方法； # 相对来说，当分组较少时，使用DunnTest比使用NemenyiTest更合适；’ ans &lt;- kwAllPairsDunnTest(count ~ spray, data = InsectSprays, p.adjust.method = &quot;bonferroni&quot;) ans &lt;- kwAllPairsNemenyiTest(count ~ spray, data = InsectSprays) summary(ans) 11.3.4.3 可视化比较解释差异：i library(ggpubr) library(rstatix) # Visualization: box plots with p-values data(&quot;ToothGrowth&quot;) df &lt;- ToothGrowth ## 设计放置描述p值的范围： my_comparisons &lt;- list( c(&quot;0.5&quot;, &quot;1&quot;), c(&quot;1&quot;, &quot;2&quot;), c(&quot;0.5&quot;, &quot;2&quot;) ) p &lt;- ggboxplot(df, x = &quot;dose&quot;, y = &quot;len&quot;, color = &quot;dose&quot;, palette =c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), add = &quot;jitter&quot;, shape = &quot;dose&quot;) ## 参见compare_means()含义体，支持参数和非参数，也支持p值调整： p + stat_compare_means(comparisons = my_comparisons, label = &quot;p.signif&quot;) + stat_compare_means(label.y = 50) 11.3.4.4 单因素与多因素方差置换检验 ## 单变量方差置换检验： # 优点不需要对数据做前置假设，也不需要进行非参数的秩排序； # 只能进行两两比较： library(lmPerm) sdmdata$species &lt;- factor(sdmdata$species) data &lt;- c(1:8) lapply(data,function(x){summary(aovp(sdmdata[,x]~class,data=sdmdata,perm=&quot;Prob&quot;))}) ## 多元方差置换分析检验： # 当大量数据参与建模时，检验过程时间漫长； # 默认矩阵为bray”布雷距离，但是该距离矩阵中不能包含负值。 # 因此上述的代码中选择使用欧式距离矩阵进行计算 library(vegan) va.dist&lt;-vegdist(sdmdata[,1:8], method=&#39;euclidean&#39;) set.seed(36) #reproducible results va_monva&lt;-adonis2(va.dist~class, data=sdmdata, permutations = 9, method=&quot;euclidean&quot;) va_monva 11.3.4.5 协方差分析 11.3.4.5.1 数据要求 变量特征 因素（自变量）：二分或分类变量 ——主要是指因变量，或者先验设计条件。 协变量：连续的等距或等比数据，且数据无界 因变量：连续的等距或等比数据，且数据无界 变量分布假设 在每个组内，结果变量应该近似服从正态分布。可用直方图目测，用统计方法：正态性统计检验方法（如K-S统计检验）。 每个组的方差应该是近似的。统计检验：Levene统计量，若不显著，则齐性 协变量与自变量之间相互独立，不相关。协变量与因变量之间是线性关系，且在每个组之间因变量对协变量的回归系数无显著差异 11.3.4.5.2 原理 协方差分析仍然沿承方差分析的基本思想，并在分析观测变量变差的时候，考虑了协变量的影响，人为观测变量的变动受四个方面的影响，即控制变量的独立作用、控制变量的交互作用、协变量的作用和随机因素的作用，并且在扣除协变量的影响后，再分析控制变量的影响 11.3.4.5.3 平行性检验 也就是协变量与自变量之间本身就相关，且协变量是连续变量时，这种一个情况下，协变量不再是用于被控制掉的变量，而是也变成自变量来作分析。 对于协方差分析，X是定类数据，Y是定量数据；协变量为定量数据；如果协变量是定类数据，可考虑将其纳入X即自变量中，也或者将协变量作虚拟变量处理；协变量为干扰项，但并非核心研究项；因此通常情况下只需要将其纳入模型中即可，并不需要过多的分析； 因此此时，协方差分析需要满足一个重要的假设：即“平行性检验”，如果交互项（即有号项）的p 值&gt;0.05则说明平行，满足“平行性检验”，可进行分析。如果协方差分析不满足“平行性”，交互项（即有号项）的p 值&lt; 0.05则说明不平行，不满足“平行性检验”，此时则应该将协变量项移出。 分类协变量可以通过设置虚拟变量引入回归，有关虚拟变量的设置，你可以参考有关的计量经济学书籍，很简单，如果你的分类变量有三个分类，那么你要设置两个虚拟变量表示教育这个分类变量。例如：教育分为三类（初中，高中，大学），你可以如此设置虚拟变量：D1=（1-高中，0-其它）；D2=(1-大学，0-其它）。然后将D1、D2引入回归模型即可。 协方差分析中，X是定类数据，Y是定量数据；协变量通常为定量数据；如果协变量是定类数据，可考虑将其纳入X即自变量中，或者将协变量做虚拟变量处理。 平行性检验：协方差分析有一个重要的假设即“平行性检验”。“平行性”是指：自变量X与协变量对于因变量Y的影响时，自变量X与协变量之间保持独立性。 如果交互项（即有*号项）的P值&gt;0.05则说明平行，满足“平行性检验”，可进行后续分析。如果协方差分析不满足“平行性”条件，则应该将协变量项移出。 11.3.4.5.4 R代码构建： 控制协变量之后，p值会小一点，这里的案例协变量对于体重是显著影响的。关于函数aov(y ~ x1 + x2, data)，写法的不同表示不同的结果，注意x1位置的地方才是协变量的位置 单变量协方差模型： data（&quot;litter&quot;） # 第一种的是控制的协变量是gesttime，去讨论不同剂量对于老鼠体重的影响 # gesttime是一个小老鼠处在母老鼠中的孕期时间， summary(aov(weight ~ gesttime + dose ,data = litter)) 多因素协方差分析： litter$type = rep(c(&quot;a&quot;,&quot;b&quot;),each = 5,length = 74) ## 还是控制协变量gesttime summary(aov(wight ~ gesttime +dose*type ,data = litter)) 11.3.4.5.5 协方差权重分析 ## 协方差分析的一般写法： ## 学习使用线性协方差分析： # 单因素与多因素协方差分析： library(multcomp) data(litter) head(litter) ## one-way anova: # dose --weigth # -Ggesttime,weight ~dose # 注意以下两种写法存在差异，表示的y-x之间的关系； # 其中weight ~ gesttime+dose，dose表示协方差； # 注意两种写法得到的结果不同；weight ~ dose+ gesttime summaty(aov(weight ~ gesttime+dose,data =litter)) summaty(aov(weight ~ dose+ gesttime,data =litter)) ## 计算权重；根据协方差重新估算权重； library(dplyr) library(effects) litter$dose &lt;- as.factor(litter$dose) fit &lt;- aov(weight ~ gesttime +dose,data = litter) with(litter ,effect(&quot;dose&quot;,fit)) ## 注意这里with的用法； litter %&gt;% group_by(dose) %&gt;% summarise(mean = mean(weight)) ## 单因素方差分析： litter$rate &lt;- litter$weight/litter$gesttime summary(aov(rate ~dose,data =litter)) # 多因素方差分析：使用 * 号； litter$type &lt;- rep(c(&quot;A&quot;,&quot;B&quot;),each =5,length=74) summary(aov(weight ~ gesttime + dose*type,data =litter)) ## 多因素方差分析重复测量； # 单因素与双因素； data(CO2) CO2 chill &lt;- subset(CO2,Treatment == &#39;chilled&#39;) head(chill) ## 后面这个Error(Plant/conc)，一般是先写组间再写组内； summary(aov(uptake ~ type +Error(Plant/conc),chill)) summary(aov(uptake ~ conc*Type +Error(Plant/conc),chill)) ## 非参数分析 ## 重抽样进行非参数方差分析： library(lmPerm) head(litter) summary(avop(wight ~ dose*type,data =litter)) summary(aov(weight ~ dose*type,data =litter)) summary(avop(weight ~ gesttime +dose*type,data =litter)) summary(aov(weight ~ gesttime +dose*type,data =litter)) 11.3.4.6 重复方差分析 重复测量资料(repeated measurement data)最常见的情况是同一个试验对象前后有2次测量结果的试验结果，也称作前后测量设计(premeasure-postmeasure design)，例如患者治疗前、治疗后；或者治疗后0-30-60-90min； 此外，重复测量还会设计到在前后顺序改变过程中的交互变量发生改变，这意味着总方差的分解过程会伴随着交互变量的改变而改变； ## 重复方差设计预配对t检验的区别： 1 、配对设计中同一对子的两个实验单位可以随机分配处理，两个实验单位同期观察试验结果。而前后测量设计不能同期观察试验结果，虽然可以在前后测量之间安排处理，但本质上比较的是前后差别，推论处理是否有效是有条件的，即假定测量时间对观察结果没有影响。 2、 配对t检验要求同一对子的两个实验单位的观察结果分别与差值相互独立，差值服从正态分布。而前后两次观察结果通常与差值不独立，大多数情况下第一次观察结果与差值存在负相关关系。 3、配对设计用平均差值推论处理的作用，前后测量测序除了分析平均差值外，还可以进行相关回归分析， ## 参见： https://www.jianshu.com/p/c069b8e1f116?u_atoken=19661ad4-06d4-4328-8b96-7ec983cccab6&amp;u_asession=019sinCUmhZijSx5BcKwUSz-31dp_WQ2KyV1DtH7R8wqlR9PNRMffTu0wDcSyp3DPiX0KNBwm7Lovlpxjd_P_q4JsKWYrT3W_NKPr8w6oU7K8LlDszb4VKvZSLj6z16asYCvvWHyhA8I9G3hxoTho1LGBkFo3NEHBv0PZUm6pbxQU&amp;u_asig=05WNt_hYBhw9zV7ayn6DKF9el7aghxS8TgXtIIMVtP4GuLjq_Dv_H02vjshnpjcoTOi4qU6j5JoSUbOjbqpPWWybn8e6oLup71aP5KvjNsdoB2Tg6_3P7sPmwCIwOQD4SHudy2Z8O0dH01UmttFfv8i2LZnTk3gII2pm-F2sxgK5z9JS7q8ZD7Xtz2Ly-b0kmuyAKRFSVJkkdwVUnyHAIJzb7SyALGM6K39o0gaVD9xOOiVnEg-9w7mAfXSq6H6sczChTz2MQxpCmDDGYlh3aZze3h9VXwMyh6PgyDIVSG1W9uaEkgYtB28u977gxyvmpd2t06Kw4WXjL8pvgPxmcQynUEYxNDGjK6RsclALBwSzrS2ByRO_qDyMhSv6xqX9HqmWspDxyAEEo4kbsryBKb9Q&amp;u_aref=UIT1%2F0q4ZN1Y%2BuJN4m6X%2FAtsP5A%3D 11.3.5 卡方检验 11.3.5.1 卡方检验简介 分布为基础的假设检验方法。它的原假设是：观察频数与期望频数没有差别。值表示观察值域理论值之间的偏离程度。分布及自由度获得原假设成立情况下当前统计量的概率P。如果p值很小，说明观察值与理论值偏离程度太大，应当拒绝原假设，表示比较资料之间有显著差异。否则将不能拒绝原假设。 11.3.5.2 样本量要求 至少卡方检验的每一个单元格，要求其最小期望频数均大于1，且至少有4/5的单元格期望频数大于5，此时使用卡方分布计算出的概率值才是准确的。否则应该用确切概率法。 四格表资料的卡方检验，样本量与方法选择 样本量 方法 n＞40且T(理论频数)＞5 皮尔逊卡方 n大于40且至少一个格子1＜T＜5 校正卡方 n＜40或T＜1 Fisher确切概率法 11.3.5.3 卡方检验R代码 11.3.5.3.1 单变量卡方检验 x = c(210,312,170,85,223) chisq.test(x)#该函数默认检验概率是否均匀 11.3.5.3.2 chisq.test() # 函数对二维表的行列变量进行卡方检验 ## Pearson&#39;s Chi-squared test #使用vcd包中Arthritis数据集 library(vcd) #生成列联表格式 mytable &lt;- xtabs(~Treatment+Improved,data = Arthritis) #卡方检验 chisq.test(mytable) # Pearson s Chi-squared test #data: mytable #X-squared = 13.055, df = 2, p-value = 0.001463 #p值小于0.05,说明治疗情况和改善情况不独立。 ## 结合base-R的chisq-test: library(broom) meldata %$% # note $ sign here table(ulcer.factor, status_dss) %&gt;% chisq.test() %&gt;% tidy() 11.3.5.3.3 配对卡方检验： ### 2、配对四格表资料的卡方检验： 计数资料的配对设计常用于两种检验方法，特别是对样本中各观测单位中，与person chisp 卡方检验不同的是该检验更多的是关注同一人群干预前后状态后的配对检验。 而配对χ2检验只能给出两种方法差别是否具有统计学意义的判断。 ##当假设检验为McNemar卡方检验，检验统计量有两种情况： # 并且该方法主要用于样本量不大的资料： ## McNemar&#39;s检验（配对卡方检验）用于分析两个相关率的变化是否有统计学意义。 - 当b+c&lt;40时，使用连续性矫正，即correct =T - 当b+c》40时，不使用连续性矫正，即correct =F mcnemar.test(x,correct = T) ## 补充说明： kappa一致性检验： 二者区别：1、Kappa检验旨在评价两种方法是否存在一致性；配对χ2检验主要确定两种方法诊断结果是否有差别；2、Kappa检验会利用列联表的全部数据，而配对χ2检验只利用“不一致“数据；3、Kappa检验可计算Kappa值用于评价一致性大小，而配对χ2检验只能给出两种方法差别是否具有统计学意义的判断。Kappa值判断标准：Kappa≥0.75，说明两种方法诊断结果一致性较好; 0.4≤Kappa&lt;0.75，说明两种方法诊断结果一致性一般; Kappa&lt;0.4，说明两种方法诊断结果一致性较差。 11.3.5.3.4 Fisher精确检验 ## Fisher 卡方检验使用条件：1.随机样本数据； 2.卡方检验的理论频数不能太小。 两个独立样本比较可以分以下3种情况： 1.所有的理论数T≥5并且总样本量n≥40，用Pearson卡方进行检验。 2.如果理论数T＜5但T≥1，并且n≥40，用连续性校正的卡方进行检验。 3.如果有理论数T＜1或n＜40，则用Fisher’s检验。 ## Fisher&#39;s Exact Test for Count ## 一个调用技巧：先使用卡方检验，当出现warning时，改用fisher() ## Warning in chisq.test(.): Chi-squared approximation may be incorrect ## 格式fisher.test(mytable),mytable是一个二维列联表Data library(vcd) mytable &lt;- xtabs(~Treatment+Improved,data = Arthritis) #调用fisher.test()函数 fisher.test(mytable) Fishers Exact Test for Count Data data: mytable p-value = 0.001393 alternative hypothesis: two.sided #P小于0.05，两者之间不独立 11.3.5.3.5 分层卡方检验： 11.3.5.3.6 3.5.1 主检验–Cochran-Mantel-Haenszel检验(CMH) 假设检验： H0：为任一层的行变量x与列变量y均不相关； H1：为至少有一层x与y存在统计学关联； ## Cochran-Mantel-Haenszel test 为两个二分类变量进行分层卡方检验。 mytable &lt;- xtabs(~Treatment+Improved+Sex,data = Arthritis) #调用mantelhaen.test()函数 mantelhaen.test(mytable) Cochran-Mantel-Haenszel test data: mytable Cochran-Mantel-Haenszel M^2 = 14.632, df = 2, p-value = 0.0006647 #结果表明，患者接受的治疗与得到的改善在性别的每一水平下并不独立 ## 计算各层OR值： apply(mydata,3,function(x)(x[1,1]*x[2,2]) / (x[1,2]*x[2,1]) 11.3.5.3.7 3.5.2 OR值齐性检验–Breslow-Day检验 对于分层病例对照研究或者队列研究资料，通常应用-Breslow-Day检验对各层的效应值（OR/RR）进行齐性检验。 若不拒绝齐性假设（P&gt;0.05）才可依据CMH检验的结果退单出暴露因素是否与疾病相关。如果相关，可进一步用Mantel-Haensze法估计OR或RR值及其可信区间。 若拒绝齐性假设（P&lt;0.05）则提示分层变量与暴露因素间存在交互作用，此时CMH检验的结果不能说明问题，可进行多元逻辑回归分析。 library(DescTools) BreslowDayTest(mydata,correct =T) ## 补充： WoolfTest(x) 检验结果与BreslowDayTest一致； 11.3.5.4 分类变量的关联性统计 若得知两个分类变量之间有关联性，需进一步分析关系的密切程度时，可计算Pearson列联系数。 列联系数C取值在0-1之间，0表示完全独立，1表示完全相关。 library(survual) library(vcd) #生成列联表格式 mytable &lt;- xtabs(~Treatment+Improved,data = Arthritis) #卡方检验 chisq.test(mytable) # 关联检验： assocstats(mytable) 11.4 多元线性回归 11.4.1 多元线性回归的概念和基础公式 参见：https://zhuanlan.zhihu.com/p/289014877 11.4.2 多元线性回归的本质 求出每项参数x的系数和常数项，使得预测值和与样本值y最接近，让误差平方和Q最小。 一般使用最小二乘法来抵消正负问题，使得回归模型转为求解Q的过称； 如下图所示：求Q的最小值的问题题，是将预测值带入(这里的预测值为实际参数值)，分别对X1,X，。。XM，求偏导得到线性方程组，通过矩阵解法2得到常数项和系数的值； image-20210705094905648 11.4.3 回归方程的参数解释： 11.4.3.1 回归分解 首先对总偏差平方和进行分解，分解为回归平方和与残差平方和。对于回归方程来讲，残差平方和越小越好，因为它对应的是预测值与样本值之间的差异。这里和一元回归类似，采用 F 检验。目的就是检验：回归平方和是否大于残差平方和（F的右尾检验），如果大于，那么就可以说该方程具有统计意义。 image-20210705095241718 11.4.3.2 多元线性回归的条件 image-20210705095241718 11.4.4 回归方程的评价指标 11.4.4.1 均方误差（MSE）： 本质上与上面的最小残差平方和没有本质的区别，只是加了一个平均数；意义在于可以有效降维到数据本体的单位上 image-20210705095241718 11.4.4.2 均方根误差（RMSE）： image-20210705095241718 实质与均方误差是一样的。只不过用于数据更好的描述。 例如：要做房价预测，每平方是万元（真贵），我们预测结果也是万元。 MSE与RMSE的区别仅在于对量纲是否敏感 11.4.4.3 R2： image-20210705095241718 其中SSR表示残差，SST表示总偏差平方和 如果结果是0，就说明我们的模型跟瞎猜差不多。 如果结果是1。就说明我们模型无错误。 如果结果是0-1之间的数，就是我们模型的好坏程度。 如果结果是负数。说明我们的模型还不如瞎猜 11.4.4.4 F统计： F统计为已解释方差和未解释方差的比值。这意味着如果F值大是好的； image-20210705105800482 11.4.4.5 基于混淆矩阵评价： 11.4.4.5.1 5.1 原始混淆矩阵 ### 模型中常用的参数释义 TP: 将正类预测为正类数 FN: 将正类预测为负类数 FP: 将负类预测为正类数 TN: 将负类预测为负类数 准确率(accuracy) = 预测对的/所有 = (TP+TN)/(TP+FN+FP+TN) 精确率(precision，也叫正确率) = TP/(TP+FP) 召回率(recall) = TP/(TP+FN) ## 敏感性（sensitivity，TPR，也称之为真阳性率和召回率（recall））= TP / (TP + FN) ，“有病的被判断为有病的”越大，“漏检”(FN)越小。 特异性（specificity，TNR）= TN / (TN + FP) ，Specificity的值越大，说明“健康的被判断为健康的”的越大，“误检”(FP)越小。 F值（F1,即为精确率和召回率的调和平均值） = 精确率 * 召回率 * 2 / ( 精确率 + 召回率) 阴性预测率（negative, NPV): TN / (TN + FN): 被分为负类中分为负类的； 阳性预测量（positive, PPV）: TP / (TP+FP): 被分为正类中分为正类的； 11.4.4.5.2 5.2 混淆矩阵二次开发 ## Youden指数：Sensitivity+Specificity−1=TPR−FPR 约登指数(Youden index)：是评价筛查试验真实性的方法，假设其假阴性（漏诊率）和假阳性（误诊率）的危害性同等意义时，即可应用约登指数。约登指数是灵敏度与特异度之和减去1。表示筛检方法发现真正的患者与非患者的总能力。指数越大说明筛查实验的效果越好，真实性越大。 ## ROC曲线： 接收者操作特征曲线(receiver operating characteristic curve)，是反映敏感性和特异性连续变量的综合指标，roc曲线上每个点反映着对同一信号刺激的感受性。 具体计算原理是基于二分类计算一个预测得分，然后在给定一个分类阈值，，并将概率值（预测得分）大于阈值的测试样本预测类表记为真，否则为假，就可以得到不同分类阈值下的TPR和FPR。再将所有对应分类阈值对应的成对TPR和FPR在同一张图中连接起来即可使用。 ## AUC值： AUC (Area Under Curve) 被定义为ROC曲线下的面积，显然这个面积的数值不会大于1。又由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值范围一般在0.5和1之间。使用AUC值作为评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好，而作为一个数值，对应AUC更大的分类器效果更好。 11.4.5 回归解释相关参数： 11.4.5.1 多元线性回归的参数选择 ## 模型选择： https://en.wikipedia.org/wiki/Feature_engineering ## 特征选择： https://en.wikipedia.org/wiki/Feature_engineering ## 特征学习： https://en.wikipedia.org/wiki/Feature_learning 11.4.5.2 线性回归的五大基本假设 假设一：误差的分布是正态分布 因为只有误差的分布是正态分布的时候，最小二乘估计才是最优解/最有可能的值。 如果误差项不呈正态分布，意味着置信区间会变得很不稳定，我们往往需要重点关注一些异常的点（误差较大但出现频率较高），来得到更好的模型。 假设二：误差的方差是常数 如果误差的方差不是常数，也就是异方差性。那么在假设一中说了误差的分布需要是正态分布，也就是与方差得是一个常数矛盾。所以当误差的方差是一个可变值的时候，意味着当我们进行建立回归模型的时候，往往会高估误差项（outlier）的重要性，导致回归效果不好。 假设三：误差项之间相互独立 同理，在假设一中，若误差项的分布为正态分布，那么误差项之间也需要相互独立。如果误差项之间不相互独立的话，那么就是说明他们存在自相关性。也就是后一项的值会受到前一项的影响（常常出现在时间序列数据集上）。当自相关性发生的时候，我们测的标准差往往会偏小，进而会导致置信区间变窄。 假设四：不存在多重共线性 首先，要弄清楚多重共线性与变量之间不存在相关关系区别开。变量之间没有多重共线性，不意味着他们没有相关关系，反之亦然。 多重共线性是指，如果我们发现本应相互独立的自变量们出现了一定程度（甚至高度）的相关性，那我们就很难得知自变量与因变量之间真正的关系了。 当多重共线性性出现的时候，变量之间的联动关系会导致我们测得的标准差偏大，置信区间变宽。那也就是说，使用最小二乘法求解得到的回归线不再是最佳的，有效性减小。 假设五：线性性与可加性 线性性：X1每变动一个单位，Y相应变动a1个单位，与X1的绝对数值大小无关。 可加性：X1对Y的影响是独立于其他自变量（如X2）的。 11.4.6 回归解释相关补充： 11.4.6.1 线性回归与多元逻辑回归的区别 线性回归解决的是回归问题，逻辑回归相当于是线性回归的基础上，来解决分类问题。 简单来说，回归问题使用的是最小二乘法来逼近回归解，而逻辑回归则使用似然估计方法来找到最佳分类概率值。逻辑回归可以理解为在线性回归后加了一个sigmoid函数。将线性回归变成一个0~1输出的分类问题。线性回归得到大于0的输出，逻辑回归就会得到0.5 ~ 1的输出；线性回归得到小于0的输出，逻辑回归就会得到0 ~ 0.5的输出； 联系： 逻辑回归可以理解为在线性回归后加了一个sigmoid函数。将线性回归变成一个0~1输出的分类问题。 区别： 1.线性回归用来预测连续的变量（房价预测），逻辑回归用来预测离散的变量（分类，癌症预测）； 2.线性回归是拟合函数，逻辑回归是预测函数 3.线性回归的参数计算方法是最小二乘法，逻辑回归的参数计算方法是似然估计的方法 附加1： 1）线性回归要求变量服从正态分布，逻辑回归对变量分布没有要求。 2）线性回归要求因变量是连续性数值变量，逻辑回归要求因变量是分类型变量。 3）线性回归要求自变量和因变量呈线性关系，逻辑回归不要求自变量和因变量呈线性关系 4）逻辑回归是分析因变量取某个值的概率与自变量的关系，而线性回归是直接分析因变量与自变量的关系 11.4.6.2 线性回归与lasso回归与岭回归（ridge）回归的区别 与逻辑回归一样，线性回归同样面临着在training的时候过分依赖训练集的数据，导致过拟合问题，所以我们需要在原线性回归的损失函数中加点别的东西，让回归/拟合过程中减少对训练集的“关注”。同样地，采取的策略就是在损失函数中加入正则项L1或者L2. lasso regression = linear regression + L1 regularization ridge regression = linear regression + L2 regularization 随着正则化强度的增大，θ的取值会逐渐变小，L1正则化会将参数压缩到0，L2正则化只会让参数尽量小，不会取到0。 所以在L1正则化在逐渐加强的过程中，相对不重要的特征的参数会比相对重要的特征的参数更快地变成0. 所以L1正则化本质是一个特征选择的过程。选出少量但重要的特征，以防止过拟合问题。 而L2正则化在加强的过程中，会尽量让每个特征对模型都有一些贡献，相对不重要的特征的参数会非常接近0. 11.5 相关检验 11.5.1 相关检验和系数 x=c(113,120,138,120,100,118,138,123) y=c(138,116,125,136,110,132,130,110) ## 计算协方差： 协方差用于衡量两个变量的总体误差。而方差是协方差的一种特 殊情况，即当两个变量是相同的情况。设 X,Y 为两个随机变量， cov(x,y) ## 相关系数： 相关系数是用以反映变量之间相关关系密切程度的统计指标。相关系 数是按积差方法计算，同样以两变量与各自平均值的离差为基础，通过两个 离差相乘来反映两变量之间相关程度。 cor(x,y) cor.test(x,y) 11.6 医疗相关检验 11.6.1 Log-rank检验 适合于肿瘤临床实验中，生存时间是非常重要的指标。但以时间为指标时，存在着“截尾/删失”现象。因此会使用累积阶段的生存率或者生存期作为组间比较的工具。 其中生存过程的描述称之为：KM曲线法； 生存过程的比较：log-rank检验。 注意：（1）logrank检验存在等比例风险假设，需要注意试验组和对照组不能存在交叉； image-20220518170259286 "],["id_-Build-R-package.html", "第 12 章 构筑R package 12.1 构筑R package相关参数配置", " 第 12 章 构筑R package 12.1 构筑R package相关参数配置 12.1.1 创建本地环境并使用renv初始化本地安装环境 ## 1 创建本地环境并使用renv初始化本地安装环境：---- library(devtools) library(usethis) library(roxygen2) library(tidyverse) library(lubridate) library(gtsummary) setwd(&quot;&quot;) setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) # ## 在指定路径下创建R包： usethis::create_package(&quot;./&quot;) ## 进行本地版本控制： # install.packages(&quot;renv&quot;) # 初始化环境 renv::init() # 保存当前所用的包环境，当然我们才刚刚开始开发，别的包都没有引入 renv::snapshot() 12.1.2 常用配置 12.1.2.1 描述文件（description）的常用配置 ## 2.1 描述文件（description）的常用配置 ---- ## Description： # Description是对包的描述，每行不超过80个字符，行间使用4个空格分开 ## Version： # Version表示版本号，版本号最少要有2个整数中间用点号或者横线隔开 # 推荐的格式： # releaesd版本由3个数字构成：&lt;major&gt;.&lt;minor&gt;.&lt;patch&gt; # In-development版本由4个数字构成，第四个是开发版本，从9000开始，所以包的第一个版本是0.0.0.9000 ## Auther@R： Authors@R: person(given = &quot;First&quot;, ## given在前(名)，family在后(姓) family = &quot;Last&quot;, role = c(&quot;aut&quot;, &quot;cre&quot;), # cre creator or maintainer 有问题时应该联系额人 # aut 对包贡献最大的人 # ctb 贡献者 # cph copyright holder nicheerfeng@gmail.com 如果版权是作者以外的人或机构，要注明 email = &quot;first.last@example.com&quot;, comment = c(ORCID = &quot;YOUR-ORCID-ID&quot;)) ## 添加R包：在DESCRIPTION中添加说明文件； ## 添加依赖的R包： # Imports # 描述的是包工作所必需的包，在我们的包被安装的时候， # 如果这些包之前没有被安装，这个时候会被安装 # Suggests # 不是必需安装的，可能在示例数据，运行测试， # 从R包中指定函数输出； use_package(&quot;forcats&quot;) use_package(package, type = &quot;Imports&quot;, min_version = NULL) ## 添加许可： # mit -- 任意使用； # gpl3 -- 任意使用，需公开源码； # ccby -- 不可用于商用； ## 添加mit-license: # 这一步会在三个文件中添加参数： # 第一个是在LICENSE中添加字符描述； # 第二个是在LICENSE.md中添加详细描述； # 第三个是在Description中添加license的字段描述； ## 这里还可以再引号内输入姓名，用于指明该文件的说明者； usethis::use_mit_license(&quot;MIT license&quot;) ## 在description中添加指定描述： # URL: https://github.com/swsoyee/rPackageTutorial # BugReports: https://github.com/swsoyee/rPackageTutorial/issues 12.1.2.2 namespace的常用配置 ## namespace的常用配置：---- 创建一个专门用于该R包进行环境开发的R文件，用于封装参数； 注意：下面的null是必须的； 此外， `%&gt;%`的使用需要使用usethis::pip()来生成描述文件， 把描述文件的内容复制下来和这个一起即可。 另外，这个函数文件中通常还会涉及到由非标准引用造成的check错误； 需要使用utils::globalvirables()来添加对应的描述。 #&#39; @import dplyr #&#39; @import tibble #&#39; @import lubridate #&#39; @import parallel #&#39; @import gtsummary #&#39; @import tidyselect #&#39; @import methods #&#39; @importFrom stringr str_replace str_c #&#39; @importFrom rlang enquo #&#39; @importFrom utils globalVariables #&#39; @importFrom utils data #&#39; @importFrom pacman p_load #&#39; @importFrom purrr map map_df #&#39; @importFrom magrittr %&gt;% #&#39; @importFrom tidyr spread separate_rows replace_na crossing separate gather #&#39; @importFrom stats sd NULL #&#39; Pipe operator #&#39; #&#39; See \\code{magrittr::\\link[magrittr:pipe]{\\%&gt;\\%}} for details. #&#39; #&#39; @name %&gt;% #&#39; @rdname pipe #&#39; @keywords internal #&#39; @export #&#39; @importFrom magrittr %&gt;% #&#39; @usage lhs \\%&gt;\\% rhs #&#39; @param lhs A value or the magrittr placeholder. #&#39; @param rhs A function call using the magrittr semantics. #&#39; @return The result of calling `rhs(lhs)`. NULL 12.1.3 构建函数 12.1.3.1 构建函数文 use_r(&quot;test&quot;) 12.1.3.2 函数参数说明 12.1.3.2.1 常规顺序 ## 3.2.1 常规顺序：---- # 函数名、描述(Description)、函数体(Usage)、参数(Arguments)、 # 详情补充(Details)、输出值(Value)、举例(Examples)、 # # 其中： # 默认常规顺序是： # 1 函数名 # 2 描述(Description) # 3 详情补充(Details) # 3.1 补充说明(see also) @seealso # # 次参数顺序： # 4 参数(Arguments) -- @param # 5 输出值(value) -- @return # 6 举例(Examples) -- @examples 12.1.3.2.2 补充参数 ## 3.2.2 补充参数：----- ## 继承参数： # @rdname 从主函数中引用，然后在同一描述文档中展示功能； #&#39; @rdname tr -- 其中tr为主函数； #&#39; @family dfa -- 一般用于函数族的描述，类似于ggplot2中的函数控件； ## 参考文献： #&#39; @references -- \\emph ：表示斜体； #&#39; Michael Friendly (2002). #&#39; \\emph{Corrgrams: Exploratory displays for correlation matrices}. #&#39; The American Statistician, 56, 316--324. #&#39; D.J. Murdoch, E.D. Chow (1996). ## 添加作者： #&#39; @author #&#39; ## 添加声明文档： #&#39; @note #&#39; @seealso ## 添加示例文档： ## @example可以 vignettes/example-corrplot.R #&#39; @example 12.1.3.2.3 辅助描述 ## 添加网页引用： # \\url{https://en.wikipedia.org/wiki/Integer_overflow} ## 添加本R包的函数引用： \\link{function} ## 引用其他R包的另外一种写法： \\code{\\link{print}} ## 添加其他R包的函数引用： \\link[base]{cut()} ## 代码提示： `function` / [function()] 12.1.3.3 函数构建的经验 # 使用invisible()来隐藏函数输出； funs = function(x,y){ z = x+y invisible(z) } # 使用...来输入无限多的参数： funs = function(...){ args = list(...) Reduce(`+`,args) } funs(1,2,3,4,5,6) # 函数中套用函数： funs = func(a,b){ b(a) } funs(c(1,2),mean) ## 辅助函数构建的非标准引入： ## 参数的非标准引用：必须发生在函数内部： terst = function(data,test){ var = enquo(test) datatable2 %&gt;% select(!!var) %&gt;% dplyr::n_distinct(.)} terst(datatable2,patient) ## 字符串的非标准引用： raw_patient_id = &quot;patient&quot; var &lt;- sym(raw_patient_id) datatable2 %&gt;% select(!!var) %&gt;% dplyr::n_distinct(.) 12.1.4 函数测试 12.1.4.1 函数快速测试 devtools::load_all() # - 快捷键Ctrl + Shift + L ； 12.1.4.2 函数单元测试 # 使用testthat包： # context 写一个简短的介绍文件中的测试内容 # # expect_equal()是基于all.equal()的 - 相对估计； # expect_identical()是基于identical - 相对精准； # expect_match 是基于grepl，识别字符使用； # expect_output()匹配输出类型； # expect_message()检查信息； # expect_warning()检查warning； # expect_error()检查错误 # expect_is()检查某个对象是不是继承自一个特定的类： # expect_true() and expect_false() 当没有其他的expectation可用时使用 ## 测试案例： ## expect_is() 检查某个对象是不是继承自一个特定的类： model &lt;- lm(mpg ~ wt, data = mtcars) class(model) #[1] &quot;lm&quot; expect_is(model, &quot;lm&quot;) ## 匹配输出类型： a &lt;- list(1:10, letters) str(a) # List of 2 # $ : int [1:10] 1 2 3 4 5 6 7 8 9 10 # $ : chr [1:26] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; ... expect_output(str(a), &quot;List of 2&quot;) ## 测试案例的基本完整写法： ## 包含生成覆盖率检验--- 首先一个逻辑是这个测试是针对的函数本身，并不针对函数内部的细节参数； 其次，他有一个相对固定的格式来判断的数据 的结果可行性； 本质：他是example的一个结果验证： context(&quot;Just testing printer&quot;) test_that( &quot;just test whether printer is ok&quot;.{ set.seed(1) res = printer(x = rnorm(5),r -r(norm(5))) expect_equal(nrow(res),5)}) 写完测试后的检查： devtools::test() 检查完后输出：测试覆盖率； 在addins中输入coverage，点击即可输出每个函数的测试覆盖率。 然后输入： covr::package_coverage(type =&quot;all&quot;) 输入该参数后： 会在.trasvis.yml文件中生成： after_success: `Rescript e `covr::codecov(type =&quot;all&quot;)`` 12.1.5 添加项目内置数据集 12.1.5.1 仅供函数内部使用 # 如果想要存储原始数据，可以放到inst/extdata里面 usethis::use_data_raw() usethis::use_data(data_med,internal = TRUE,overwrite = TRUE) 12.1.5.2 使用inst 或者DATA usethis::use_data(data_med,internal = FALSE,overwrite = TRUE) # 内部使用形式的具体说明： # 下面的data不需要指明加载路径，只需要提供输入参数的名称即可； #&#39; @format ..... #&#39; \\describe{ #&#39; \\item{price}{price,in Us dollars} #&#39; \\item{carrt}{....} #&#39; ... #&#39; } #&#39; @source \\url{http://www.doa.info/} &quot;data&quot; 12.1.6 添加辅助描述文件 12.1.6.1 添置readme/NEWS/buideignore代码美化和代码规范 ## buideignore文件的写法： ^data$ ^data\\.md$ ## 添加说明文件： usethis::use_readme_md() # 创建可执行的read.me文件方便进行参数构建； use_readme_rmd() ## 增加 NEWS 页面，用于记录每一次升级所做出的变更 usethis::use_news_md() # 添加 Code of Conduct # 根据弹出的提示，把自动生成的内容添加到 README.Rmd 中并且重新生成 README.md 后提交本次变更 usethis::use_code_of_conduct() ## 使用styler进行代码美化： install.packages(&quot;styler&quot;) styler::style_pkg() ## 代码规范： ## install.packages(&quot;lintr&quot;) # 对整个包进行不符合规范的代码查询 lintr::lint_package() 12.1.6.2 构建vignette usethis::use_vignette(&quot;pmed_RWD&quot;) 12.1.6.3 添加包的说明在线网站 ## 添加pkg： library(usethis) usethis::use_pkgdown() ## 安装远程厂库到本地： use_github_file() ## 管理issuse: use_github_labels() ## 还可以使用：直接在R本地执行远程推送； use_github(protocol = &quot;https&quot;) ## 添加origin #git remote add main https://github.com/nicheerfeng/pmed ## 添加token # git remote set-url origin https://oauth2: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@github.com/nicheerfeng/pmed.git ## 推送token： # git push -u origin main ## 补充： # 以前的github里的master，就是现在的main。 # 实现将 pkgdown 站点自动发布到 GitHub 页面所需的 GitHub 设置： # 创建新的分支，用于可视化展示站点信息； use_pkgdown_github_pages() # 构建站点： pkgdown::build_site() # 关于构建网站之后404原因的解释： # https://pkgdown.r-lib.org/reference/build_site.html?q=development#development-mode # _pkgdown.yml 需要设置： development: mode: release # 因为原始的mode为auto，自动识别0.0.0类型为禁止发布类型； 12.1.7 R包整体测试 12.1.7.1 常规测试 devtools::check() # -Ctrl + Shift + E ## 拼写检查： devtools::spell_check() ## 多平台测试： devtools::check_win_devel() # win 平台 usethis::use_github_action_check_standard() # git多平台； ## 复杂检查： devtools::release() 12.1.7.2 测试报错结局方案 ## (1) 测试问题：--- no visible binding for global variable a -&gt;用了dplyr # 解决方案 1 (这里用到了utils包哦) utils::globalVariables(c(&quot;a&quot;)) # 这种方法的具体实现思路： 在R文件下新创建一个env的R文件，并在其中 添加对应的，使用usethis::check()显示参数未发现的变量； 将所有参数未发现的变量，打包成字符串集合的形式放置在globalVariables()内部； if(getRversion() &gt;= &quot;2.15.1&quot;) utils::globalVariables(c(&quot;!!&quot;,&quot;:=&quot;,&quot;.&quot;,&quot;*&quot;)) # 解决方案二 # 这种方案是在每个执行函数文件中将对应的未check到的变量赋值未null。不能使用字符串的形式，必须单个命名为null。 a &lt;- NULL 12.1.8 构建及安装R包 devtools::build() ## 然后就可以通过install()函数来安装这个包： devtools::install() library(tdftest) a = c(&#39;a&#39;) b = c(&#39;b&#39;) fbind(a,b) ## 另外一种本地安装的方法为： install.packages(&quot;~/test/test.tar.gz&quot;, repos = NULL, type = &quot;source&quot;) 12.1.9 构建后上传到github多平台检查 暂定； "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
